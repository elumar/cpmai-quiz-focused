phase,task_group,task,question,option_a,option_b,option_c,option_d,correct_answer,explanation
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,"A project team has started building a churn prediction model. They have gathered extensive customer data and are experimenting with various algorithms. However, they cannot clearly state what business problem they are solving or what value the model will provide. What is the most likely consequence of this approach?",The data science team will eventually be able to define the business objectives once they see which algorithm performs best on the dataset.,The project will deliver a technically sound model that fails to meet business needs or provide measurable value to the organization.,The model will likely have high accuracy on test data but low precision in production due to undefined performance requirements.,The project timeline will be shortened because the team started technical work immediately without waiting for stakeholder alignment.,B,"Option B is correct. Starting model building without clear business objectives means the team lacks a target for what constitutes value. This leads to a technically proficient solution that solves the wrong problem or creates no business value ‚Äî a key pitfall addressed in the Determine Business Objectives task. Option A is incorrect because business objectives should be the first step in Phase I, not derived after model selection; reversing this sequence violates the CPMAI lifecycle. Option C is incorrect because while undefined requirements may cause issues, the primary consequence of missing business objectives is misalignment with business needs, not a specific accuracy/precision trade-off. Option D is incorrect because skipping business objective definition typically leads to rework, scope creep, and delays ‚Äî not a shortened timeline. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,What is the primary output of the 'Determine Business Objectives' task in a cognitive project?,"A detailed list of data sources, required data quality thresholds, and data access permissions needed for the AI project.",A set of model performance metrics like F1-score and precision that define what technically acceptable model output looks like.,"A project schedule with milestones for data collection, model training, validation cycles, and stakeholder review periods.","A clear problem statement, desired business outcomes, and documented stakeholder alignment on the project's business purpose.",D,"Option A is incorrect because data source identification and quality thresholds are defined later during Phase II (Data Understanding), not during Determine Business Objectives. Option B is incorrect because model performance metrics are technical specifications defined under the AI System Performance and Operation task group, not under Determine Business Objectives. Option C is incorrect because a project schedule is part of the Assess Situation task group (Schedule Requirements), not the primary output of this task. Option D is correct. The primary output of Determine Business Objectives is a business-focused definition of what the project must achieve, including the problem statement, desired outcomes, and stakeholder alignment. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,A retailer wants to use AI to 'optimize inventory.' The project manager schedules a workshop with stakeholders. Which of the following best represents a well-defined business objective that should emerge from this workshop?,Our data science team will build a model with 95% accuracy for predicting next-week demand across all product categories in our system.,"We will reduce overstock of perishable items by 20% in Q4, leading to a 5% reduction in waste costs across regional distribution centers.",We will implement a random forest algorithm integrated with our existing ERP system to generate automated stock level predictions weekly.,"We need a real-time inventory dashboard that displays current stock levels, reorder points, and supplier lead times for all warehouses.",B,"Option A is incorrect because it defines a model performance metric (95% accuracy), which is a technical target under AI System Performance and Operation, not a business objective. Option B is correct. It states a clear, measurable business outcome (reduced overstock and waste costs) tied to specific business value ‚Äî the hallmark of a well-defined business objective. Option C is incorrect because it specifies a technical solution (algorithm and system integration) before the business objective is fully defined; implementation details belong in later phases. Option D is incorrect because it defines a tool or feature (a dashboard), not a business outcome; dashboards may support an objective but are not objectives themselves. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,"A team is defining the objective for an AI project. One member suggests, 'Our business objective is to build a highly accurate image recognition system.' Why is this statement insufficient as a business objective?",It fails to identify the target department or business unit that will use the image recognition system and measure its impact.,"It correctly focuses on the AI model's core technical performance, which is exactly what business objectives should specify first.",It describes a technical objective ‚Äî specifying what the AI system will do ‚Äî rather than a business objective that states the business outcome.,It should include the specific dataset and training infrastructure required before it can be considered a complete business objective.,C,"Option A is incorrect because while identifying the target department might add context, the fundamental problem is that the statement describes a technical capability, not a business outcome. Adding a department name would not fix this. Option B is incorrect because it represents a common misconception ‚Äî business objectives should be stated in business terms (revenue, cost, customer impact), not technical performance terms. Option C is correct. The statement describes a technical specification (build an image recognition system) rather than a business outcome (e.g., reduce product defect rates by 15%). The CPMAI framework requires business objectives to be stated in business terms. Option D is incorrect because datasets and infrastructure are resource requirements under the Assess Situation task group, not components of a business objective. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,"An e-commerce company's business objective is to 'improve customer retention.' They have moved into the modeling phase. Who should have defined the specific success criteria for this objective, and when?","The data scientists, during the modeling phase, by selecting the best achievable F1-score on the validation dataset as the benchmark for success.","The business stakeholders, before modeling begins, as measurable indicators of retention improvement such as subscription renewal rate targets.","The project sponsor, after the model is deployed to production, by observing whether customer behavior changed compared to the prior quarter.","The IT department, during the data integration phase, by defining which customer data fields must pass quality checks before model training.",B,"Option A is incorrect because data scientists define model performance metrics (like F1-score), not business success criteria. Success criteria measure business outcomes, not technical model performance. Option B is correct. Business success criteria are defined by business stakeholders and must be established before modeling begins. They are measurable indicators that the business objective has been met, as specified in the Determine Business Success Criteria task. Option C is incorrect because defining success criteria after deployment defeats their purpose ‚Äî they are needed upfront to guide project decisions and evaluate outcomes. Option D is incorrect because IT defines infrastructure and data quality requirements, not business success criteria. Data quality is addressed in Phase II. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,A company has defined a business objective to 'reduce customer churn by 15% within six months.' The project team needs to establish how this objective connects to measurable success criteria. Which statement best describes the relationship between business objectives and success criteria?,"Success criteria are the technical model metrics such as accuracy, precision, and recall that prove the AI system is performing as designed when measured against the test dataset in production.",Success criteria should be defined after the model is deployed to production so the team can observe actual business impact over a meaningful period before committing to specific targets.,"Success criteria are the measurable indicators that translate the business objective into concrete, verifiable outcomes ‚Äî such as tracking the actual churn rate change from 17.5% to 14.8%.","Success criteria are the data quality thresholds that must be met before the AI model can be trained, ensuring the input data supports the objective.",C,"Option A is incorrect because it confuses model performance metrics with business success criteria. Technical metrics like accuracy and precision measure the model, not the business outcome. Option B is incorrect because success criteria must be established before modeling begins, not after deployment ‚Äî they guide the project, not just evaluate it after the fact. Option C is correct. Success criteria are the measurable indicators that the business objective has been met. They translate objectives into concrete, verifiable outcomes and must be defined by business stakeholders in Phase I. Option D is incorrect because data quality thresholds are part of Phase II (Data Quality ‚Äì Verify Data Quality), not business success criteria. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,A data scientist on a project proposes that the project's success criteria be defined as achieving 92% precision on the validation set. The project manager should correct this by explaining that:,Precision is not a valid metric for classification models and should be replaced with a more comprehensive metric like the AUC-ROC curve.,The validation set is not the correct dataset for measuring final success ‚Äî the held-out test set should be used for all final performance evaluation.,"This is a model performance metric, while success criteria must be measurable business outcomes defined by business stakeholders, not the technical team.","The success criteria must be defined by the data scientist but should use a business-relevant metric instead, such as revenue impact or cost savings.",C,"Option A is incorrect because precision is a valid and commonly used classification metric. The issue is not the metric itself but rather that a model metric is being proposed as a business success criterion. Option B is incorrect because while using the test set for final evaluation is technically correct, this misses the fundamental problem ‚Äî the data scientist is confusing a model metric with a business success criterion. Option C is correct. The CPMAI framework explicitly distinguishes between model performance metrics (technical, like precision) and business success criteria (business outcomes, like cost reduction). Success criteria must be defined by business stakeholders, not data scientists. Option D is incorrect because success criteria ownership belongs to business stakeholders, not data scientists. Even suggesting a business-relevant metric would not fix the ownership issue. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,A bank's business objective is to 'increase the efficiency of loan processing.' Which of the following is the most appropriate business success criterion for this objective?,"The AI model correctly classifies 95% of loan applications into approved, denied, or review-needed categories based on the test dataset.","The average time to process a loan application decreases from 5 days to 3 days, as measured by the operations team's tracking system.",The model's false positive rate for fraud detection during loan processing is reduced to below 1% across all application channels.,The new AI system integrates with the legacy CRM without any data migration errors and processes all records within the nightly batch window.,B,"Option A is incorrect because model classification accuracy is a technical performance metric, not a direct measure of business process efficiency. Option B is correct. This option provides a direct, measurable business indicator of efficiency ‚Äî a reduction in processing time ‚Äî which is exactly what business success criteria should measure. Option C is incorrect because fraud detection false positive rate is a model performance metric for a different objective; the stated objective is processing efficiency, not fraud reduction. Option D is incorrect because system integration and data migration are technical implementation criteria, not measures of the business outcome of improved processing efficiency. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Cost-Benefit Analysis,"A project manager is preparing a proposal for an AI initiative to automate claims processing at an insurance company. She needs to determine whether the project is financially viable before presenting it to the steering committee. In which phase should this financial assessment occur, and what decision does it inform?","Phase I: Business Understanding, to inform the AI Go/No-Go decision by determining whether the project's expected benefits justify its estimated costs.","Phase II: Data Understanding, to assess whether the cost of acquiring and preparing the required data is justified by the project's expected outcomes.","Phase IV: Model Development, to evaluate whether the computational cost of training the selected model is within the approved project budget.","Phase III: Data Preparation, to determine whether the cost of cleaning, labeling, and transforming the data is proportionate to expected business gains.",A,"Option A is correct. Cost-Benefit Analysis is explicitly a Phase I (Business Understanding) activity under the Determine Business Objectives task group. Its purpose is to assess whether the project is worth pursuing from a financial and resource perspective, directly feeding into the AI Go/No-Go decision. Option B is incorrect because while data costs are a consideration, the overall cost-benefit analysis occurs in Phase I, not Phase II. Phase II focuses on understanding the data, not assessing project-level financial viability. Option C is incorrect because computational training costs are an operational detail within Phase IV, not a strategic cost-benefit assessment. The CBA must happen before model development begins. Option D is incorrect because data preparation costs are Phase III activities; the strategic financial assessment must occur in Phase I before any data work begins. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Cost-Benefit Analysis]"
Phase I: Business Understanding,Determine Business Objectives,Cost-Benefit Analysis,A healthcare provider is considering an AI project to automate patient intake. What elements should be included in the cost-benefit analysis during Phase I?,A comprehensive user interface design specification with wireframes and user journey maps that demonstrates how the AI system will integrate into the intake workflow.,A detailed comparison of specific machine learning algorithms (random forest vs. neural network) to determine which approach delivers the best performance-to-cost ratio.,The accuracy score of the final trained model benchmarked against the existing manual intake process to quantify the improvement margin in percentage terms.,"Estimated costs for data acquisition, ML talent, cloud infrastructure, and ongoing maintenance, weighed against expected benefits like reduced administrative hours and improved patient throughput.",D,"Option D is correct. A proper cost-benefit analysis includes a comprehensive view of estimated costs (data, infrastructure, talent, maintenance) and expected benefits (revenue gains, cost savings, efficiency improvements), with a risk-adjusted ROI assessment. This feeds into the Go/No-Go decision. Option B is incorrect because algorithm comparison is a Phase IV (Model Development) activity under Select Modeling Technique. Cost-benefit analysis assesses project-level viability, not algorithm-level trade-offs. Option C is incorrect because model accuracy scores are not available during Phase I ‚Äî no model has been built yet. CBA uses estimated benefits, not measured model performance. Option A is incorrect because UI design is an implementation detail that occurs much later in the project. Cost-benefit analysis assesses financial viability, not system design specifications. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Cost-Benefit Analysis]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A project team has a business objective to 'automatically categorize customer support tickets by urgency and topic.' They begin defining cognitive requirements. One team member proposes: 'We need a Python-based microservice using TensorFlow with a REST API.' Why is this proposal problematic?,It correctly identifies the technology stack but fails to specify the minimum accuracy threshold that the microservice must achieve in production.,"It focuses on technical implementation details rather than the cognitive capabilities the AI system needs to demonstrate, such as natural language understanding.",It is too narrowly focused on one programming language when the team should evaluate multiple technology options before committing to a platform.,"It omits important deployment considerations like cloud hosting requirements, auto-scaling policies, and API rate limiting that should accompany technical specs.",B,"Option B is correct. Cognitive requirements define the cognitive capabilities the system needs (e.g., natural language understanding, classification by urgency) ‚Äî not the technology stack. The proposal describes technical implementation, not cognitive requirements. Option A is incorrect because the issue is not about missing accuracy thresholds ‚Äî the entire proposal is the wrong type of requirement. Accuracy thresholds belong under AI System Performance and Operation. Option C is incorrect because while evaluating multiple technologies is good practice, the fundamental problem is that the proposal defines a technical architecture rather than a cognitive capability. Broadening the tech options doesn't fix this. Option D is incorrect because deployment details like hosting and scaling are operational concerns for Phase VI, not cognitive requirements in Phase I. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A bank's business objective is to 'reduce fraudulent credit card transactions.' Which of the following correctly translates this business objective into a cognitive requirement?,The system must integrate with the bank's existing fraud management platform and process transactions within 50 milliseconds to meet SLA requirements.,"The system must generate a monthly report summarizing transaction patterns, flagged accounts, and false positive rates for the fraud operations team.",The system must be capable of real-time anomaly detection and pattern recognition in transaction data to identify potentially fraudulent activity.,"The system must store all transaction records in a secure, encrypted database that complies with PCI-DSS standards for payment card data protection.",C,"Option A is incorrect because integration requirements and processing speed are technical/system architecture specifications, not cognitive capabilities. These belong in technical requirements, not cognitive requirements. Option B is incorrect because report generation is an output/reporting requirement, not a cognitive capability that the AI system must demonstrate. Option C is correct. This translates the business objective (reduce fraud) into the cognitive capability the system needs ‚Äî real-time anomaly detection and pattern recognition. This is exactly what cognitive requirements should specify. Option D is incorrect because data storage, encryption, and compliance are infrastructure and security requirements, not cognitive capabilities. PCI-DSS compliance relates to the Trustworthy AI Requirements task group. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A project team has defined a cognitive requirement as 'the system must understand natural language in customer emails to extract intent and sentiment.' Which downstream phase is MOST directly affected by this specific cognitive requirement?,"Phase V: Model Evaluation, because the evaluation team will need to define fairness metrics that measure whether sentiment analysis is equitable across demographics.","Phase VI: Model Operationalization, because the operations team will need to build a monitoring dashboard that tracks sentiment classification accuracy in real time.","Phase IV: Model Development, because the modeling team will need to select NLP-specific algorithms and architectures that can handle intent extraction and sentiment analysis.","Phase II: Data Understanding, because the data team will need to collect labeled text corpora with annotated intent categories and sentiment ratings for model training.",D,"Option A is incorrect because while fairness evaluation matters, it is not the most direct downstream impact. The cognitive requirement first drives what data is needed before any evaluation occurs. Option B is incorrect because monitoring is important but comes after the model is built; the cognitive requirement must first shape data collection and model selection. Option C is incorrect because while Phase IV model selection is directly affected, Phase II comes first in the lifecycle ‚Äî the team needs labeled text data before they can select or train NLP models. Option D is correct. The cognitive requirement for NLP-based intent and sentiment extraction most directly drives Phase II (Data Understanding), where the team must collect labeled text corpora with annotated intents and sentiments. Data needs are the first downstream decision affected. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A team is working on a project to predict equipment failures in a manufacturing plant. The business objective is to 'reduce unplanned downtime by 30%.' Which of the following correctly represents the cognitive requirement for this project?,The ability to perform predictive modeling and time-series analysis on sensor data to identify patterns that precede equipment failures before they occur.,"The installation of IoT sensors on all critical manufacturing equipment to collect vibration, temperature, and pressure data at five-second intervals.",The development of a mobile application that alerts maintenance technicians in real time when the AI system predicts an equipment failure is imminent.,The integration of the prediction system with the plant's existing SAP maintenance module to automatically generate work orders for preventive service.,A,"Option A is correct. Predictive modeling and time-series analysis on sensor data represent the cognitive capability the system needs to demonstrate ‚Äî this directly translates the business objective (reduce downtime) into the type of intelligence required. Option B is incorrect because IoT sensor installation is a data infrastructure requirement, not a cognitive capability. Sensors provide the data, but the cognitive requirement is about what the system does with that data. Option C is incorrect because a mobile alert application is a user interface and notification requirement, not a cognitive capability of the AI system itself. Option D is incorrect because SAP integration and automated work order generation are system integration requirements that belong in later phases, not cognitive requirements in Phase I. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,"A project's cognitive requirement is to 'predict the future sales volume of a product for the next 12 months based on historical sales data, seasonality, and promotional events.' Which AI pattern is the most appropriate match for this cognitive requirement?","Classification, because the system needs to categorize future sales into predefined volume brackets such as high, medium, and low demand levels.","Clustering, because the system should group similar sales periods together to identify recurring seasonal patterns across product categories.","Regression or time-series forecasting, because the system needs to predict a continuous numerical output (sales volume) across a future time horizon.","Anomaly detection, because the system must identify unusual spikes or drops in sales volume that deviate from established historical patterns.",C,"Option A is incorrect because classification assigns discrete categories, but the requirement is to predict a continuous numerical value (sales volume), not to categorize it into brackets. Option B is incorrect because clustering groups similar data points for pattern discovery, but the requirement is to predict a specific future value, not to discover groupings in historical data. Option C is correct. Predicting a continuous numerical value (sales volume) over a future time horizon is a regression or time-series forecasting problem. This directly matches the cognitive requirement. Option D is incorrect because anomaly detection identifies unusual deviations, but the requirement is to predict expected future values, not to flag anomalies in existing data. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,"A team has a business objective to 'personalize product recommendations for website visitors based on their browsing history and purchase behavior.' They correctly identified the AI pattern as 'Recommendation System' in Phase I, but a junior data scientist argues the pattern should be identified later during model development. Why is this a problem?","Delaying pattern identification until Phase IV means the team may collect and prepare the wrong type of data in Phases II and III, wasting significant effort and resources.",Identifying the pattern early locks the team into a single approach and prevents them from exploring alternative algorithms during the model development phase.,The AI pattern should be identified during Phase V: Model Evaluation so the team can compare multiple patterns against the actual model performance results.,Delaying pattern identification has no practical impact because modern AutoML tools can automatically determine the correct pattern from any sufficiently large dataset.,A,"Option A is correct. AI Pattern Identification is a Phase I activity because the selected pattern drives data collection (Phase II) and data preparation (Phase III). Delaying it to Phase IV means the team may collect and prepare data unsuitable for the correct pattern, cascading into wasted effort across multiple phases. Option B is incorrect because identifying the pattern early does not lock the team into a single algorithm ‚Äî the pattern (recommendation) is different from the specific algorithm. Multiple algorithms can implement the same pattern. Option C is incorrect because Phase V evaluates model results against pre-established criteria; it does not determine which AI pattern to use. Pattern identification must occur in Phase I. Option D is incorrect because while AutoML can assist with algorithm selection, it cannot compensate for fundamentally wrong data collected because the pattern was not identified upfront. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,A team is defining a project to 'detect unusual patterns in network traffic that may indicate cybersecurity threats.' They have identified the cognitive requirement as 'real-time anomaly detection in high-volume streaming data.' Why is identifying the AI pattern (anomaly detection) in Phase I critical for this project?,It ensures the team focuses on collecting the right type of data in Phase II ‚Äî such as labeled normal and anomalous network traffic logs with appropriate temporal granularity.,"The AI pattern should actually be identified during Phase IV: Model Development, after the team has explored the data and determined which algorithms are most effective.",Identifying the AI pattern in Phase I is a formality that has little practical impact because experienced data scientists can adapt any modeling approach to fit the data.,"It ensures that the Phase I project plan includes a cost-benefit analysis specifically for anomaly detection infrastructure, which is required before the Go/No-Go decision.",A,"Option A is correct. Identifying anomaly detection as the AI pattern in Phase I ensures that Phase II data collection focuses on the right data ‚Äî labeled network traffic with normal and anomalous examples, appropriate temporal granularity, and sufficient volume. Phase I pattern identification guides all subsequent data and modeling decisions. Option B is incorrect because the CPMAI framework places AI Pattern Identification in Phase I, not Phase IV. Deferring to Phase IV risks collecting wrong data in Phase II and preparing it incorrectly in Phase III. Option C is incorrect because AI Pattern Identification is a substantive Phase I activity, not a formality. The selected pattern drives data collection, preparation, model selection, and evaluation criteria across the entire lifecycle. Option D is incorrect because while cost-benefit analysis is a Phase I activity, it assesses overall project viability ‚Äî it is not specifically triggered by or dependent on the AI pattern selection. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,A team's business objective is to 'automatically assess the sentiment of customer reviews and route negative reviews to the retention team within one hour.' The cognitive requirement is 'natural language understanding to classify review sentiment.' Which AI pattern is the correct match?,"Regression, because the system needs to assign a continuous sentiment score from negative to positive on a numerical scale for each customer review.","Clustering, because the system should group reviews into natural categories based on the language patterns and topics discussed by customers.","Anomaly detection, because the system must identify reviews that deviate significantly from the typical positive sentiment baseline of the customer population.","Classification, because the system needs to assign each review to a discrete sentiment category (positive, negative, neutral) to enable routing decisions.",D,"Option A is incorrect because while sentiment can be scored numerically, the business requirement is to route negative reviews ‚Äî this requires discrete categorization (positive/negative/neutral), not a continuous score. The routing decision is inherently categorical. Option B is incorrect because clustering discovers natural groupings in unlabeled data, but the requirement specifies predefined sentiment categories. This is a supervised classification task, not unsupervised clustering. Option C is incorrect because anomaly detection identifies unusual deviations from normal patterns, but sentiment classification assigns every review to a category ‚Äî it is not looking for outliers. Option D is correct. Classifying each review into a discrete sentiment category (positive, negative, neutral) is a classification task. This directly matches the cognitive requirement and enables the business process of routing negative reviews to the retention team. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,A project team is defining resource requirements for a new predictive maintenance system. The project manager needs to secure budget for specialized cloud-based GPU instances to train deep learning models on sensor data. Which resource category does this requirement primarily fall under?,Technology resources ‚Äî the need for specific computational infrastructure such as cloud-based GPU instances to support model training.,Human resources ‚Äî the need to hire additional data scientists with deep learning expertise to build and tune the prediction models.,Data resources ‚Äî the requirement for high-frequency sensor data streams from manufacturing equipment to train the predictive models.,"Financial resources ‚Äî the overall project budget allocation that must cover infrastructure, personnel, data acquisition, and maintenance.",A,"Option A is correct. The requirement for cloud-based GPU instances is a specific type of computational infrastructure, which falls under the technology resources category. Resource requirements in Phase I must categorize needs across human, data, technology, and financial dimensions. Option B is incorrect because it describes human expertise and staffing needs, not the computational hardware being requested. Option C is incorrect because it describes the data itself that will be used for training, not the technology infrastructure needed to process it. Option D is incorrect because while the GPU cost is part of the overall financial budget, the requirement being defined here is for a specific technology asset, not the financial category as a whole. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,"A retail company is planning an AI project to optimize inventory across 500 stores. The initial resource plan includes one part-time data scientist and assumes all necessary data is readily available in a clean data warehouse. Six weeks into the project, the team discovers that inventory data is spread across legacy systems in different formats, requiring significant manual effort to clean and integrate. What is the primary reason for this project setback?","The business objective was not clearly defined by stakeholders, leading to misaligned expectations about the scope and complexity of the work required.","The team underestimated the data resources required, specifically the effort needed for data access, cleaning, and integration across legacy systems.","The AI pattern was incorrectly identified during Phase I, which caused the team to prepare for the wrong type of modeling approach and data structure.","The schedule requirements did not account for adequate stakeholder review periods, which delayed critical decisions about data source prioritization.",B,"Option A is incorrect because while clear business objectives are important, the setback described stems from a resource planning failure ‚Äî underestimating data complexity ‚Äî not from unclear objectives. Option B is correct. This scenario directly illustrates the common failure mode of underestimating resource requirements, specifically the effort and complexity associated with data resources (access, cleaning, integration across legacy systems). Realistic resource assessment is critical in Phase I. Option C is incorrect because the problem is data availability and quality, not the wrong AI pattern selection. The team would face this data challenge regardless of the pattern chosen. Option D is incorrect because the issue is insufficient resources allocated for data work, not a missed stakeholder review. The schedule failed because the resource estimate was unrealistic. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,"A healthcare startup has a strong business case for an AI-driven diagnostic tool with a projected high ROI. They have secured funding and have enthusiastic business stakeholders. However, their assessment reveals that the only data available is a small set of unstructured physician notes, and they have no budget to acquire or label additional data. According to CPMAI principles, what should the outcome of the AI Go/No-Go decision be, and why?","Go, because the strong business case, secured funding, and stakeholder support are sufficient to outweigh any data limitations in the current plan.","Go, but with a revised schedule that extends the project timeline to allow the team to gradually accumulate and label additional data over time.","No-Go, because the business objective needs to be redefined to align with a less data-intensive approach that can work with the available physician notes.","No-Go, because insufficient data resources make the project infeasible to execute successfully, regardless of the strength of the business case.",D,"Option A is incorrect because a strong business case cannot compensate for a fundamental lack of data resources. The Go/No-Go decision requires business, data, AND execution feasibility ‚Äî all three must pass. Option B is incorrect because extending the timeline does not solve the core problem ‚Äî the startup has no budget to acquire or label additional data, so more time alone will not produce the necessary data resources. Option C is incorrect because while redefining the objective might eventually help, the immediate infeasibility is due to the lack of data resources, not a poorly defined objective. The objective itself is valid. Option D is correct. The AI Go/No-Go decision requires all three feasibility dimensions (business, data, execution) to pass. Even with strong business feasibility, the lack of necessary data resources means data feasibility fails, resulting in a No-Go decision. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,"A project manager is tasked with identifying resource requirements for an AI project. To ensure a comprehensive and realistic assessment, who should the project manager involve in this process?","Only the data science team lead, as they have the deepest understanding of technical needs like compute infrastructure, model complexity, and data volume.","Only the business sponsor, as they control the budget allocation and have final authority over which resources are approved for the project.","Only the IT infrastructure team, as they manage the cloud platforms, on-premise servers, and networking resources the AI system will require.","Both business stakeholders to define needs and expected outcomes, and technical stakeholders to estimate the effort, talent, and assets required.",D,"Option A is incorrect because while the data science team understands technical needs, they may not fully understand business requirements, budget constraints, or organizational readiness ‚Äî leading to an incomplete assessment. Option B is incorrect because while the business sponsor controls the budget, they typically lack the technical expertise to estimate infrastructure, talent, and data resource needs accurately. Option C is incorrect because the IT infrastructure team manages technology resources but does not have visibility into business objectives, data science talent needs, or domain expertise requirements. Option D is correct. Resource assessment is a project management activity that requires input from both business stakeholders (who define what is needed and the expected outcomes) and technical stakeholders (who estimate the effort, talent, and assets required to deliver). This dual input ensures a comprehensive and realistic assessment. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"An executive sponsor for an AI project demands that the team commit to a fixed launch date of December 31st, with no room for deviation, arguing that traditional software projects can meet fixed deadlines. The project manager explains that AI projects have inherent uncertainty that makes fixed deadlines problematic. What is the primary source of this uncertainty that the project manager should cite?","The unpredictable nature of model performance, which may require multiple iterations of experimentation and validation to achieve acceptable results.","The potential for key team members to leave the project unexpectedly, creating knowledge gaps that delay critical development milestones and deliverables.",The risk of budget cuts from the finance department that could reduce available compute resources and force the team to scale back the project scope.,"The possibility that a competitor will launch a similar AI product first, forcing the team to reprioritize features and adjust the project roadmap.",A,"Option A is correct. A key characteristic of AI projects is the inherent uncertainty in model performance. It is not known in advance whether the data will support the required performance level, making iterative experimentation necessary. The CPMAI framework acknowledges this through its iterative structure, which is why fixed deadlines are problematic for AI projects. Option B is incorrect because team turnover is a general project risk that applies to all projects, not a source of uncertainty unique to AI development. Option C is incorrect because budget cuts are a financial risk that affects all project types equally; this is not the AI-specific uncertainty the project manager should cite. Option D is incorrect because competitive pressure is a market risk, not a source of technical uncertainty in AI model development. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"A team is developing the initial schedule for a customer churn prediction project. Which of the following activities must be explicitly considered as part of the schedule requirements during Phase I, due to the iterative nature of AI projects?",The final marketing campaign launch date to target at-risk customers with personalized retention offers based on the model's churn predictions.,"The procurement process for office furniture, desk equipment, and workspace setup for any new team members joining the project.","Multiple iterations for data acquisition, model experimentation, validation cycles, and stakeholder review periods throughout the project lifecycle.","The specific dates and calendar invitations for daily stand-up meetings, sprint retrospectives, and weekly status reports to stakeholders.",C,"Option A is incorrect because a marketing campaign launch date is a downstream business activity that depends on the AI model's output; it is not a schedule requirement for the AI project's development lifecycle. Option B is incorrect because office furniture procurement is an administrative task unrelated to the AI project lifecycle. While supporting new team members matters, it is not a key schedule consideration for AI projects. Option C is correct. Schedule requirements for AI projects must explicitly account for iterative cycles ‚Äî including data acquisition timelines, model experimentation iterations, validation cycles, and stakeholder review periods. The CPMAI framework acknowledges that AI projects are inherently iterative and schedules must reflect this. Option D is incorrect because daily stand-ups and sprint ceremonies are project management rituals, not the key phase milestones and iterative cycles that drive AI project schedule requirements. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"A project team has an aggressive launch deadline. Their initial resource assessment shows they have a limited number of data scientists. The schedule requirements task reveals that extensive data labeling is needed, which is very time-consuming. What is the most realistic trade-off the team should consider to meet the business need?","Remove the data labeling step entirely and use the raw, unlabeled data for model training, accepting whatever accuracy level the unsupervised approach can achieve.",Accept the original timeline as-is and hope the existing data scientists can work faster than estimated to complete the labeling within the current schedule.,"Increase resources by hiring or contracting additional personnel specifically for data labeling, thereby shortening the labeling timeline while maintaining quality.","Delay the project indefinitely until more data scientists become available through the company's internal hiring pipeline, then restart the planning process.",C,"Option A is incorrect because removing a necessary step like data labeling would likely produce an unusable or significantly degraded model, undermining the entire project. Data preparation quality directly affects model performance. Option B is incorrect because hoping team members will work faster than estimated is not a realistic planning strategy. The CPMAI framework requires realistic resource and schedule assessments. Option C is correct. This illustrates the direct interaction between schedule requirements and resource requirements ‚Äî the time-resource trade-off. To meet an aggressive timeline with a fixed workload, the team can add resources (hire/contract labelers) to compress the schedule. This is a core planning consideration in Phase I. Option D is incorrect because indefinite delay is not a trade-off to meet the original business need ‚Äî it abandons the timeline entirely. While delaying is sometimes necessary, the question asks for a trade-off to meet the aggressive deadline. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"A project team has completed the Assess Situation tasks for a new AI project. They have documented that the project will require six months and a specific set of resources. According to the CPMAI outline, how do these documented schedule and resource requirements directly influence the next steps in Phase I?",They are filed in the project repository for reference during the project closure reports and lessons learned review in Phase VI.,"They feed into the AI Go/No-Go decision as inputs to feasibility assessment, and they are incorporated into the project plan that governs Phases II through VI.",They are used to determine the business success criteria by establishing what outcomes are achievable within the documented time and resource constraints.,They are sent directly to the procurement department to initiate hardware purchases and vendor contract negotiations for the required infrastructure.,B,"Option B is correct. The outputs of the Assess Situation tasks (Resource Requirements and Schedule Requirements) directly feed into two downstream Phase I activities: the AI Go/No-Go decision (where they inform execution feasibility) and the Produce Project Plan task group (where they become part of the formal plan for Phases II through VI). Option A is incorrect because these requirements are used for active planning and decision-making within Phase I, not archived for project closure. The project closure report is a Phase VI activity. Option C is incorrect because business success criteria are determined in the Determine Business Objectives task group, which is a separate and earlier task group within Phase I. Schedule and resource requirements do not define success criteria. Option D is incorrect because while procurement may eventually occur, sending requirements to procurement is an operational action, not the direct next step in the Phase I CPMAI workflow. The requirements must first inform the Go/No-Go decision. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,A project team is defining acceptable model performance values for a new fraud detection system. When should these technical thresholds be established according to the PMI-CPMAI framework?,"During Phase I: Business Understanding, before any data is collected or models are built, so the project has clear technical targets from the start.","During Phase IV: Model Development, after the team has had a chance to experiment with different algorithms and evaluate initial model behavior.","During Phase V: Model Evaluation, when the model is tested against a holdout dataset to measure its actual performance against business requirements.","During Phase VI: Model Operationalization, after deployment to production so thresholds can be calibrated against real-world operating conditions.",A,"Option A is correct. Acceptable model performance values are set during Phase I: Business Understanding, establishing the target thresholds before any modeling begins. This ensures the project has clear technical goals from the outset that guide all subsequent work. Option B is incorrect because Phase IV: Model Development builds models against predefined targets; it does not establish those targets. Setting thresholds after experimentation begins risks anchoring to what is achievable rather than what the business needs. Option C is incorrect because Phase V: Model Evaluation evaluates model results against thresholds that should have been defined much earlier in Phase I. Phase V is the checkpoint, not the definition point. Option D is incorrect because Phase VI: Model Operationalization monitors performance against thresholds defined in Phase I, creating a feedback loop ‚Äî it does not establish the original thresholds. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,A healthcare AI project requires a diagnostic model with extremely low false negatives. The data science team proposes a minimum recall of 0.95 and plans to present this threshold at the next stakeholder meeting. What is the primary issue with this approach?,Recall is not an appropriate metric for diagnostic models in healthcare applications and should be replaced with a domain-specific evaluation measure.,The data science team is unilaterally defining acceptable performance thresholds without sufficient input from business stakeholders who understand acceptable risk.,The threshold of 0.95 recall is too aggressive and almost certainly unachievable given the complexity and noise present in typical healthcare datasets.,"The team should be presenting precision targets instead of recall for this use case, since false positives are the primary concern in medical diagnostics.",B,"Option A is incorrect because recall is highly appropriate for minimizing false negatives in diagnostic models ‚Äî it directly measures the rate of missed positive cases, which is critical in healthcare. Option B is correct. Acceptable model performance values must be set collaboratively by business stakeholders (who define acceptable risk levels) and technical experts (who advise on feasibility). The data science team defining thresholds unilaterally ignores the business context and risk tolerance that stakeholders must provide. Option C is incorrect because it makes an unsupported assumption about data feasibility without knowing the actual data characteristics. A 0.95 recall target may or may not be achievable. Option D is incorrect because recall is the appropriate priority when false negatives (missed diagnoses) are the primary concern. Precision would be prioritized when false positives are the main risk. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,"A project team is establishing acceptable model performance values for a content recommendation system. The business context requires minimizing user exposure to inappropriate content, even if it means some relevant content is filtered out. Which metric should be prioritized when setting performance thresholds for this use case?","Root Mean Square Error (RMSE), which measures the average magnitude of prediction errors for continuous rating values in recommendation systems.","Recall, which prioritizes capturing all relevant content and minimizing false negatives, even at the cost of occasionally showing inappropriate material.","F1 score, which balances precision and recall equally without giving preference to either false positives or false negatives in the filtering system.","Precision, which minimizes false positives and ensures that content shown to users has a high probability of being appropriate and relevant.",D,"Option A is incorrect because RMSE measures continuous value prediction error (e.g., rating predictions), not classification accuracy for content filtering. The business requirement is about filtering inappropriate content, which is a classification problem. Option B is incorrect because prioritizing recall would maximize content shown, which increases the risk of exposing users to inappropriate content ‚Äî the opposite of the business requirement. Option C is incorrect because F1 score gives equal weight to precision and recall, failing to prioritize the critical business requirement of minimizing inappropriate content exposure. The business explicitly accepts missing some relevant content. Option D is correct. Precision measures the proportion of positive identifications that were actually correct. Minimizing false positives (showing inappropriate content) directly addresses the business requirement, even if some relevant content is filtered out (lower recall). [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,A team establishes an acceptable F1 score of 0.85 for a classification model during Phase I. In which subsequent phase will this threshold be formally used as an evaluation benchmark?,"Phase II: Data Understanding, where the team assesses whether the available data quality and volume are sufficient to support achieving the target score.","Phase III: Data Preparation, where feature engineering decisions are guided by the performance goal to ensure the prepared data can support the threshold.","Phase IV: Model Development, where the threshold is used to tune hyperparameters and guide iterative model training toward the acceptable performance level.","Phase V: Model Evaluation, where the final model's results are formally assessed against the predefined acceptable threshold to determine readiness.",D,"Option A is incorrect because Phase II: Data Understanding assesses data availability and quality but does not formally evaluate model performance against thresholds. The threshold cannot be tested before a model exists. Option B is incorrect because Phase III: Data Preparation focuses on selecting, cleaning, and transforming data. While good preparation supports performance, Phase III does not formally evaluate models against thresholds. Option C is incorrect because while Phase IV: Model Development may use the target for guidance during training iterations, the formal go/no-go evaluation against Phase I thresholds occurs in Phase V, not Phase IV. Option D is correct. Acceptable model performance values established in Phase I become the formal evaluation benchmarks used in Phase V: Model Evaluation to determine whether the model meets predefined criteria and is ready for operationalization. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,"A customer churn prediction model achieves 92% accuracy during Phase V evaluation, exceeding the acceptable model performance threshold of 88% set in Phase I. However, after deployment in Phase VI, the company observes no reduction in actual customer churn rates. What does this scenario most clearly illustrate?",The model performance threshold of 88% accuracy was set too low during Phase I and should have been calibrated higher to drive meaningful business outcomes.,The data science team used the wrong evaluation metric during Phase V and should have prioritized recall over accuracy to better capture at-risk customers.,Model performance metrics and business KPI performance values measure different things ‚Äî a model can meet technical thresholds without driving the intended business impact.,The project should have spent more time in Phase IV: Model Development improving the model's architecture and feature engineering before proceeding to evaluation.,C,"Option A is incorrect because setting a higher accuracy threshold would not necessarily translate to business impact. The problem is the disconnect between technical metrics and business outcomes, not the threshold level itself. Option B is incorrect because it speculates about metric choice without evidence that recall would have changed the business outcome. The fundamental issue is that technical model metrics do not guarantee business KPI movement. Option C is correct. This scenario illustrates the critical distinction between model performance metrics (technical accuracy) and KPI performance values (business impact like churn reduction). A model can perform well technically but fail to move business KPIs ‚Äî this is why both must be defined separately in Phase I. Option D is incorrect because more model development time likely would not address the disconnect between technical performance and business impact. The model already exceeded its technical threshold. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,An e-commerce company's business objective is to increase average order value. The project team has defined acceptable model performance values for a product recommendation algorithm. Which of the following represents an appropriate KPI performance value that should also be established in Phase I to measure business impact?,"Achieving an F1 score of 0.82 on the recommendation model's validation set, indicating the model accurately predicts user preferences and purchase behavior.","Increasing the average order value by 12% within six months of deployment, directly measuring the intended business outcome of the recommendation system.","Reducing model training time from four hours to under thirty minutes per cycle, improving the team's ability to iterate and deploy updates more frequently.","Processing 10,000 recommendation requests per second during peak traffic periods, ensuring the system can handle the company's highest-volume shopping events.",B,"Option A is incorrect because an F1 score is a model performance metric (technical), not a business KPI. It measures how well the model predicts, not whether the business objective (increased order value) is achieved. Option B is correct. A 12% increase in average order value is a business-level KPI that directly measures the intended business impact of the project. This connects to the business success criteria defined in the Determine Business Objectives task group. Option C is incorrect because model training time is a technical efficiency metric related to development operations, not a business outcome KPI. It measures process speed, not business impact. Option D is incorrect because request throughput is a system performance or scalability metric. While important for operations, it does not measure the business outcome of increased order value. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,A project team has established a business success criterion of reducing loan processing time by 30% during the Determine Business Objectives task. How do the Acceptable KPI Performance Values defined later in Phase I relate to this criterion?,The KPI performance values replace the business success criterion with more specific and measurable technical metrics that the data science team can directly optimize.,The KPI performance values are defined independently by the technical team and have no direct relationship to the business success criterion set by stakeholders.,"The KPI performance values should be derived from the business success criterion ‚Äî for example, measuring the actual reduction in processing time achieved post-deployment.",The KPI performance values are only relevant during Phase VI monitoring and do not need to be connected back to the original Phase I business objectives.,C,"Option A is incorrect because KPI performance values do not replace business success criteria ‚Äî they operationalize their measurement. The success criterion (30% reduction) remains the target; the KPI provides the mechanism to track whether it is being achieved. Option B is incorrect because KPI performance values are explicitly connected to business success criteria. They are not independent ‚Äî they form a measurement chain that links technical work to business outcomes. Option C is correct. Acceptable KPI performance values connect directly to the business success criteria, creating a measurement chain. The KPI (e.g., actual processing time reduction percentage) measures whether the business success criterion (30% reduction target) is being achieved after deployment. Option D is incorrect because while KPIs are monitored in Phase VI, they are defined in Phase I and must connect back to the business objectives established in the Determine Business Objectives task group. The connection is established during Phase I, not deferred. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,"A project team defines acceptable KPI performance values during Phase I, including a target 15% reduction in manufacturing defects. In which phase are these KPIs primarily monitored, and what action do the results trigger?","Phase VI: Model Operationalization, where KPI results are tracked in production and create a feedback loop to Phase I for continuous improvement and iteration.","Phase IV: Model Development, where the team adjusts hyperparameters and retrains the model based on real-time KPI performance feedback during development iterations.","Phase II: Data Understanding, where the team collects additional manufacturing data if early analysis suggests the KPI target is unlikely to be achievable.","Phase V: Model Evaluation, where KPI values replace model performance metrics as the primary criteria for deciding whether to proceed to operationalization.",A,"Option D is incorrect because Phase V: Model Evaluation evaluates model performance against technical thresholds (accuracy, precision, recall), not long-term business KPIs. KPI monitoring requires the model to be deployed in production first. Option B is incorrect because Phase IV: Model Development occurs before deployment. KPI performance values measure real-world business impact, which cannot be assessed during model training iterations. Option C is incorrect because Phase II: Data Understanding occurs early in the lifecycle and focuses on data collection and exploration, not post-deployment KPI monitoring. Option A is correct. Acceptable KPI performance values are primarily monitored during Phase VI: Model Operationalization after the model is deployed to production. The monitoring results create a feedback loop back to Phase I, informing whether business objectives are being met and whether adjustments or a new iteration is needed. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,Trustworthy AI Requirements,Use of Trustworthy AI Framework,"An AI project team is developing a resume screening system to identify qualified candidates. During Phase I, they consider potential bias against certain demographic groups and establish requirements for regular bias audits. Which pillar of the Trustworthy AI Framework are they primarily addressing?","Fairness, by proactively working to prevent discrimination against protected groups and establishing ongoing bias audit requirements.","Transparency, by documenting how the system makes screening decisions so that candidates and regulators can understand the process.","Robustness, by ensuring the system performs consistently and reliably across different applicant populations and demographic groups.","Privacy, by protecting candidate personal data from unauthorized access and ensuring compliance with data protection regulations.",A,"Option A is correct. The scenario directly addresses fairness by considering potential bias against demographic groups and establishing bias audit requirements. Fairness as a Trustworthy AI pillar focuses on preventing discriminatory outcomes. Option B is incorrect because transparency concerns openness about how the system works and makes decisions, not specifically bias prevention or audit mechanisms. Option C is incorrect because robustness concerns system reliability and consistent performance under varying conditions, not the identification and prevention of discriminatory bias. Option D is incorrect because privacy concerns data protection and access controls, not discriminatory outcomes or bias in screening decisions. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Use of Trustworthy AI Framework]"
Phase I: Business Understanding,Trustworthy AI Requirements,Use of Trustworthy AI Framework,"A project team plans to build the model first, prove its technical feasibility in Phase IV, and then 'bolt on' the Trustworthy AI Framework elements during Phase VI before launch. The project manager corrects this approach, explaining that the Trustworthy AI Framework must be applied starting in which phase, and why?","Phase IV: Model Development, because that is when the model is built and fairness constraints can be coded directly into the algorithms and training process.","Phase VI: Model Operationalization, because trustworthy AI elements like monitoring for drift and bias only become relevant after the system is live in production.","Phase I: Business Understanding, because relevant trustworthy AI dimensions must be identified and requirements established before any data work or modeling begins.","Phase II: Data Understanding, because trustworthy AI is primarily about ensuring the training data is clean, representative, and free from historical biases.",C,"Option A is incorrect because waiting until Phase IV: Model Development to consider trustworthy AI is too late ‚Äî by that point, data has been collected and prepared without trustworthy AI guidance, potentially embedding bias or missing compliance requirements. Option B is incorrect because Phase VI: Model Operationalization is for monitoring and maintenance, not initial framework application. Bolting on trustworthy AI elements at deployment is the exact anti-pattern the project manager is correcting. Option C is correct. The Trustworthy AI Framework must be applied starting in Phase I: Business Understanding, where relevant dimensions (fairness, transparency, explainability, etc.) are identified and requirements established before any data work or modeling begins. This ensures trustworthy AI considerations guide all subsequent phases. Option D is incorrect because while data quality and representation are important, the framework application begins even earlier in Phase I, where the requirements are defined that will guide data collection in Phase II. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Use of Trustworthy AI Framework]"
Phase I: Business Understanding,Trustworthy AI Requirements,Use of Trustworthy AI Framework,"During Phase I for a medical diagnosis AI project, the team documents that the system must provide clear reasoning for its predictions to enable doctor review and must maintain performance across different hospital patient populations. Which two Trustworthy AI pillars are they establishing requirements for?",Accountability and Privacy ‚Äî governance structures for oversight and data protection protocols for patient medical records and personal health information.,Safety and Robustness ‚Äî physical safety protections for patients and system stability guarantees under varying clinical conditions and hospital environments.,Explainability and Fairness ‚Äî clear prediction reasoning for clinical review and equitable performance across diverse patient demographic groups.,Transparency and Adversarial Robustness ‚Äî general openness about system design and specific defenses against malicious attempts to manipulate diagnostic inputs.,C,"Option A is incorrect because accountability involves governance structures and responsibility assignment, not prediction reasoning, and privacy involves data protection rather than equitable performance across populations. Neither matches the scenario. Option B is incorrect because safety concerns physical or psychological harm prevention, and robustness concerns system stability under varying conditions ‚Äî neither specifically addresses prediction reasoning or performance equity across demographic groups. Option C is correct. Providing clear reasoning for predictions directly addresses explainability (enabling doctors to understand and review AI decisions), while maintaining performance across different patient populations addresses fairness (ensuring equitable outcomes across demographic groups). Option D is incorrect because transparency is broader than explainability (it encompasses general openness, not specifically prediction-level reasoning), and adversarial robustness specifically concerns defense against intentional attacks, not performance across patient populations. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Use of Trustworthy AI Framework]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,"An AI project aims to optimize delivery routes for a logistics company. During stakeholder analysis in Phase I, the team discovers that optimizing solely for speed would consistently route deliveries through lower-income neighborhoods while avoiding affluent areas. What ethical consideration must the team address?",The technical challenge of accurately predicting traffic patterns in different neighborhoods and accounting for road infrastructure quality across the service area.,"The potential for discriminatory impact on certain communities based on the optimization algorithm's design, disproportionately affecting lower-income residents.",The need for additional GPS hardware and IoT sensors to track delivery vehicles more precisely across all neighborhoods in the service coverage area.,The cost-benefit analysis of upgrading the fleet to electric vehicles to reduce environmental impact across all delivery routes in affected communities.,B,"Option A is incorrect because predicting traffic patterns is a technical performance consideration, not an ethical one. The ethical issue is about discriminatory impact on communities, not prediction accuracy. Option B is correct. The scenario reveals a potential discriminatory impact on lower-income neighborhoods, which is an ethical consideration regarding fairness and affected populations. Ethical AI considerations in Phase I require identifying who might be harmed by the system's design. Option C is incorrect because GPS hardware and IoT sensors are infrastructure and resource requirements, not ethical considerations. Additional tracking equipment does not address the routing bias. Option D is incorrect because fleet upgrades to electric vehicles are a sustainability and cost consideration unrelated to the ethical issue of disparate routing patterns affecting specific communities. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,A project team is developing an AI system to predict employee performance for promotion decisions. What activity in Phase I would best help the team identify potential ethical considerations related to historical bias?,Selecting a random forest algorithm because of its high accuracy with tabular data and interpretable feature importance outputs for stakeholder review.,Setting an acceptable model performance threshold of 0.85 AUC-ROC to ensure the prediction system meets minimum accuracy requirements before deployment.,Estimating the cloud computing costs and infrastructure budget required to train the model on the company's historical HR data across all business units.,Conducting an impact assessment that reviews historical promotion data for patterns of discrimination against protected groups and identifies affected populations.,D,"Option A is incorrect because selecting a random forest algorithm is a Phase IV: Model Development decision about modeling technique, not a Phase I activity for identifying ethical considerations. Algorithm selection does not reveal historical bias. Option B is incorrect because setting model performance thresholds belongs to the AI System Performance and Operation task group. Accuracy targets do not identify ethical risks or historical discrimination patterns. Option C is incorrect because estimating cloud computing costs is a resource planning activity under the Assess Situation task group, not an ethical analysis activity. Infrastructure budgeting does not address bias. Option D is correct. Conducting an impact assessment that reviews historical data for discrimination patterns directly addresses ethical considerations by identifying potential biases that could be perpetuated by the AI system. This is a core Phase I activity under Required Ethical AI Considerations. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,"A facial recognition system achieves 99% accuracy across all demographic groups during Phase V evaluation. However, during Phase I ethical analysis, the team determines the system will be used in public spaces without explicit consent from individuals being scanned. According to CPMAI principles, what should the team consider?",The model is technically excellent with 99% accuracy and should proceed directly to deployment in Phase VI without additional ethical review or modifications.,"The ethical consideration of consent and surveillance impacts means the team must evaluate whether the system should be built at all, regardless of its technical performance.",The accuracy metric can be adjusted downward to 95% to make the system more ethically acceptable by reducing the precision of identification in public spaces.,The consent issue should be delegated to the company's legal team to resolve independently after the model has been deployed to production environments.,B,"Option A is incorrect because technical excellence (99% accuracy) does not override fundamental ethical concerns. The CPMAI framework requires ethical considerations to be evaluated in Phase I regardless of model performance. Option B is correct. Ethical AI considerations go beyond model metrics to include fundamental questions about whether the system should be built, who benefits, and who might be harmed. Consent in public surveillance is a fundamental ethical consideration that must be addressed in Phase I, potentially affecting the Go/No-Go decision. Option C is incorrect because adjusting accuracy does not address the ethical violation of scanning people without consent. The consent issue is about the system's use, not its precision level. Option D is incorrect because ethical considerations must be addressed in Phase I as part of the Trustworthy AI Requirements, not delegated to another team after deployment. Deferring ethics undermines the CPMAI framework's proactive approach. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,"A credit scoring AI project reveals during Phase I ethical analysis that the model would disproportionately deny loans to applicants from certain postal codes, even if the model is technically accurate. The team explores mitigation strategies but finds no way to eliminate the disparate impact while maintaining business viability. What should the outcome of the AI Go/No-Go decision be, according to CPMAI principles?","Go, because the model meets all technical performance thresholds and will maximize shareholder value through more efficient lending operations and reduced default rates.","Go, but with a public relations campaign to explain the lending decisions to affected communities and manage reputational risk through transparent communication.","No-Go, because ethical risks that cannot be adequately mitigated can and should result in a No-Go decision under the CPMAI framework's feasibility assessment.",Defer the decision to Phase VI: Model Operationalization and monitor the actual impact on affected communities after deployment before making a final determination.,C,"Option A is incorrect because meeting technical performance thresholds does not justify proceeding when ethical risks cannot be mitigated. The CPMAI framework considers ethical feasibility alongside business and technical feasibility in the Go/No-Go decision. Option B is incorrect because a public relations campaign addresses perception management, not the substantive ethical harm of disparate lending impact. Managing reputation does not resolve the underlying discriminatory outcome. Option C is correct. The CPMAI framework explicitly connects ethical considerations to the AI Go/No-Go decision. When ethical risks cannot be adequately mitigated, a No-Go decision is the appropriate outcome ‚Äî the project should not proceed regardless of technical or financial merit. Option D is incorrect because deferring to Phase VI would mean deploying a system already known to have unacceptable ethical impacts. The Go/No-Go decision must be made in Phase I, not after deployment. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,AI Failure Modes,"A fraud detection model is deployed and performs well for six months. Gradually, it begins missing new types of fraudulent transactions that differ from patterns in the training data. The model's accuracy declines despite retraining on recent data. Which AI failure mode does this scenario best describe?","Concept drift, where the underlying relationship between features and the target variable has changed over time as fraud patterns evolve beyond training data.","Overfitting, where the model memorized noise and spurious correlations in the original training data instead of learning generalizable fraud detection patterns.","Adversarial attack, where bad actors intentionally manipulate transaction inputs with carefully crafted features designed to fool the model's detection algorithms.","Catastrophic forgetting, where retraining the model on new data causes it to lose previously learned knowledge about earlier types of fraudulent transactions.",A,"Option A is correct. Concept drift occurs when the statistical properties of the target variable change over time. Evolving fraud patterns represent a classic case ‚Äî the real-world relationship between transaction features and fraud has shifted, making the model's learned patterns outdated. This is a key failure mode to identify in Phase I. Option B is incorrect because overfitting relates to the model memorizing training data noise rather than learning generalizable patterns. The scenario describes evolving real-world patterns, not a training artifact. Option C is incorrect because there is no indication of intentional input manipulation. The fraud patterns are naturally evolving, not being crafted to exploit model weaknesses. Option D is incorrect because catastrophic forgetting applies to sequential learning tasks where new training overwrites old knowledge. The scenario describes changing real-world patterns, not a training methodology problem. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì AI Failure Modes]"
Phase I: Business Understanding,Trustworthy AI Requirements,AI Failure Modes,"A project team is in Phase I planning for a customer churn prediction system. The project manager suggests waiting until Phase VI to think about potential failure modes, arguing that they can't predict what will go wrong before building anything. The CPMAI professional on the team disagrees. Why is proactive failure mode analysis in Phase I essential?","Because Phase I is when the project budget is formally approved by the steering committee, and failure mode analysis requires significant dedicated funding and specialized resources.",Because PMI certification standards require a complete failure mode register with detailed risk scores to be submitted and approved before any project work can begin.,"Because failure modes can only be identified by business stakeholders during Phase I workshops, since technical team members lack the domain context needed for risk analysis.","Because identifying potential failure modes in Phase I allows the team to design data collection, model selection, and monitoring plans that address those risks from the start.",D,"Option A is incorrect because failure mode analysis is not primarily a budget-driven activity. It is a planning activity that shapes how the project is designed, not a line item requiring dedicated funding approval. Option B is incorrect because it invents a certification requirement not present in the CPMAI outline. There is no mandated failure mode register submission process in the framework. Option C is incorrect because failure modes are identified by both business and technical stakeholders collaboratively. Technical experts contribute knowledge of algorithmic risks, data risks, and system vulnerabilities that business stakeholders alone cannot identify. Option D is correct. Proactive failure mode analysis in Phase I enables the team to design the entire project lifecycle ‚Äî including data collection strategies in Phase II, model selection in Phase IV, and monitoring mechanisms in Phase VI ‚Äî to address identified risks from the start, rather than reacting to failures after deployment. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì AI Failure Modes]"
Phase I: Business Understanding,Trustworthy AI Requirements,AI Failure Modes,"During Phase I for a recommendation system, the team identifies potential feedback loops where user engagement with recommended content could increasingly narrow the content shown, amplifying biases. How should this identified failure mode directly influence downstream phases?","It should be documented in the Phase I report and set aside until Phase VI, when monitoring dashboards can detect whether the amplification pattern actually materializes.","It should inform the design of monitoring mechanisms in Phase VI specifically configured to detect this amplification pattern, with predefined mitigation plans and thresholds ready.","It should trigger an immediate No-Go decision, because feedback loops are inherently unacceptable failure modes in recommendation systems and cannot be effectively mitigated.",It should be addressed by simplifying the project scope to remove the recommendation feature entirely and replace it with a manually curated content selection approach.,B,"Option A is incorrect because identified failure modes should actively shape project planning across all downstream phases, not be passively documented and ignored until deployment. Setting it aside defeats the purpose of Phase I analysis. Option C is incorrect because it is too extreme ‚Äî identified risks can often be monitored and mitigated with appropriate safeguards. A No-Go is reserved for risks that cannot be adequately addressed, not all identified failure modes. Option B is correct. Failure modes identified in Phase I directly connect to monitoring mechanisms designed in Phase VI, ensuring the system can detect and respond to known risks like feedback loops. This demonstrates the CPMAI principle that Phase I analysis shapes the entire project lifecycle. Option D is incorrect because removing the core feature entirely is overly restrictive when the failure mode can be monitored and mitigated. The recommendation system can proceed with appropriate monitoring safeguards informed by Phase I analysis. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì AI Failure Modes]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,"A project team is initiating an AI system that will process personal data of European Union residents to personalize marketing content. During Phase I, what is the primary compliance obligation the team must identify and document?","The need to comply with the General Data Protection Regulation (GDPR) regarding consent, data rights, and lawful processing of personal data.",The requirement to achieve 95% accuracy on the personalization model's predictions before the system can be approved for production deployment.,The selection of a large language model architecture to generate personalized content that maximizes engagement metrics across customer segments.,The establishment of a cloud infrastructure budget for storing customer data securely across multiple geographic regions and availability zones.,A,"Option A is correct. GDPR is the primary regulation governing the processing of personal data of EU residents and must be identified during Phase I compliance analysis as part of the Trustworthy AI Requirements. Option B is incorrect because model accuracy thresholds belong to the AI System Performance and Operation task group, not compliance with regulations and laws. Option C is incorrect because model architecture selection is a Phase IV: Model Development decision about modeling technique, not a Phase I compliance requirement. Option D is incorrect because cloud infrastructure budgeting is a resource planning activity under the Assess Situation task group, not regulatory compliance. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,A healthcare provider is developing an AI system to assist in diagnosing patient conditions from medical imaging. Which regulatory framework must the team identify during Phase I as applicable to this project?,"The Sarbanes-Oxley Act (SOX), which governs financial reporting and internal controls for publicly traded companies and their audit processes.","The Health Insurance Portability and Accountability Act (HIPAA), which governs protected health information privacy and security in healthcare settings.","The California Consumer Privacy Act (CCPA), which governs consumer data rights specifically for California residents and their personal information.","The EU Artificial Intelligence Act's provisions for general-purpose AI models, which establish risk-based compliance tiers for AI systems in European markets.",B,"Option A is incorrect because the Sarbanes-Oxley Act applies to financial reporting and internal controls for publicly traded companies, not healthcare diagnostic systems or patient data. Option B is correct. HIPAA is the primary regulation governing protected health information in the United States and applies directly to healthcare providers developing AI systems that handle patient medical imaging and diagnostic data. Option C is incorrect because while CCPA governs consumer data rights, it is not the primary healthcare regulation. A healthcare diagnostic system handling patient data falls primarily under HIPAA, not general consumer privacy law. Option D is incorrect because while the EU AI Act addresses AI systems, this scenario describes a US healthcare provider where HIPAA is the most directly applicable regulatory framework. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,A financial services AI project must comply with regulations requiring auditable records of all credit decisions. How does this Phase I compliance requirement constrain subsequent phases of the CPMAI lifecycle?,"It only affects Phase VI: Model Operationalization, where audit logs and decision records are generated and stored during system runtime in production environments.","It requires that in Phase II: Data Understanding, the team collects and stores all training data with complete provenance documentation and lineage tracking.",It has no impact on technical phases and is solely addressed through legal disclaimers and compliance statements added during the deployment and launch process.,It influences Phase IV model selection by potentially ruling out black-box models that cannot provide the explainable decision records required for regulatory audits.,D,"Option A is incorrect because compliance requirements affect multiple phases throughout the lifecycle, not just Phase VI. Audit requirements shape data collection, model selection, and monitoring ‚Äî not only runtime logging. Option B is incorrect because while Phase II data provenance matters, this option is incomplete as a description of cross-phase constraints. The compliance requirement affects model selection in Phase IV and monitoring in Phase VI in addition to data handling. Option C is incorrect because compliance deeply impacts technical phases. Requiring auditable credit decisions constrains which models can be selected, how they are trained, and how they are monitored ‚Äî legal disclaimers alone cannot satisfy regulatory requirements. Option D is correct. Compliance requirements identified in Phase I constrain all subsequent phases, including Phase IV: Model Development. Auditable credit decisions may require interpretable or explainable models, potentially ruling out opaque black-box architectures that cannot produce the required decision records. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,"A project team determines during Phase I that their AI system would violate a newly enacted data privacy regulation in a key market. The violation cannot be designed out without fundamentally changing the business objective. According to CPMAI principles, what should the outcome of the AI Go/No-Go decision be, and why?","Go, because compliance issues can typically be addressed through user agreements and terms of service that transfer liability to end users after launch.","Go, but only in markets where the regulation does not yet apply, delaying entry into the restricted market until the legal landscape becomes more favorable.","No-Go, because compliance failures identified in Phase I that cannot be mitigated can and should trigger a No-Go decision under the CPMAI feasibility framework.","Defer the decision to Phase V: Model Evaluation, when the team can better assess the actual compliance impact by testing the model against real-world regulatory scenarios.",C,"Option A is incorrect because user agreements and terms of service cannot override regulatory requirements. If a regulation prohibits certain data processing, contractual language with users does not create legal compliance. Option B is incorrect because while market selection might be a partial strategy, it does not resolve the fundamental problem ‚Äî the business objective requires the regulated market, and avoiding it changes the business case entirely. Option C is correct. The CPMAI framework explicitly connects compliance with the AI Go/No-Go decision. When compliance failures cannot be mitigated without fundamentally changing the business objective, a No-Go decision is the appropriate outcome under the feasibility assessment. Option D is incorrect because deferring to Phase V would mean proceeding through multiple phases despite a known, unmitigated compliance violation. The Go/No-Go decision must be made in Phase I before committing resources. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Transparency Considerations,A project team is defining transparency requirements for a credit scoring AI system. They document the need to know what data sources were used to train the model and how input features are processed. Which dimension of transparency does this requirement address?,"Decision transparency, which concerns how final credit decisions are presented to loan applicants and the format and content of denial or approval notifications.","Model transparency, which concerns the internal logic, feature processing, and algorithmic mechanisms of the model that transforms inputs into credit score outputs.","Data transparency, which concerns the origin, collection methods, consent basis, and handling procedures of the training data used to build the credit scoring model.","Stakeholder transparency, which concerns which parties within the organization have access to system documentation, audit reports, and model performance dashboards.",B,"Option A is incorrect because decision transparency addresses how outputs and final decisions are communicated to end users (loan applicants), not how the model internally processes features and data sources. Option B is correct. Model transparency addresses how the model processes inputs, including its internal logic, feature processing mechanisms, and algorithmic structure. The requirement to understand feature processing is a model transparency concern. Option C is incorrect because data transparency concerns the origin and handling of training data. While the requirement mentions data sources, the core focus is on how features are processed (model transparency), not just where data came from. Option D is incorrect because stakeholder transparency is not one of the core transparency dimensions in the Trustworthy AI Framework. It conflates access control and documentation distribution with the substantive transparency dimensions. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Transparency Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Transparency Considerations,"An AI system is being developed for hospital resource allocation. During Phase I transparency planning, the team recognizes that regulators will need to audit the system's decision-making, while nurses using the system simply need to know what the recommended action is. How should these different transparency needs shape the requirements?",Only the regulator's needs matter since they have legal authority; nurses should accept the system's recommendations without questioning the underlying decision-making logic.,"The team should design a single transparency report that serves all stakeholders equally, providing the same level of detail to regulators, nurses, administrators, and patients.","The requirements should specify different levels of transparency for different stakeholders: detailed audit trails for regulators and clear, actionable recommendations for nurses.",Transparency requirements should be deferred until Phase VI: Model Operationalization when actual users can provide direct input on what transparency information they need.,C,"Option A is incorrect because end-user needs are important for effective system adoption and patient safety. Nurses need to understand recommendations to apply clinical judgment, and dismissing their transparency needs undermines system effectiveness. Option B is incorrect because a one-size-fits-all transparency approach typically fails to meet diverse stakeholder needs. Regulators need audit-level detail that would overwhelm clinical staff, while nurses need actionable clarity that would be insufficient for regulatory review. Option C is correct. Different stakeholders require different levels of transparency tailored to their roles. Regulators need detailed audit trails and decision records, while end users (nurses) need clear, actionable recommendations. Phase I requirements should specify these distinctions. Option D is incorrect because transparency requirements must be defined in Phase I to guide system design across all subsequent phases. Deferring to Phase VI means building a system without transparency considerations, making retrofitting extremely difficult. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Transparency Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Transparency Considerations,A project team defines a transparency requirement in Phase I stating that the internal logic of the model must be fully inspectable by technical auditors. How does this requirement most directly influence Phase IV: Model Development?,It requires that the model be deployed on specialized hardware that supports encrypted computation and secure multi-party evaluation for audit compliance purposes.,"It mandates that all training data be made publicly available for independent review by external researchers, journalists, and affected community stakeholder groups.","It has no impact on Phase IV because transparency is addressed exclusively through post-hoc documentation and compliance reporting, not through model architecture selection.",It may rule out certain black-box model types like deep neural networks in favor of more interpretable architectures such as decision trees or logistic regression models.,D,"Option A is incorrect because hardware and encrypted computation relate to deployment infrastructure and security requirements, not to whether a model's internal logic can be inspected by auditors. Inspectability is about model architecture, not compute platform. Option B is incorrect because making training data publicly available addresses data transparency and open science, not model logic inspectability. The requirement is about understanding how the model processes data, not making the data itself publicly available. Option C is incorrect because transparency requirements deeply influence technical choices in Phase IV. Model inspectability cannot be achieved through documentation alone ‚Äî it requires selecting architectures that are inherently interpretable or can produce meaningful explanations. Option D is correct. Transparency requirements defined in Phase I directly constrain model selection in Phase IV: Model Development. If internal logic must be fully inspectable, black-box models like deep neural networks may not satisfy this requirement, steering the team toward more interpretable architectures. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Transparency Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Explainability Considerations,"A project team is defining requirements for a lending AI system. They document that when a loan application is denied, the system must provide the applicant with a specific explanation of why, such as 'insufficient credit history' rather than 'low model score.' Which distinction does this requirement best illustrate?",The distinction between transparency (understanding how the system works at a general level) and explainability (understanding why a specific individual decision was made).,"The distinction between model performance metrics and business KPI performance values, which measure technical accuracy versus actual business outcome impact respectively.","The distinction between data transparency and model transparency, which address where data originates versus how the model internally processes information to produce outputs.","The distinction between compliance with regulations and ethical AI considerations, which address legal mandates versus voluntary moral principles in AI system design respectively.",A,"Option A is correct. The scenario illustrates the key distinction between transparency (understanding how the system works at a general level) and explainability (understanding why a specific decision was made for a particular applicant). Providing 'insufficient credit history' rather than 'low model score' is an explainability requirement ‚Äî it explains a specific decision in understandable terms. Option B is incorrect because the distinction between model performance metrics and business KPIs (addressed in the AI System Performance and Operation task group) is unrelated to explaining individual lending decisions. Option C is incorrect because while data and model transparency are related concepts, the scenario specifically illustrates the transparency-versus-explainability distinction, not the difference between data and model transparency. Option D is incorrect because the scenario does not illustrate the compliance-versus-ethics distinction. Both compliance and ethics might require explainability, but the scenario focuses on defining what explainability means, not whether it is legally mandated or ethically motivated. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Explainability Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Explainability Considerations,"An insurance underwriting AI project requires explanations for individual risk score decisions to comply with state regulations. During Phase I, the team must establish explainability requirements that will guide Phase IV model selection. Which approach to explainability should the team consider requiring?",Selecting only black-box deep learning models because they typically achieve the highest predictive accuracy for complex insurance underwriting and risk assessment tasks.,Deferring all explainability considerations until Phase VI: Model Operationalization when customer complaints about unexplained decisions can inform the explanation design approach.,"Requiring post-hoc explanations generated by tools like LIME or SHAP to explain individual predictions, or selecting inherently interpretable models like decision trees.",Requiring that all training data volume be doubled to improve model predictive performance instead of investing development resources in explainability infrastructure and tooling.,C,"Option A is incorrect because selecting black-box deep learning models solely for accuracy directly contradicts the explainability requirement. If individual decisions must be explained to comply with state regulations, prioritizing model opacity over interpretability undermines the compliance goal. Option B is incorrect because explainability considerations must be established in Phase I to guide model selection in Phase IV. Deferring to Phase VI means the team may build an unexplainable system that cannot meet regulatory requirements after significant investment. Option C is correct. Post-hoc explanation tools like LIME and SHAP, or inherently interpretable models like decision trees, are established approaches to achieving individual-decision explainability. Phase I requirements should guide Phase IV toward these explainable approaches. Option D is incorrect because increasing training data volume addresses model predictive performance, not explainability. More data may improve accuracy but does not make individual predictions more explainable to regulators or customers. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Explainability Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Explainability Considerations,"A healthcare AI project will provide treatment recommendations to physicians. The team recognizes that in high-stakes medical contexts, affected individuals have a right to understand why a particular recommendation was made. What Phase I activity should the team undertake to address this?","Selecting a pre-trained large language model because it generates natural language explanations automatically, eliminating the need for separate explainability requirements.","Documenting explainability requirements specifying that the system must provide interpretable rationales for its recommendations, which will influence model selection in Phase IV.",Waiting until Phase V: Model Evaluation to test whether physicians can understand the recommendations before investing time in formal explainability requirement documentation.,Focusing exclusively on maximizing model prediction accuracy because clinical outcomes are the only consideration that matters in high-stakes healthcare AI applications.,B,"Option A is incorrect because selecting a specific technical solution (a pre-trained LLM) without proper requirements analysis bypasses the Phase I process. The team must first define what explainability means in this clinical context before selecting technology. Option B is correct. Explainability requirements must be documented in Phase I, particularly in high-stakes domains like healthcare where affected individuals have a right to understand decisions. These requirements will guide model selection in Phase IV and evaluation criteria in Phase V. Option C is incorrect because deferring explainability to Phase V means building a system without explainability guidance, risking a model that cannot provide interpretable rationales regardless of evaluation results. Requirements must precede development. Option D is incorrect because it ignores the critical explainability need in healthcare contexts. While clinical accuracy matters, physicians and patients also need to understand why specific recommendations are made to exercise informed clinical judgment. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Explainability Considerations]"