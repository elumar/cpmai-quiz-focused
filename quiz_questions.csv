phase,task_group,task,question,option_a,option_b,option_c,option_d,correct_answer,explanation
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,"A project team has started building a churn prediction model. They have gathered extensive customer data and are experimenting with various algorithms. However, they cannot clearly state what business problem they are solving or what value the model will provide. What is the most likely consequence of this approach?",The data science team will eventually be able to define the business objectives once they see which algorithm performs best on the dataset.,The project will deliver a technically sound model that fails to meet business needs or provide measurable value to the organization.,The model will likely have high accuracy on test data but low precision in production due to undefined performance requirements.,The project timeline will be shortened because the team started technical work immediately without waiting for stakeholder alignment.,B,"Option B is correct. Starting model building without clear business objectives means the team lacks a target for what constitutes value. This leads to a technically proficient solution that solves the wrong problem or creates no business value ‚Äî a key pitfall addressed in the Determine Business Objectives task. Option A is incorrect because business objectives should be the first step in Phase I, not derived after model selection; reversing this sequence violates the CPMAI lifecycle. Option C is incorrect because while undefined requirements may cause issues, the primary consequence of missing business objectives is misalignment with business needs, not a specific accuracy/precision trade-off. Option D is incorrect because skipping business objective definition typically leads to rework, scope creep, and delays ‚Äî not a shortened timeline. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,What is the primary output of the 'Determine Business Objectives' task in a cognitive project?,"A detailed list of data sources, required data quality thresholds, and data access permissions needed for the AI project.",A set of model performance metrics like F1-score and precision that define what technically acceptable model output looks like.,"A project schedule with milestones for data collection, model training, validation cycles, and stakeholder review periods.","A clear problem statement, desired business outcomes, and documented stakeholder alignment on the project's business purpose.",D,"Option A is incorrect because data source identification and quality thresholds are defined later during Phase II (Data Understanding), not during Determine Business Objectives. Option B is incorrect because model performance metrics are technical specifications defined under the AI System Performance and Operation task group, not under Determine Business Objectives. Option C is incorrect because a project schedule is part of the Assess Situation task group (Schedule Requirements), not the primary output of this task. Option D is correct. The primary output of Determine Business Objectives is a business-focused definition of what the project must achieve, including the problem statement, desired outcomes, and stakeholder alignment. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,A retailer wants to use AI to 'optimize inventory.' The project manager schedules a workshop with stakeholders. Which of the following best represents a well-defined business objective that should emerge from this workshop?,Our data science team will build a model with 95% accuracy for predicting next-week demand across all product categories in our system.,"We will reduce overstock of perishable items by 20% in Q4, leading to a 5% reduction in waste costs across regional distribution centers.",We will implement a random forest algorithm integrated with our existing ERP system to generate automated stock level predictions weekly.,"We need a real-time inventory dashboard that displays current stock levels, reorder points, and supplier lead times for all warehouses.",B,"Option A is incorrect because it defines a model performance metric (95% accuracy), which is a technical target under AI System Performance and Operation, not a business objective. Option B is correct. It states a clear, measurable business outcome (reduced overstock and waste costs) tied to specific business value ‚Äî the hallmark of a well-defined business objective. Option C is incorrect because it specifies a technical solution (algorithm and system integration) before the business objective is fully defined; implementation details belong in later phases. Option D is incorrect because it defines a tool or feature (a dashboard), not a business outcome; dashboards may support an objective but are not objectives themselves. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Objectives,"A team is defining the objective for an AI project. One member suggests, 'Our business objective is to build a highly accurate image recognition system.' Why is this statement insufficient as a business objective?",It fails to identify the target department or business unit that will use the image recognition system and measure its impact.,"It correctly focuses on the AI model's core technical performance, which is exactly what business objectives should specify first.",It describes a technical objective ‚Äî specifying what the AI system will do ‚Äî rather than a business objective that states the business outcome.,It should include the specific dataset and training infrastructure required before it can be considered a complete business objective.,C,"Option A is incorrect because while identifying the target department might add context, the fundamental problem is that the statement describes a technical capability, not a business outcome. Adding a department name would not fix this. Option B is incorrect because it represents a common misconception ‚Äî business objectives should be stated in business terms (revenue, cost, customer impact), not technical performance terms. Option C is correct. The statement describes a technical specification (build an image recognition system) rather than a business outcome (e.g., reduce product defect rates by 15%). The CPMAI framework requires business objectives to be stated in business terms. Option D is incorrect because datasets and infrastructure are resource requirements under the Assess Situation task group, not components of a business objective. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Objectives]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,"An e-commerce company's business objective is to 'improve customer retention.' They have moved into the modeling phase. Who should have defined the specific success criteria for this objective, and when?","The data scientists, during the modeling phase, by selecting the best achievable F1-score on the validation dataset as the benchmark for success.","The business stakeholders, before modeling begins, as measurable indicators of retention improvement such as subscription renewal rate targets.","The project sponsor, after the model is deployed to production, by observing whether customer behavior changed compared to the prior quarter.","The IT department, during the data integration phase, by defining which customer data fields must pass quality checks before model training.",B,"Option A is incorrect because data scientists define model performance metrics (like F1-score), not business success criteria. Success criteria measure business outcomes, not technical model performance. Option B is correct. Business success criteria are defined by business stakeholders and must be established before modeling begins. They are measurable indicators that the business objective has been met, as specified in the Determine Business Success Criteria task. Option C is incorrect because defining success criteria after deployment defeats their purpose ‚Äî they are needed upfront to guide project decisions and evaluate outcomes. Option D is incorrect because IT defines infrastructure and data quality requirements, not business success criteria. Data quality is addressed in Phase II. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,A company has defined a business objective to 'reduce customer churn by 15% within six months.' The project team needs to establish how this objective connects to measurable success criteria. Which statement best describes the relationship between business objectives and success criteria?,"Success criteria are the technical model metrics such as accuracy, precision, and recall that prove the AI system is performing as designed when measured against the test dataset in production.",Success criteria should be defined after the model is deployed to production so the team can observe actual business impact over a meaningful period before committing to specific targets.,"Success criteria are the measurable indicators that translate the business objective into concrete, verifiable outcomes ‚Äî such as tracking the actual churn rate change from 17.5% to 14.8%.","Success criteria are the data quality thresholds that must be met before the AI model can be trained, ensuring the input data supports the objective.",C,"Option A is incorrect because it confuses model performance metrics with business success criteria. Technical metrics like accuracy and precision measure the model, not the business outcome. Option B is incorrect because success criteria must be established before modeling begins, not after deployment ‚Äî they guide the project, not just evaluate it after the fact. Option C is correct. Success criteria are the measurable indicators that the business objective has been met. They translate objectives into concrete, verifiable outcomes and must be defined by business stakeholders in Phase I. Option D is incorrect because data quality thresholds are part of Phase II (Data Quality ‚Äì Verify Data Quality), not business success criteria. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,A data scientist on a project proposes that the project's success criteria be defined as achieving 92% precision on the validation set. The project manager should correct this by explaining that:,Precision is not a valid metric for classification models and should be replaced with a more comprehensive metric like the AUC-ROC curve.,The validation set is not the correct dataset for measuring final success ‚Äî the held-out test set should be used for all final performance evaluation.,"This is a model performance metric, while success criteria must be measurable business outcomes defined by business stakeholders, not the technical team.","The success criteria must be defined by the data scientist but should use a business-relevant metric instead, such as revenue impact or cost savings.",C,"Option A is incorrect because precision is a valid and commonly used classification metric. The issue is not the metric itself but rather that a model metric is being proposed as a business success criterion. Option B is incorrect because while using the test set for final evaluation is technically correct, this misses the fundamental problem ‚Äî the data scientist is confusing a model metric with a business success criterion. Option C is correct. The CPMAI framework explicitly distinguishes between model performance metrics (technical, like precision) and business success criteria (business outcomes, like cost reduction). Success criteria must be defined by business stakeholders, not data scientists. Option D is incorrect because success criteria ownership belongs to business stakeholders, not data scientists. Even suggesting a business-relevant metric would not fix the ownership issue. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Determine Business Success Criteria,A bank's business objective is to 'increase the efficiency of loan processing.' Which of the following is the most appropriate business success criterion for this objective?,"The AI model correctly classifies 95% of loan applications into approved, denied, or review-needed categories based on the test dataset.","The average time to process a loan application decreases from 5 days to 3 days, as measured by the operations team's tracking system.",The model's false positive rate for fraud detection during loan processing is reduced to below 1% across all application channels.,The new AI system integrates with the legacy CRM without any data migration errors and processes all records within the nightly batch window.,B,"Option A is incorrect because model classification accuracy is a technical performance metric, not a direct measure of business process efficiency. Option B is correct. This option provides a direct, measurable business indicator of efficiency ‚Äî a reduction in processing time ‚Äî which is exactly what business success criteria should measure. Option C is incorrect because fraud detection false positive rate is a model performance metric for a different objective; the stated objective is processing efficiency, not fraud reduction. Option D is incorrect because system integration and data migration are technical implementation criteria, not measures of the business outcome of improved processing efficiency. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Determine Business Success Criteria]"
Phase I: Business Understanding,Determine Business Objectives,Cost-Benefit Analysis,"A project manager is preparing a proposal for an AI initiative to automate claims processing at an insurance company. She needs to determine whether the project is financially viable before presenting it to the steering committee. In which phase should this financial assessment occur, and what decision does it inform?","Phase I: Business Understanding, to inform the AI Go/No-Go decision by determining whether the project's expected benefits justify its estimated costs.","Phase II: Data Understanding, to assess whether the cost of acquiring and preparing the required data is justified by the project's expected outcomes.","Phase IV: Model Development, to evaluate whether the computational cost of training the selected model is within the approved project budget.","Phase III: Data Preparation, to determine whether the cost of cleaning, labeling, and transforming the data is proportionate to expected business gains.",A,"Option A is correct. Cost-Benefit Analysis is explicitly a Phase I (Business Understanding) activity under the Determine Business Objectives task group. Its purpose is to assess whether the project is worth pursuing from a financial and resource perspective, directly feeding into the AI Go/No-Go decision. Option B is incorrect because while data costs are a consideration, the overall cost-benefit analysis occurs in Phase I, not Phase II. Phase II focuses on understanding the data, not assessing project-level financial viability. Option C is incorrect because computational training costs are an operational detail within Phase IV, not a strategic cost-benefit assessment. The CBA must happen before model development begins. Option D is incorrect because data preparation costs are Phase III activities; the strategic financial assessment must occur in Phase I before any data work begins. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Cost-Benefit Analysis]"
Phase I: Business Understanding,Determine Business Objectives,Cost-Benefit Analysis,A healthcare provider is considering an AI project to automate patient intake. What elements should be included in the cost-benefit analysis during Phase I?,A comprehensive user interface design specification with wireframes and user journey maps that demonstrates how the AI system will integrate into the intake workflow.,A detailed comparison of specific machine learning algorithms (random forest vs. neural network) to determine which approach delivers the best performance-to-cost ratio.,The accuracy score of the final trained model benchmarked against the existing manual intake process to quantify the improvement margin in percentage terms.,"Estimated costs for data acquisition, ML talent, cloud infrastructure, and ongoing maintenance, weighed against expected benefits like reduced administrative hours and improved patient throughput.",D,"Option D is correct. A proper cost-benefit analysis includes a comprehensive view of estimated costs (data, infrastructure, talent, maintenance) and expected benefits (revenue gains, cost savings, efficiency improvements), with a risk-adjusted ROI assessment. This feeds into the Go/No-Go decision. Option B is incorrect because algorithm comparison is a Phase IV (Model Development) activity under Select Modeling Technique. Cost-benefit analysis assesses project-level viability, not algorithm-level trade-offs. Option C is incorrect because model accuracy scores are not available during Phase I ‚Äî no model has been built yet. CBA uses estimated benefits, not measured model performance. Option A is incorrect because UI design is an implementation detail that occurs much later in the project. Cost-benefit analysis assesses financial viability, not system design specifications. [Maps to: Phase I ‚Äì Determine Business Objectives ‚Äì Cost-Benefit Analysis]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A project team has a business objective to 'automatically categorize customer support tickets by urgency and topic.' They begin defining cognitive requirements. One team member proposes: 'We need a Python-based microservice using TensorFlow with a REST API.' Why is this proposal problematic?,It correctly identifies the technology stack but fails to specify the minimum accuracy threshold that the microservice must achieve in production.,"It focuses on technical implementation details rather than the cognitive capabilities the AI system needs to demonstrate, such as natural language understanding.",It is too narrowly focused on one programming language when the team should evaluate multiple technology options before committing to a platform.,"It omits important deployment considerations like cloud hosting requirements, auto-scaling policies, and API rate limiting that should accompany technical specs.",B,"Option B is correct. Cognitive requirements define the cognitive capabilities the system needs (e.g., natural language understanding, classification by urgency) ‚Äî not the technology stack. The proposal describes technical implementation, not cognitive requirements. Option A is incorrect because the issue is not about missing accuracy thresholds ‚Äî the entire proposal is the wrong type of requirement. Accuracy thresholds belong under AI System Performance and Operation. Option C is incorrect because while evaluating multiple technologies is good practice, the fundamental problem is that the proposal defines a technical architecture rather than a cognitive capability. Broadening the tech options doesn't fix this. Option D is incorrect because deployment details like hosting and scaling are operational concerns for Phase VI, not cognitive requirements in Phase I. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A bank's business objective is to 'reduce fraudulent credit card transactions.' Which of the following correctly translates this business objective into a cognitive requirement?,The system must integrate with the bank's existing fraud management platform and process transactions within 50 milliseconds to meet SLA requirements.,"The system must generate a monthly report summarizing transaction patterns, flagged accounts, and false positive rates for the fraud operations team.",The system must be capable of real-time anomaly detection and pattern recognition in transaction data to identify potentially fraudulent activity.,"The system must store all transaction records in a secure, encrypted database that complies with PCI-DSS standards for payment card data protection.",C,"Option A is incorrect because integration requirements and processing speed are technical/system architecture specifications, not cognitive capabilities. These belong in technical requirements, not cognitive requirements. Option B is incorrect because report generation is an output/reporting requirement, not a cognitive capability that the AI system must demonstrate. Option C is correct. This translates the business objective (reduce fraud) into the cognitive capability the system needs ‚Äî real-time anomaly detection and pattern recognition. This is exactly what cognitive requirements should specify. Option D is incorrect because data storage, encryption, and compliance are infrastructure and security requirements, not cognitive capabilities. PCI-DSS compliance relates to the Trustworthy AI Requirements task group. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A project team has defined a cognitive requirement as 'the system must understand natural language in customer emails to extract intent and sentiment.' Which downstream phase is MOST directly affected by this specific cognitive requirement?,"Phase V: Model Evaluation, because the evaluation team will need to define fairness metrics that measure whether sentiment analysis is equitable across demographics.","Phase VI: Model Operationalization, because the operations team will need to build a monitoring dashboard that tracks sentiment classification accuracy in real time.","Phase IV: Model Development, because the modeling team will need to select NLP-specific algorithms and architectures that can handle intent extraction and sentiment analysis.","Phase II: Data Understanding, because the data team will need to collect labeled text corpora with annotated intent categories and sentiment ratings for model training.",D,"Option A is incorrect because while fairness evaluation matters, it is not the most direct downstream impact. The cognitive requirement first drives what data is needed before any evaluation occurs. Option B is incorrect because monitoring is important but comes after the model is built; the cognitive requirement must first shape data collection and model selection. Option C is incorrect because while Phase IV model selection is directly affected, Phase II comes first in the lifecycle ‚Äî the team needs labeled text data before they can select or train NLP models. Option D is correct. The cognitive requirement for NLP-based intent and sentiment extraction most directly drives Phase II (Data Understanding), where the team must collect labeled text corpora with annotated intents and sentiments. Data needs are the first downstream decision affected. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,Cognitive Requirements,A team is working on a project to predict equipment failures in a manufacturing plant. The business objective is to 'reduce unplanned downtime by 30%.' Which of the following correctly represents the cognitive requirement for this project?,The ability to perform predictive modeling and time-series analysis on sensor data to identify patterns that precede equipment failures before they occur.,"The installation of IoT sensors on all critical manufacturing equipment to collect vibration, temperature, and pressure data at five-second intervals.",The development of a mobile application that alerts maintenance technicians in real time when the AI system predicts an equipment failure is imminent.,The integration of the prediction system with the plant's existing SAP maintenance module to automatically generate work orders for preventive service.,A,"Option A is correct. Predictive modeling and time-series analysis on sensor data represent the cognitive capability the system needs to demonstrate ‚Äî this directly translates the business objective (reduce downtime) into the type of intelligence required. Option B is incorrect because IoT sensor installation is a data infrastructure requirement, not a cognitive capability. Sensors provide the data, but the cognitive requirement is about what the system does with that data. Option C is incorrect because a mobile alert application is a user interface and notification requirement, not a cognitive capability of the AI system itself. Option D is incorrect because SAP integration and automated work order generation are system integration requirements that belong in later phases, not cognitive requirements in Phase I. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì Cognitive Requirements]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,"A project's cognitive requirement is to 'predict the future sales volume of a product for the next 12 months based on historical sales data, seasonality, and promotional events.' Which AI pattern is the most appropriate match for this cognitive requirement?","Classification, because the system needs to categorize future sales into predefined volume brackets such as high, medium, and low demand levels.","Clustering, because the system should group similar sales periods together to identify recurring seasonal patterns across product categories.","Regression or time-series forecasting, because the system needs to predict a continuous numerical output (sales volume) across a future time horizon.","Anomaly detection, because the system must identify unusual spikes or drops in sales volume that deviate from established historical patterns.",C,"Option A is incorrect because classification assigns discrete categories, but the requirement is to predict a continuous numerical value (sales volume), not to categorize it into brackets. Option B is incorrect because clustering groups similar data points for pattern discovery, but the requirement is to predict a specific future value, not to discover groupings in historical data. Option C is correct. Predicting a continuous numerical value (sales volume) over a future time horizon is a regression or time-series forecasting problem. This directly matches the cognitive requirement. Option D is incorrect because anomaly detection identifies unusual deviations, but the requirement is to predict expected future values, not to flag anomalies in existing data. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,"A team has a business objective to 'personalize product recommendations for website visitors based on their browsing history and purchase behavior.' They correctly identified the AI pattern as 'Recommendation System' in Phase I, but a junior data scientist argues the pattern should be identified later during model development. Why is this a problem?","Delaying pattern identification until Phase IV means the team may collect and prepare the wrong type of data in Phases II and III, wasting significant effort and resources.",Identifying the pattern early locks the team into a single approach and prevents them from exploring alternative algorithms during the model development phase.,The AI pattern should be identified during Phase V: Model Evaluation so the team can compare multiple patterns against the actual model performance results.,Delaying pattern identification has no practical impact because modern AutoML tools can automatically determine the correct pattern from any sufficiently large dataset.,A,"Option A is correct. AI Pattern Identification is a Phase I activity because the selected pattern drives data collection (Phase II) and data preparation (Phase III). Delaying it to Phase IV means the team may collect and prepare data unsuitable for the correct pattern, cascading into wasted effort across multiple phases. Option B is incorrect because identifying the pattern early does not lock the team into a single algorithm ‚Äî the pattern (recommendation) is different from the specific algorithm. Multiple algorithms can implement the same pattern. Option C is incorrect because Phase V evaluates model results against pre-established criteria; it does not determine which AI pattern to use. Pattern identification must occur in Phase I. Option D is incorrect because while AutoML can assist with algorithm selection, it cannot compensate for fundamentally wrong data collected because the pattern was not identified upfront. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,A team is defining a project to 'detect unusual patterns in network traffic that may indicate cybersecurity threats.' They have identified the cognitive requirement as 'real-time anomaly detection in high-volume streaming data.' Why is identifying the AI pattern (anomaly detection) in Phase I critical for this project?,It ensures the team focuses on collecting the right type of data in Phase II ‚Äî such as labeled normal and anomalous network traffic logs with appropriate temporal granularity.,"The AI pattern should actually be identified during Phase IV: Model Development, after the team has explored the data and determined which algorithms are most effective.",Identifying the AI pattern in Phase I is a formality that has little practical impact because experienced data scientists can adapt any modeling approach to fit the data.,"It ensures that the Phase I project plan includes a cost-benefit analysis specifically for anomaly detection infrastructure, which is required before the Go/No-Go decision.",A,"Option A is correct. Identifying anomaly detection as the AI pattern in Phase I ensures that Phase II data collection focuses on the right data ‚Äî labeled network traffic with normal and anomalous examples, appropriate temporal granularity, and sufficient volume. Phase I pattern identification guides all subsequent data and modeling decisions. Option B is incorrect because the CPMAI framework places AI Pattern Identification in Phase I, not Phase IV. Deferring to Phase IV risks collecting wrong data in Phase II and preparing it incorrectly in Phase III. Option C is incorrect because AI Pattern Identification is a substantive Phase I activity, not a formality. The selected pattern drives data collection, preparation, model selection, and evaluation criteria across the entire lifecycle. Option D is incorrect because while cost-benefit analysis is a Phase I activity, it assesses overall project viability ‚Äî it is not specifically triggered by or dependent on the AI pattern selection. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Cognitive Project Requirements,AI Pattern Identification,A team's business objective is to 'automatically assess the sentiment of customer reviews and route negative reviews to the retention team within one hour.' The cognitive requirement is 'natural language understanding to classify review sentiment.' Which AI pattern is the correct match?,"Regression, because the system needs to assign a continuous sentiment score from negative to positive on a numerical scale for each customer review.","Clustering, because the system should group reviews into natural categories based on the language patterns and topics discussed by customers.","Anomaly detection, because the system must identify reviews that deviate significantly from the typical positive sentiment baseline of the customer population.","Classification, because the system needs to assign each review to a discrete sentiment category (positive, negative, neutral) to enable routing decisions.",D,"Option A is incorrect because while sentiment can be scored numerically, the business requirement is to route negative reviews ‚Äî this requires discrete categorization (positive/negative/neutral), not a continuous score. The routing decision is inherently categorical. Option B is incorrect because clustering discovers natural groupings in unlabeled data, but the requirement specifies predefined sentiment categories. This is a supervised classification task, not unsupervised clustering. Option C is incorrect because anomaly detection identifies unusual deviations from normal patterns, but sentiment classification assigns every review to a category ‚Äî it is not looking for outliers. Option D is correct. Classifying each review into a discrete sentiment category (positive, negative, neutral) is a classification task. This directly matches the cognitive requirement and enables the business process of routing negative reviews to the retention team. [Maps to: Phase I ‚Äì Cognitive Project Requirements ‚Äì AI Pattern Identification]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,A project team is defining resource requirements for a new predictive maintenance system. The project manager needs to secure budget for specialized cloud-based GPU instances to train deep learning models on sensor data. Which resource category does this requirement primarily fall under?,Technology resources ‚Äî the need for specific computational infrastructure such as cloud-based GPU instances to support model training.,Human resources ‚Äî the need to hire additional data scientists with deep learning expertise to build and tune the prediction models.,Data resources ‚Äî the requirement for high-frequency sensor data streams from manufacturing equipment to train the predictive models.,"Financial resources ‚Äî the overall project budget allocation that must cover infrastructure, personnel, data acquisition, and maintenance.",A,"Option A is correct. The requirement for cloud-based GPU instances is a specific type of computational infrastructure, which falls under the technology resources category. Resource requirements in Phase I must categorize needs across human, data, technology, and financial dimensions. Option B is incorrect because it describes human expertise and staffing needs, not the computational hardware being requested. Option C is incorrect because it describes the data itself that will be used for training, not the technology infrastructure needed to process it. Option D is incorrect because while the GPU cost is part of the overall financial budget, the requirement being defined here is for a specific technology asset, not the financial category as a whole. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,"A retail company is planning an AI project to optimize inventory across 500 stores. The initial resource plan includes one part-time data scientist and assumes all necessary data is readily available in a clean data warehouse. Six weeks into the project, the team discovers that inventory data is spread across legacy systems in different formats, requiring significant manual effort to clean and integrate. What is the primary reason for this project setback?","The business objective was not clearly defined by stakeholders, leading to misaligned expectations about the scope and complexity of the work required.","The team underestimated the data resources required, specifically the effort needed for data access, cleaning, and integration across legacy systems.","The AI pattern was incorrectly identified during Phase I, which caused the team to prepare for the wrong type of modeling approach and data structure.","The schedule requirements did not account for adequate stakeholder review periods, which delayed critical decisions about data source prioritization.",B,"Option A is incorrect because while clear business objectives are important, the setback described stems from a resource planning failure ‚Äî underestimating data complexity ‚Äî not from unclear objectives. Option B is correct. This scenario directly illustrates the common failure mode of underestimating resource requirements, specifically the effort and complexity associated with data resources (access, cleaning, integration across legacy systems). Realistic resource assessment is critical in Phase I. Option C is incorrect because the problem is data availability and quality, not the wrong AI pattern selection. The team would face this data challenge regardless of the pattern chosen. Option D is incorrect because the issue is insufficient resources allocated for data work, not a missed stakeholder review. The schedule failed because the resource estimate was unrealistic. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,"A healthcare startup has a strong business case for an AI-driven diagnostic tool with a projected high ROI. They have secured funding and have enthusiastic business stakeholders. However, their assessment reveals that the only data available is a small set of unstructured physician notes, and they have no budget to acquire or label additional data. According to CPMAI principles, what should the outcome of the AI Go/No-Go decision be, and why?","Go, because the strong business case, secured funding, and stakeholder support are sufficient to outweigh any data limitations in the current plan.","Go, but with a revised schedule that extends the project timeline to allow the team to gradually accumulate and label additional data over time.","No-Go, because the business objective needs to be redefined to align with a less data-intensive approach that can work with the available physician notes.","No-Go, because insufficient data resources make the project infeasible to execute successfully, regardless of the strength of the business case.",D,"Option A is incorrect because a strong business case cannot compensate for a fundamental lack of data resources. The Go/No-Go decision requires business, data, AND execution feasibility ‚Äî all three must pass. Option B is incorrect because extending the timeline does not solve the core problem ‚Äî the startup has no budget to acquire or label additional data, so more time alone will not produce the necessary data resources. Option C is incorrect because while redefining the objective might eventually help, the immediate infeasibility is due to the lack of data resources, not a poorly defined objective. The objective itself is valid. Option D is correct. The AI Go/No-Go decision requires all three feasibility dimensions (business, data, execution) to pass. Even with strong business feasibility, the lack of necessary data resources means data feasibility fails, resulting in a No-Go decision. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Resource Requirements,"A project manager is tasked with identifying resource requirements for an AI project. To ensure a comprehensive and realistic assessment, who should the project manager involve in this process?","Only the data science team lead, as they have the deepest understanding of technical needs like compute infrastructure, model complexity, and data volume.","Only the business sponsor, as they control the budget allocation and have final authority over which resources are approved for the project.","Only the IT infrastructure team, as they manage the cloud platforms, on-premise servers, and networking resources the AI system will require.","Both business stakeholders to define needs and expected outcomes, and technical stakeholders to estimate the effort, talent, and assets required.",D,"Option A is incorrect because while the data science team understands technical needs, they may not fully understand business requirements, budget constraints, or organizational readiness ‚Äî leading to an incomplete assessment. Option B is incorrect because while the business sponsor controls the budget, they typically lack the technical expertise to estimate infrastructure, talent, and data resource needs accurately. Option C is incorrect because the IT infrastructure team manages technology resources but does not have visibility into business objectives, data science talent needs, or domain expertise requirements. Option D is correct. Resource assessment is a project management activity that requires input from both business stakeholders (who define what is needed and the expected outcomes) and technical stakeholders (who estimate the effort, talent, and assets required to deliver). This dual input ensures a comprehensive and realistic assessment. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Resource Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"An executive sponsor for an AI project demands that the team commit to a fixed launch date of December 31st, with no room for deviation, arguing that traditional software projects can meet fixed deadlines. The project manager explains that AI projects have inherent uncertainty that makes fixed deadlines problematic. What is the primary source of this uncertainty that the project manager should cite?","The unpredictable nature of model performance, which may require multiple iterations of experimentation and validation to achieve acceptable results.","The potential for key team members to leave the project unexpectedly, creating knowledge gaps that delay critical development milestones and deliverables.",The risk of budget cuts from the finance department that could reduce available compute resources and force the team to scale back the project scope.,"The possibility that a competitor will launch a similar AI product first, forcing the team to reprioritize features and adjust the project roadmap.",A,"Option A is correct. A key characteristic of AI projects is the inherent uncertainty in model performance. It is not known in advance whether the data will support the required performance level, making iterative experimentation necessary. The CPMAI framework acknowledges this through its iterative structure, which is why fixed deadlines are problematic for AI projects. Option B is incorrect because team turnover is a general project risk that applies to all projects, not a source of uncertainty unique to AI development. Option C is incorrect because budget cuts are a financial risk that affects all project types equally; this is not the AI-specific uncertainty the project manager should cite. Option D is incorrect because competitive pressure is a market risk, not a source of technical uncertainty in AI model development. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"A team is developing the initial schedule for a customer churn prediction project. Which of the following activities must be explicitly considered as part of the schedule requirements during Phase I, due to the iterative nature of AI projects?",The final marketing campaign launch date to target at-risk customers with personalized retention offers based on the model's churn predictions.,"The procurement process for office furniture, desk equipment, and workspace setup for any new team members joining the project.","Multiple iterations for data acquisition, model experimentation, validation cycles, and stakeholder review periods throughout the project lifecycle.","The specific dates and calendar invitations for daily stand-up meetings, sprint retrospectives, and weekly status reports to stakeholders.",C,"Option A is incorrect because a marketing campaign launch date is a downstream business activity that depends on the AI model's output; it is not a schedule requirement for the AI project's development lifecycle. Option B is incorrect because office furniture procurement is an administrative task unrelated to the AI project lifecycle. While supporting new team members matters, it is not a key schedule consideration for AI projects. Option C is correct. Schedule requirements for AI projects must explicitly account for iterative cycles ‚Äî including data acquisition timelines, model experimentation iterations, validation cycles, and stakeholder review periods. The CPMAI framework acknowledges that AI projects are inherently iterative and schedules must reflect this. Option D is incorrect because daily stand-ups and sprint ceremonies are project management rituals, not the key phase milestones and iterative cycles that drive AI project schedule requirements. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"A project team has an aggressive launch deadline. Their initial resource assessment shows they have a limited number of data scientists. The schedule requirements task reveals that extensive data labeling is needed, which is very time-consuming. What is the most realistic trade-off the team should consider to meet the business need?","Remove the data labeling step entirely and use the raw, unlabeled data for model training, accepting whatever accuracy level the unsupervised approach can achieve.",Accept the original timeline as-is and hope the existing data scientists can work faster than estimated to complete the labeling within the current schedule.,"Increase resources by hiring or contracting additional personnel specifically for data labeling, thereby shortening the labeling timeline while maintaining quality.","Delay the project indefinitely until more data scientists become available through the company's internal hiring pipeline, then restart the planning process.",C,"Option A is incorrect because removing a necessary step like data labeling would likely produce an unusable or significantly degraded model, undermining the entire project. Data preparation quality directly affects model performance. Option B is incorrect because hoping team members will work faster than estimated is not a realistic planning strategy. The CPMAI framework requires realistic resource and schedule assessments. Option C is correct. This illustrates the direct interaction between schedule requirements and resource requirements ‚Äî the time-resource trade-off. To meet an aggressive timeline with a fixed workload, the team can add resources (hire/contract labelers) to compress the schedule. This is a core planning consideration in Phase I. Option D is incorrect because indefinite delay is not a trade-off to meet the original business need ‚Äî it abandons the timeline entirely. While delaying is sometimes necessary, the question asks for a trade-off to meet the aggressive deadline. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,Assess Situation,Schedule Requirements,"A project team has completed the Assess Situation tasks for a new AI project. They have documented that the project will require six months and a specific set of resources. According to the CPMAI outline, how do these documented schedule and resource requirements directly influence the next steps in Phase I?",They are filed in the project repository for reference during the project closure reports and lessons learned review in Phase VI.,"They feed into the AI Go/No-Go decision as inputs to feasibility assessment, and they are incorporated into the project plan that governs Phases II through VI.",They are used to determine the business success criteria by establishing what outcomes are achievable within the documented time and resource constraints.,They are sent directly to the procurement department to initiate hardware purchases and vendor contract negotiations for the required infrastructure.,B,"Option B is correct. The outputs of the Assess Situation tasks (Resource Requirements and Schedule Requirements) directly feed into two downstream Phase I activities: the AI Go/No-Go decision (where they inform execution feasibility) and the Produce Project Plan task group (where they become part of the formal plan for Phases II through VI). Option A is incorrect because these requirements are used for active planning and decision-making within Phase I, not archived for project closure. The project closure report is a Phase VI activity. Option C is incorrect because business success criteria are determined in the Determine Business Objectives task group, which is a separate and earlier task group within Phase I. Schedule and resource requirements do not define success criteria. Option D is incorrect because while procurement may eventually occur, sending requirements to procurement is an operational action, not the direct next step in the Phase I CPMAI workflow. The requirements must first inform the Go/No-Go decision. [Maps to: Phase I ‚Äì Assess Situation ‚Äì Schedule Requirements]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,A project team is defining acceptable model performance values for a new fraud detection system. When should these technical thresholds be established according to the PMI-CPMAI framework?,"During Phase I: Business Understanding, before any data is collected or models are built, so the project has clear technical targets from the start.","During Phase IV: Model Development, after the team has had a chance to experiment with different algorithms and evaluate initial model behavior.","During Phase V: Model Evaluation, when the model is tested against a holdout dataset to measure its actual performance against business requirements.","During Phase VI: Model Operationalization, after deployment to production so thresholds can be calibrated against real-world operating conditions.",A,"Option A is correct. Acceptable model performance values are set during Phase I: Business Understanding, establishing the target thresholds before any modeling begins. This ensures the project has clear technical goals from the outset that guide all subsequent work. Option B is incorrect because Phase IV: Model Development builds models against predefined targets; it does not establish those targets. Setting thresholds after experimentation begins risks anchoring to what is achievable rather than what the business needs. Option C is incorrect because Phase V: Model Evaluation evaluates model results against thresholds that should have been defined much earlier in Phase I. Phase V is the checkpoint, not the definition point. Option D is incorrect because Phase VI: Model Operationalization monitors performance against thresholds defined in Phase I, creating a feedback loop ‚Äî it does not establish the original thresholds. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,A healthcare AI project requires a diagnostic model with extremely low false negatives. The data science team proposes a minimum recall of 0.95 and plans to present this threshold at the next stakeholder meeting. What is the primary issue with this approach?,Recall is not an appropriate metric for diagnostic models in healthcare applications and should be replaced with a domain-specific evaluation measure.,The data science team is unilaterally defining acceptable performance thresholds without sufficient input from business stakeholders who understand acceptable risk.,The threshold of 0.95 recall is too aggressive and almost certainly unachievable given the complexity and noise present in typical healthcare datasets.,"The team should be presenting precision targets instead of recall for this use case, since false positives are the primary concern in medical diagnostics.",B,"Option A is incorrect because recall is highly appropriate for minimizing false negatives in diagnostic models ‚Äî it directly measures the rate of missed positive cases, which is critical in healthcare. Option B is correct. Acceptable model performance values must be set collaboratively by business stakeholders (who define acceptable risk levels) and technical experts (who advise on feasibility). The data science team defining thresholds unilaterally ignores the business context and risk tolerance that stakeholders must provide. Option C is incorrect because it makes an unsupported assumption about data feasibility without knowing the actual data characteristics. A 0.95 recall target may or may not be achievable. Option D is incorrect because recall is the appropriate priority when false negatives (missed diagnoses) are the primary concern. Precision would be prioritized when false positives are the main risk. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,"A project team is establishing acceptable model performance values for a content recommendation system. The business context requires minimizing user exposure to inappropriate content, even if it means some relevant content is filtered out. Which metric should be prioritized when setting performance thresholds for this use case?","Root Mean Square Error (RMSE), which measures the average magnitude of prediction errors for continuous rating values in recommendation systems.","Recall, which prioritizes capturing all relevant content and minimizing false negatives, even at the cost of occasionally showing inappropriate material.","F1 score, which balances precision and recall equally without giving preference to either false positives or false negatives in the filtering system.","Precision, which minimizes false positives and ensures that content shown to users has a high probability of being appropriate and relevant.",D,"Option A is incorrect because RMSE measures continuous value prediction error (e.g., rating predictions), not classification accuracy for content filtering. The business requirement is about filtering inappropriate content, which is a classification problem. Option B is incorrect because prioritizing recall would maximize content shown, which increases the risk of exposing users to inappropriate content ‚Äî the opposite of the business requirement. Option C is incorrect because F1 score gives equal weight to precision and recall, failing to prioritize the critical business requirement of minimizing inappropriate content exposure. The business explicitly accepts missing some relevant content. Option D is correct. Precision measures the proportion of positive identifications that were actually correct. Minimizing false positives (showing inappropriate content) directly addresses the business requirement, even if some relevant content is filtered out (lower recall). [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable Model Performance Values,A team establishes an acceptable F1 score of 0.85 for a classification model during Phase I. In which subsequent phase will this threshold be formally used as an evaluation benchmark?,"Phase II: Data Understanding, where the team assesses whether the available data quality and volume are sufficient to support achieving the target score.","Phase III: Data Preparation, where feature engineering decisions are guided by the performance goal to ensure the prepared data can support the threshold.","Phase IV: Model Development, where the threshold is used to tune hyperparameters and guide iterative model training toward the acceptable performance level.","Phase V: Model Evaluation, where the final model's results are formally assessed against the predefined acceptable threshold to determine readiness.",D,"Option A is incorrect because Phase II: Data Understanding assesses data availability and quality but does not formally evaluate model performance against thresholds. The threshold cannot be tested before a model exists. Option B is incorrect because Phase III: Data Preparation focuses on selecting, cleaning, and transforming data. While good preparation supports performance, Phase III does not formally evaluate models against thresholds. Option C is incorrect because while Phase IV: Model Development may use the target for guidance during training iterations, the formal go/no-go evaluation against Phase I thresholds occurs in Phase V, not Phase IV. Option D is correct. Acceptable model performance values established in Phase I become the formal evaluation benchmarks used in Phase V: Model Evaluation to determine whether the model meets predefined criteria and is ready for operationalization. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable Model Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,"A customer churn prediction model achieves 92% accuracy during Phase V evaluation, exceeding the acceptable model performance threshold of 88% set in Phase I. However, after deployment in Phase VI, the company observes no reduction in actual customer churn rates. What does this scenario most clearly illustrate?",The model performance threshold of 88% accuracy was set too low during Phase I and should have been calibrated higher to drive meaningful business outcomes.,The data science team used the wrong evaluation metric during Phase V and should have prioritized recall over accuracy to better capture at-risk customers.,Model performance metrics and business KPI performance values measure different things ‚Äî a model can meet technical thresholds without driving the intended business impact.,The project should have spent more time in Phase IV: Model Development improving the model's architecture and feature engineering before proceeding to evaluation.,C,"Option A is incorrect because setting a higher accuracy threshold would not necessarily translate to business impact. The problem is the disconnect between technical metrics and business outcomes, not the threshold level itself. Option B is incorrect because it speculates about metric choice without evidence that recall would have changed the business outcome. The fundamental issue is that technical model metrics do not guarantee business KPI movement. Option C is correct. This scenario illustrates the critical distinction between model performance metrics (technical accuracy) and KPI performance values (business impact like churn reduction). A model can perform well technically but fail to move business KPIs ‚Äî this is why both must be defined separately in Phase I. Option D is incorrect because more model development time likely would not address the disconnect between technical performance and business impact. The model already exceeded its technical threshold. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,An e-commerce company's business objective is to increase average order value. The project team has defined acceptable model performance values for a product recommendation algorithm. Which of the following represents an appropriate KPI performance value that should also be established in Phase I to measure business impact?,"Achieving an F1 score of 0.82 on the recommendation model's validation set, indicating the model accurately predicts user preferences and purchase behavior.","Increasing the average order value by 12% within six months of deployment, directly measuring the intended business outcome of the recommendation system.","Reducing model training time from four hours to under thirty minutes per cycle, improving the team's ability to iterate and deploy updates more frequently.","Processing 10,000 recommendation requests per second during peak traffic periods, ensuring the system can handle the company's highest-volume shopping events.",B,"Option A is incorrect because an F1 score is a model performance metric (technical), not a business KPI. It measures how well the model predicts, not whether the business objective (increased order value) is achieved. Option B is correct. A 12% increase in average order value is a business-level KPI that directly measures the intended business impact of the project. This connects to the business success criteria defined in the Determine Business Objectives task group. Option C is incorrect because model training time is a technical efficiency metric related to development operations, not a business outcome KPI. It measures process speed, not business impact. Option D is incorrect because request throughput is a system performance or scalability metric. While important for operations, it does not measure the business outcome of increased order value. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,A project team has established a business success criterion of reducing loan processing time by 30% during the Determine Business Objectives task. How do the Acceptable KPI Performance Values defined later in Phase I relate to this criterion?,The KPI performance values replace the business success criterion with more specific and measurable technical metrics that the data science team can directly optimize.,The KPI performance values are defined independently by the technical team and have no direct relationship to the business success criterion set by stakeholders.,"The KPI performance values should be derived from the business success criterion ‚Äî for example, measuring the actual reduction in processing time achieved post-deployment.",The KPI performance values are only relevant during Phase VI monitoring and do not need to be connected back to the original Phase I business objectives.,C,"Option A is incorrect because KPI performance values do not replace business success criteria ‚Äî they operationalize their measurement. The success criterion (30% reduction) remains the target; the KPI provides the mechanism to track whether it is being achieved. Option B is incorrect because KPI performance values are explicitly connected to business success criteria. They are not independent ‚Äî they form a measurement chain that links technical work to business outcomes. Option C is correct. Acceptable KPI performance values connect directly to the business success criteria, creating a measurement chain. The KPI (e.g., actual processing time reduction percentage) measures whether the business success criterion (30% reduction target) is being achieved after deployment. Option D is incorrect because while KPIs are monitored in Phase VI, they are defined in Phase I and must connect back to the business objectives established in the Determine Business Objectives task group. The connection is established during Phase I, not deferred. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,AI System Performance and Operation,Acceptable KPI Performance Values,"A project team defines acceptable KPI performance values during Phase I, including a target 15% reduction in manufacturing defects. In which phase are these KPIs primarily monitored, and what action do the results trigger?","Phase VI: Model Operationalization, where KPI results are tracked in production and create a feedback loop to Phase I for continuous improvement and iteration.","Phase IV: Model Development, where the team adjusts hyperparameters and retrains the model based on real-time KPI performance feedback during development iterations.","Phase II: Data Understanding, where the team collects additional manufacturing data if early analysis suggests the KPI target is unlikely to be achievable.","Phase V: Model Evaluation, where KPI values replace model performance metrics as the primary criteria for deciding whether to proceed to operationalization.",A,"Option D is incorrect because Phase V: Model Evaluation evaluates model performance against technical thresholds (accuracy, precision, recall), not long-term business KPIs. KPI monitoring requires the model to be deployed in production first. Option B is incorrect because Phase IV: Model Development occurs before deployment. KPI performance values measure real-world business impact, which cannot be assessed during model training iterations. Option C is incorrect because Phase II: Data Understanding occurs early in the lifecycle and focuses on data collection and exploration, not post-deployment KPI monitoring. Option A is correct. Acceptable KPI performance values are primarily monitored during Phase VI: Model Operationalization after the model is deployed to production. The monitoring results create a feedback loop back to Phase I, informing whether business objectives are being met and whether adjustments or a new iteration is needed. [Maps to: Phase I ‚Äì AI System Performance and Operation ‚Äì Acceptable KPI Performance Values]"
Phase I: Business Understanding,Trustworthy AI Requirements,Use of Trustworthy AI Framework,"An AI project team is developing a resume screening system to identify qualified candidates. During Phase I, they consider potential bias against certain demographic groups and establish requirements for regular bias audits. Which pillar of the Trustworthy AI Framework are they primarily addressing?","Fairness, by proactively working to prevent discrimination against protected groups and establishing ongoing bias audit requirements.","Transparency, by documenting how the system makes screening decisions so that candidates and regulators can understand the process.","Robustness, by ensuring the system performs consistently and reliably across different applicant populations and demographic groups.","Privacy, by protecting candidate personal data from unauthorized access and ensuring compliance with data protection regulations.",A,"Option A is correct. The scenario directly addresses fairness by considering potential bias against demographic groups and establishing bias audit requirements. Fairness as a Trustworthy AI pillar focuses on preventing discriminatory outcomes. Option B is incorrect because transparency concerns openness about how the system works and makes decisions, not specifically bias prevention or audit mechanisms. Option C is incorrect because robustness concerns system reliability and consistent performance under varying conditions, not the identification and prevention of discriminatory bias. Option D is incorrect because privacy concerns data protection and access controls, not discriminatory outcomes or bias in screening decisions. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Use of Trustworthy AI Framework]"
Phase I: Business Understanding,Trustworthy AI Requirements,Use of Trustworthy AI Framework,"A project team plans to build the model first, prove its technical feasibility in Phase IV, and then 'bolt on' the Trustworthy AI Framework elements during Phase VI before launch. The project manager corrects this approach, explaining that the Trustworthy AI Framework must be applied starting in which phase, and why?","Phase IV: Model Development, because that is when the model is built and fairness constraints can be coded directly into the algorithms and training process.","Phase VI: Model Operationalization, because trustworthy AI elements like monitoring for drift and bias only become relevant after the system is live in production.","Phase I: Business Understanding, because relevant trustworthy AI dimensions must be identified and requirements established before any data work or modeling begins.","Phase II: Data Understanding, because trustworthy AI is primarily about ensuring the training data is clean, representative, and free from historical biases.",C,"Option A is incorrect because waiting until Phase IV: Model Development to consider trustworthy AI is too late ‚Äî by that point, data has been collected and prepared without trustworthy AI guidance, potentially embedding bias or missing compliance requirements. Option B is incorrect because Phase VI: Model Operationalization is for monitoring and maintenance, not initial framework application. Bolting on trustworthy AI elements at deployment is the exact anti-pattern the project manager is correcting. Option C is correct. The Trustworthy AI Framework must be applied starting in Phase I: Business Understanding, where relevant dimensions (fairness, transparency, explainability, etc.) are identified and requirements established before any data work or modeling begins. This ensures trustworthy AI considerations guide all subsequent phases. Option D is incorrect because while data quality and representation are important, the framework application begins even earlier in Phase I, where the requirements are defined that will guide data collection in Phase II. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Use of Trustworthy AI Framework]"
Phase I: Business Understanding,Trustworthy AI Requirements,Use of Trustworthy AI Framework,"During Phase I for a medical diagnosis AI project, the team documents that the system must provide clear reasoning for its predictions to enable doctor review and must maintain performance across different hospital patient populations. Which two Trustworthy AI pillars are they establishing requirements for?",Accountability and Privacy ‚Äî governance structures for oversight and data protection protocols for patient medical records and personal health information.,Safety and Robustness ‚Äî physical safety protections for patients and system stability guarantees under varying clinical conditions and hospital environments.,Explainability and Fairness ‚Äî clear prediction reasoning for clinical review and equitable performance across diverse patient demographic groups.,Transparency and Adversarial Robustness ‚Äî general openness about system design and specific defenses against malicious attempts to manipulate diagnostic inputs.,C,"Option A is incorrect because accountability involves governance structures and responsibility assignment, not prediction reasoning, and privacy involves data protection rather than equitable performance across populations. Neither matches the scenario. Option B is incorrect because safety concerns physical or psychological harm prevention, and robustness concerns system stability under varying conditions ‚Äî neither specifically addresses prediction reasoning or performance equity across demographic groups. Option C is correct. Providing clear reasoning for predictions directly addresses explainability (enabling doctors to understand and review AI decisions), while maintaining performance across different patient populations addresses fairness (ensuring equitable outcomes across demographic groups). Option D is incorrect because transparency is broader than explainability (it encompasses general openness, not specifically prediction-level reasoning), and adversarial robustness specifically concerns defense against intentional attacks, not performance across patient populations. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Use of Trustworthy AI Framework]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,"An AI project aims to optimize delivery routes for a logistics company. During stakeholder analysis in Phase I, the team discovers that optimizing solely for speed would consistently route deliveries through lower-income neighborhoods while avoiding affluent areas. What ethical consideration must the team address?",The technical challenge of accurately predicting traffic patterns in different neighborhoods and accounting for road infrastructure quality across the service area.,"The potential for discriminatory impact on certain communities based on the optimization algorithm's design, disproportionately affecting lower-income residents.",The need for additional GPS hardware and IoT sensors to track delivery vehicles more precisely across all neighborhoods in the service coverage area.,The cost-benefit analysis of upgrading the fleet to electric vehicles to reduce environmental impact across all delivery routes in affected communities.,B,"Option A is incorrect because predicting traffic patterns is a technical performance consideration, not an ethical one. The ethical issue is about discriminatory impact on communities, not prediction accuracy. Option B is correct. The scenario reveals a potential discriminatory impact on lower-income neighborhoods, which is an ethical consideration regarding fairness and affected populations. Ethical AI considerations in Phase I require identifying who might be harmed by the system's design. Option C is incorrect because GPS hardware and IoT sensors are infrastructure and resource requirements, not ethical considerations. Additional tracking equipment does not address the routing bias. Option D is incorrect because fleet upgrades to electric vehicles are a sustainability and cost consideration unrelated to the ethical issue of disparate routing patterns affecting specific communities. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,A project team is developing an AI system to predict employee performance for promotion decisions. What activity in Phase I would best help the team identify potential ethical considerations related to historical bias?,Selecting a random forest algorithm because of its high accuracy with tabular data and interpretable feature importance outputs for stakeholder review.,Setting an acceptable model performance threshold of 0.85 AUC-ROC to ensure the prediction system meets minimum accuracy requirements before deployment.,Estimating the cloud computing costs and infrastructure budget required to train the model on the company's historical HR data across all business units.,Conducting an impact assessment that reviews historical promotion data for patterns of discrimination against protected groups and identifies affected populations.,D,"Option A is incorrect because selecting a random forest algorithm is a Phase IV: Model Development decision about modeling technique, not a Phase I activity for identifying ethical considerations. Algorithm selection does not reveal historical bias. Option B is incorrect because setting model performance thresholds belongs to the AI System Performance and Operation task group. Accuracy targets do not identify ethical risks or historical discrimination patterns. Option C is incorrect because estimating cloud computing costs is a resource planning activity under the Assess Situation task group, not an ethical analysis activity. Infrastructure budgeting does not address bias. Option D is correct. Conducting an impact assessment that reviews historical data for discrimination patterns directly addresses ethical considerations by identifying potential biases that could be perpetuated by the AI system. This is a core Phase I activity under Required Ethical AI Considerations. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,"A facial recognition system achieves 99% accuracy across all demographic groups during Phase V evaluation. However, during Phase I ethical analysis, the team determines the system will be used in public spaces without explicit consent from individuals being scanned. According to CPMAI principles, what should the team consider?",The model is technically excellent with 99% accuracy and should proceed directly to deployment in Phase VI without additional ethical review or modifications.,"The ethical consideration of consent and surveillance impacts means the team must evaluate whether the system should be built at all, regardless of its technical performance.",The accuracy metric can be adjusted downward to 95% to make the system more ethically acceptable by reducing the precision of identification in public spaces.,The consent issue should be delegated to the company's legal team to resolve independently after the model has been deployed to production environments.,B,"Option A is incorrect because technical excellence (99% accuracy) does not override fundamental ethical concerns. The CPMAI framework requires ethical considerations to be evaluated in Phase I regardless of model performance. Option B is correct. Ethical AI considerations go beyond model metrics to include fundamental questions about whether the system should be built, who benefits, and who might be harmed. Consent in public surveillance is a fundamental ethical consideration that must be addressed in Phase I, potentially affecting the Go/No-Go decision. Option C is incorrect because adjusting accuracy does not address the ethical violation of scanning people without consent. The consent issue is about the system's use, not its precision level. Option D is incorrect because ethical considerations must be addressed in Phase I as part of the Trustworthy AI Requirements, not delegated to another team after deployment. Deferring ethics undermines the CPMAI framework's proactive approach. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Ethical AI Considerations,"A credit scoring AI project reveals during Phase I ethical analysis that the model would disproportionately deny loans to applicants from certain postal codes, even if the model is technically accurate. The team explores mitigation strategies but finds no way to eliminate the disparate impact while maintaining business viability. What should the outcome of the AI Go/No-Go decision be, according to CPMAI principles?","Go, because the model meets all technical performance thresholds and will maximize shareholder value through more efficient lending operations and reduced default rates.","Go, but with a public relations campaign to explain the lending decisions to affected communities and manage reputational risk through transparent communication.","No-Go, because ethical risks that cannot be adequately mitigated can and should result in a No-Go decision under the CPMAI framework's feasibility assessment.",Defer the decision to Phase VI: Model Operationalization and monitor the actual impact on affected communities after deployment before making a final determination.,C,"Option A is incorrect because meeting technical performance thresholds does not justify proceeding when ethical risks cannot be mitigated. The CPMAI framework considers ethical feasibility alongside business and technical feasibility in the Go/No-Go decision. Option B is incorrect because a public relations campaign addresses perception management, not the substantive ethical harm of disparate lending impact. Managing reputation does not resolve the underlying discriminatory outcome. Option C is correct. The CPMAI framework explicitly connects ethical considerations to the AI Go/No-Go decision. When ethical risks cannot be adequately mitigated, a No-Go decision is the appropriate outcome ‚Äî the project should not proceed regardless of technical or financial merit. Option D is incorrect because deferring to Phase VI would mean deploying a system already known to have unacceptable ethical impacts. The Go/No-Go decision must be made in Phase I, not after deployment. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Ethical AI Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,AI Failure Modes,"A fraud detection model is deployed and performs well for six months. Gradually, it begins missing new types of fraudulent transactions that differ from patterns in the training data. The model's accuracy declines despite retraining on recent data. Which AI failure mode does this scenario best describe?","Concept drift, where the underlying relationship between features and the target variable has changed over time as fraud patterns evolve beyond training data.","Overfitting, where the model memorized noise and spurious correlations in the original training data instead of learning generalizable fraud detection patterns.","Adversarial attack, where bad actors intentionally manipulate transaction inputs with carefully crafted features designed to fool the model's detection algorithms.","Catastrophic forgetting, where retraining the model on new data causes it to lose previously learned knowledge about earlier types of fraudulent transactions.",A,"Option A is correct. Concept drift occurs when the statistical properties of the target variable change over time. Evolving fraud patterns represent a classic case ‚Äî the real-world relationship between transaction features and fraud has shifted, making the model's learned patterns outdated. This is a key failure mode to identify in Phase I. Option B is incorrect because overfitting relates to the model memorizing training data noise rather than learning generalizable patterns. The scenario describes evolving real-world patterns, not a training artifact. Option C is incorrect because there is no indication of intentional input manipulation. The fraud patterns are naturally evolving, not being crafted to exploit model weaknesses. Option D is incorrect because catastrophic forgetting applies to sequential learning tasks where new training overwrites old knowledge. The scenario describes changing real-world patterns, not a training methodology problem. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì AI Failure Modes]"
Phase I: Business Understanding,Trustworthy AI Requirements,AI Failure Modes,"A project team is in Phase I planning for a customer churn prediction system. The project manager suggests waiting until Phase VI to think about potential failure modes, arguing that they can't predict what will go wrong before building anything. The CPMAI professional on the team disagrees. Why is proactive failure mode analysis in Phase I essential?","Because Phase I is when the project budget is formally approved by the steering committee, and failure mode analysis requires significant dedicated funding and specialized resources.",Because PMI certification standards require a complete failure mode register with detailed risk scores to be submitted and approved before any project work can begin.,"Because failure modes can only be identified by business stakeholders during Phase I workshops, since technical team members lack the domain context needed for risk analysis.","Because identifying potential failure modes in Phase I allows the team to design data collection, model selection, and monitoring plans that address those risks from the start.",D,"Option A is incorrect because failure mode analysis is not primarily a budget-driven activity. It is a planning activity that shapes how the project is designed, not a line item requiring dedicated funding approval. Option B is incorrect because it invents a certification requirement not present in the CPMAI outline. There is no mandated failure mode register submission process in the framework. Option C is incorrect because failure modes are identified by both business and technical stakeholders collaboratively. Technical experts contribute knowledge of algorithmic risks, data risks, and system vulnerabilities that business stakeholders alone cannot identify. Option D is correct. Proactive failure mode analysis in Phase I enables the team to design the entire project lifecycle ‚Äî including data collection strategies in Phase II, model selection in Phase IV, and monitoring mechanisms in Phase VI ‚Äî to address identified risks from the start, rather than reacting to failures after deployment. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì AI Failure Modes]"
Phase I: Business Understanding,Trustworthy AI Requirements,AI Failure Modes,"During Phase I for a recommendation system, the team identifies potential feedback loops where user engagement with recommended content could increasingly narrow the content shown, amplifying biases. How should this identified failure mode directly influence downstream phases?","It should be documented in the Phase I report and set aside until Phase VI, when monitoring dashboards can detect whether the amplification pattern actually materializes.","It should inform the design of monitoring mechanisms in Phase VI specifically configured to detect this amplification pattern, with predefined mitigation plans and thresholds ready.","It should trigger an immediate No-Go decision, because feedback loops are inherently unacceptable failure modes in recommendation systems and cannot be effectively mitigated.",It should be addressed by simplifying the project scope to remove the recommendation feature entirely and replace it with a manually curated content selection approach.,B,"Option A is incorrect because identified failure modes should actively shape project planning across all downstream phases, not be passively documented and ignored until deployment. Setting it aside defeats the purpose of Phase I analysis. Option C is incorrect because it is too extreme ‚Äî identified risks can often be monitored and mitigated with appropriate safeguards. A No-Go is reserved for risks that cannot be adequately addressed, not all identified failure modes. Option B is correct. Failure modes identified in Phase I directly connect to monitoring mechanisms designed in Phase VI, ensuring the system can detect and respond to known risks like feedback loops. This demonstrates the CPMAI principle that Phase I analysis shapes the entire project lifecycle. Option D is incorrect because removing the core feature entirely is overly restrictive when the failure mode can be monitored and mitigated. The recommendation system can proceed with appropriate monitoring safeguards informed by Phase I analysis. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì AI Failure Modes]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,"A project team is initiating an AI system that will process personal data of European Union residents to personalize marketing content. During Phase I, what is the primary compliance obligation the team must identify and document?","The need to comply with the General Data Protection Regulation (GDPR) regarding consent, data rights, and lawful processing of personal data.",The requirement to achieve 95% accuracy on the personalization model's predictions before the system can be approved for production deployment.,The selection of a large language model architecture to generate personalized content that maximizes engagement metrics across customer segments.,The establishment of a cloud infrastructure budget for storing customer data securely across multiple geographic regions and availability zones.,A,"Option A is correct. GDPR is the primary regulation governing the processing of personal data of EU residents and must be identified during Phase I compliance analysis as part of the Trustworthy AI Requirements. Option B is incorrect because model accuracy thresholds belong to the AI System Performance and Operation task group, not compliance with regulations and laws. Option C is incorrect because model architecture selection is a Phase IV: Model Development decision about modeling technique, not a Phase I compliance requirement. Option D is incorrect because cloud infrastructure budgeting is a resource planning activity under the Assess Situation task group, not regulatory compliance. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,A healthcare provider is developing an AI system to assist in diagnosing patient conditions from medical imaging. Which regulatory framework must the team identify during Phase I as applicable to this project?,"The Sarbanes-Oxley Act (SOX), which governs financial reporting and internal controls for publicly traded companies and their audit processes.","The Health Insurance Portability and Accountability Act (HIPAA), which governs protected health information privacy and security in healthcare settings.","The California Consumer Privacy Act (CCPA), which governs consumer data rights specifically for California residents and their personal information.","The EU Artificial Intelligence Act's provisions for general-purpose AI models, which establish risk-based compliance tiers for AI systems in European markets.",B,"Option A is incorrect because the Sarbanes-Oxley Act applies to financial reporting and internal controls for publicly traded companies, not healthcare diagnostic systems or patient data. Option B is correct. HIPAA is the primary regulation governing protected health information in the United States and applies directly to healthcare providers developing AI systems that handle patient medical imaging and diagnostic data. Option C is incorrect because while CCPA governs consumer data rights, it is not the primary healthcare regulation. A healthcare diagnostic system handling patient data falls primarily under HIPAA, not general consumer privacy law. Option D is incorrect because while the EU AI Act addresses AI systems, this scenario describes a US healthcare provider where HIPAA is the most directly applicable regulatory framework. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,A financial services AI project must comply with regulations requiring auditable records of all credit decisions. How does this Phase I compliance requirement constrain subsequent phases of the CPMAI lifecycle?,"It only affects Phase VI: Model Operationalization, where audit logs and decision records are generated and stored during system runtime in production environments.","It requires that in Phase II: Data Understanding, the team collects and stores all training data with complete provenance documentation and lineage tracking.",It has no impact on technical phases and is solely addressed through legal disclaimers and compliance statements added during the deployment and launch process.,It influences Phase IV model selection by potentially ruling out black-box models that cannot provide the explainable decision records required for regulatory audits.,D,"Option A is incorrect because compliance requirements affect multiple phases throughout the lifecycle, not just Phase VI. Audit requirements shape data collection, model selection, and monitoring ‚Äî not only runtime logging. Option B is incorrect because while Phase II data provenance matters, this option is incomplete as a description of cross-phase constraints. The compliance requirement affects model selection in Phase IV and monitoring in Phase VI in addition to data handling. Option C is incorrect because compliance deeply impacts technical phases. Requiring auditable credit decisions constrains which models can be selected, how they are trained, and how they are monitored ‚Äî legal disclaimers alone cannot satisfy regulatory requirements. Option D is correct. Compliance requirements identified in Phase I constrain all subsequent phases, including Phase IV: Model Development. Auditable credit decisions may require interpretable or explainable models, potentially ruling out opaque black-box architectures that cannot produce the required decision records. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required Compliance With Regulations and Laws,"A project team determines during Phase I that their AI system would violate a newly enacted data privacy regulation in a key market. The violation cannot be designed out without fundamentally changing the business objective. According to CPMAI principles, what should the outcome of the AI Go/No-Go decision be, and why?","Go, because compliance issues can typically be addressed through user agreements and terms of service that transfer liability to end users after launch.","Go, but only in markets where the regulation does not yet apply, delaying entry into the restricted market until the legal landscape becomes more favorable.","No-Go, because compliance failures identified in Phase I that cannot be mitigated can and should trigger a No-Go decision under the CPMAI feasibility framework.","Defer the decision to Phase V: Model Evaluation, when the team can better assess the actual compliance impact by testing the model against real-world regulatory scenarios.",C,"Option A is incorrect because user agreements and terms of service cannot override regulatory requirements. If a regulation prohibits certain data processing, contractual language with users does not create legal compliance. Option B is incorrect because while market selection might be a partial strategy, it does not resolve the fundamental problem ‚Äî the business objective requires the regulated market, and avoiding it changes the business case entirely. Option C is correct. The CPMAI framework explicitly connects compliance with the AI Go/No-Go decision. When compliance failures cannot be mitigated without fundamentally changing the business objective, a No-Go decision is the appropriate outcome under the feasibility assessment. Option D is incorrect because deferring to Phase V would mean proceeding through multiple phases despite a known, unmitigated compliance violation. The Go/No-Go decision must be made in Phase I before committing resources. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required Compliance With Regulations and Laws]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Transparency Considerations,A project team is defining transparency requirements for a credit scoring AI system. They document the need to know what data sources were used to train the model and how input features are processed. Which dimension of transparency does this requirement address?,"Decision transparency, which concerns how final credit decisions are presented to loan applicants and the format and content of denial or approval notifications.","Model transparency, which concerns the internal logic, feature processing, and algorithmic mechanisms of the model that transforms inputs into credit score outputs.","Data transparency, which concerns the origin, collection methods, consent basis, and handling procedures of the training data used to build the credit scoring model.","Stakeholder transparency, which concerns which parties within the organization have access to system documentation, audit reports, and model performance dashboards.",B,"Option A is incorrect because decision transparency addresses how outputs and final decisions are communicated to end users (loan applicants), not how the model internally processes features and data sources. Option B is correct. Model transparency addresses how the model processes inputs, including its internal logic, feature processing mechanisms, and algorithmic structure. The requirement to understand feature processing is a model transparency concern. Option C is incorrect because data transparency concerns the origin and handling of training data. While the requirement mentions data sources, the core focus is on how features are processed (model transparency), not just where data came from. Option D is incorrect because stakeholder transparency is not one of the core transparency dimensions in the Trustworthy AI Framework. It conflates access control and documentation distribution with the substantive transparency dimensions. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Transparency Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Transparency Considerations,"An AI system is being developed for hospital resource allocation. During Phase I transparency planning, the team recognizes that regulators will need to audit the system's decision-making, while nurses using the system simply need to know what the recommended action is. How should these different transparency needs shape the requirements?",Only the regulator's needs matter since they have legal authority; nurses should accept the system's recommendations without questioning the underlying decision-making logic.,"The team should design a single transparency report that serves all stakeholders equally, providing the same level of detail to regulators, nurses, administrators, and patients.","The requirements should specify different levels of transparency for different stakeholders: detailed audit trails for regulators and clear, actionable recommendations for nurses.",Transparency requirements should be deferred until Phase VI: Model Operationalization when actual users can provide direct input on what transparency information they need.,C,"Option A is incorrect because end-user needs are important for effective system adoption and patient safety. Nurses need to understand recommendations to apply clinical judgment, and dismissing their transparency needs undermines system effectiveness. Option B is incorrect because a one-size-fits-all transparency approach typically fails to meet diverse stakeholder needs. Regulators need audit-level detail that would overwhelm clinical staff, while nurses need actionable clarity that would be insufficient for regulatory review. Option C is correct. Different stakeholders require different levels of transparency tailored to their roles. Regulators need detailed audit trails and decision records, while end users (nurses) need clear, actionable recommendations. Phase I requirements should specify these distinctions. Option D is incorrect because transparency requirements must be defined in Phase I to guide system design across all subsequent phases. Deferring to Phase VI means building a system without transparency considerations, making retrofitting extremely difficult. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Transparency Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Transparency Considerations,A project team defines a transparency requirement in Phase I stating that the internal logic of the model must be fully inspectable by technical auditors. How does this requirement most directly influence Phase IV: Model Development?,It requires that the model be deployed on specialized hardware that supports encrypted computation and secure multi-party evaluation for audit compliance purposes.,"It mandates that all training data be made publicly available for independent review by external researchers, journalists, and affected community stakeholder groups.","It has no impact on Phase IV because transparency is addressed exclusively through post-hoc documentation and compliance reporting, not through model architecture selection.",It may rule out certain black-box model types like deep neural networks in favor of more interpretable architectures such as decision trees or logistic regression models.,D,"Option A is incorrect because hardware and encrypted computation relate to deployment infrastructure and security requirements, not to whether a model's internal logic can be inspected by auditors. Inspectability is about model architecture, not compute platform. Option B is incorrect because making training data publicly available addresses data transparency and open science, not model logic inspectability. The requirement is about understanding how the model processes data, not making the data itself publicly available. Option C is incorrect because transparency requirements deeply influence technical choices in Phase IV. Model inspectability cannot be achieved through documentation alone ‚Äî it requires selecting architectures that are inherently interpretable or can produce meaningful explanations. Option D is correct. Transparency requirements defined in Phase I directly constrain model selection in Phase IV: Model Development. If internal logic must be fully inspectable, black-box models like deep neural networks may not satisfy this requirement, steering the team toward more interpretable architectures. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Transparency Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Explainability Considerations,"A project team is defining requirements for a lending AI system. They document that when a loan application is denied, the system must provide the applicant with a specific explanation of why, such as 'insufficient credit history' rather than 'low model score.' Which distinction does this requirement best illustrate?",The distinction between transparency (understanding how the system works at a general level) and explainability (understanding why a specific individual decision was made).,"The distinction between model performance metrics and business KPI performance values, which measure technical accuracy versus actual business outcome impact respectively.","The distinction between data transparency and model transparency, which address where data originates versus how the model internally processes information to produce outputs.","The distinction between compliance with regulations and ethical AI considerations, which address legal mandates versus voluntary moral principles in AI system design respectively.",A,"Option A is correct. The scenario illustrates the key distinction between transparency (understanding how the system works at a general level) and explainability (understanding why a specific decision was made for a particular applicant). Providing 'insufficient credit history' rather than 'low model score' is an explainability requirement ‚Äî it explains a specific decision in understandable terms. Option B is incorrect because the distinction between model performance metrics and business KPIs (addressed in the AI System Performance and Operation task group) is unrelated to explaining individual lending decisions. Option C is incorrect because while data and model transparency are related concepts, the scenario specifically illustrates the transparency-versus-explainability distinction, not the difference between data and model transparency. Option D is incorrect because the scenario does not illustrate the compliance-versus-ethics distinction. Both compliance and ethics might require explainability, but the scenario focuses on defining what explainability means, not whether it is legally mandated or ethically motivated. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Explainability Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Explainability Considerations,"An insurance underwriting AI project requires explanations for individual risk score decisions to comply with state regulations. During Phase I, the team must establish explainability requirements that will guide Phase IV model selection. Which approach to explainability should the team consider requiring?",Selecting only black-box deep learning models because they typically achieve the highest predictive accuracy for complex insurance underwriting and risk assessment tasks.,Deferring all explainability considerations until Phase VI: Model Operationalization when customer complaints about unexplained decisions can inform the explanation design approach.,"Requiring post-hoc explanations generated by tools like LIME or SHAP to explain individual predictions, or selecting inherently interpretable models like decision trees.",Requiring that all training data volume be doubled to improve model predictive performance instead of investing development resources in explainability infrastructure and tooling.,C,"Option A is incorrect because selecting black-box deep learning models solely for accuracy directly contradicts the explainability requirement. If individual decisions must be explained to comply with state regulations, prioritizing model opacity over interpretability undermines the compliance goal. Option B is incorrect because explainability considerations must be established in Phase I to guide model selection in Phase IV. Deferring to Phase VI means the team may build an unexplainable system that cannot meet regulatory requirements after significant investment. Option C is correct. Post-hoc explanation tools like LIME and SHAP, or inherently interpretable models like decision trees, are established approaches to achieving individual-decision explainability. Phase I requirements should guide Phase IV toward these explainable approaches. Option D is incorrect because increasing training data volume addresses model predictive performance, not explainability. More data may improve accuracy but does not make individual predictions more explainable to regulators or customers. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Explainability Considerations]"
Phase I: Business Understanding,Trustworthy AI Requirements,Required AI Explainability Considerations,"A healthcare AI project will provide treatment recommendations to physicians. The team recognizes that in high-stakes medical contexts, affected individuals have a right to understand why a particular recommendation was made. What Phase I activity should the team undertake to address this?","Selecting a pre-trained large language model because it generates natural language explanations automatically, eliminating the need for separate explainability requirements.","Documenting explainability requirements specifying that the system must provide interpretable rationales for its recommendations, which will influence model selection in Phase IV.",Waiting until Phase V: Model Evaluation to test whether physicians can understand the recommendations before investing time in formal explainability requirement documentation.,Focusing exclusively on maximizing model prediction accuracy because clinical outcomes are the only consideration that matters in high-stakes healthcare AI applications.,B,"Option A is incorrect because selecting a specific technical solution (a pre-trained LLM) without proper requirements analysis bypasses the Phase I process. The team must first define what explainability means in this clinical context before selecting technology. Option B is correct. Explainability requirements must be documented in Phase I, particularly in high-stakes domains like healthcare where affected individuals have a right to understand decisions. These requirements will guide model selection in Phase IV and evaluation criteria in Phase V. Option C is incorrect because deferring explainability to Phase V means building a system without explainability guidance, risking a model that cannot provide interpretable rationales regardless of evaluation results. Requirements must precede development. Option D is incorrect because it ignores the critical explainability need in healthcare contexts. While clinical accuracy matters, physicians and patients also need to understand why specific recommendations are made to exercise informed clinical judgment. [Maps to: Phase I ‚Äì Trustworthy AI Requirements ‚Äì Required AI Explainability Considerations]"
Phase I: Business Understanding,AI Go/No-Go,Business Feasibility,"A project team has demonstrated that a churn prediction model is technically achievable with 92% accuracy using available customer data. However, the cost-benefit analysis shows that implementing retention campaigns for identified at-risk customers would cost more than the expected revenue from customers retained. According to CPMAI principles, what should the AI Go/No-Go decision be based on this information?","Go, because the technical team has proven the model can be built with high accuracy.","No-Go, because the business feasibility assessment shows negative ROI despite technical feasibility.","Go, but with a revised scope to focus only on high-value customers where retention costs are lower.","No-Go, because the data quality is insufficient for regulatory compliance requirements.",B,"The correct answer is B. Option B is correct because business feasibility evaluates whether the project delivers positive business value based on ROI, not just technical capability. The negative ROI from retention campaign costs exceeding retained revenue indicates poor business feasibility, warranting a No-Go decision. Option A is incorrect because it prioritizes technical feasibility (model accuracy) over business value; high accuracy alone does not justify a project with negative returns. Option C is incorrect because, while narrowing scope might improve feasibility in theory, the question asks about the decision based on the current cost-benefit analysis results, not a hypothetical redesign. Option D is incorrect because it cites a data quality and regulatory compliance issue that is not mentioned anywhere in the scenario; the data feasibility concern is fabricated. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Business Feasibility]"
Phase I: Business Understanding,AI Go/No-Go,Business Feasibility,Which Phase I outputs serve as primary inputs to the business feasibility assessment for an AI project?,"The business objectives, success criteria, cost-benefit analysis results, and acceptable KPI performance values from Phase I task groups.",The cognitive requirements and AI pattern identification documents from the Cognitive Project Requirements task group within Phase I.,The data schema documentation and initial data quality reports generated during Phase II: Data Understanding activities.,The resource requirements and schedule requirements documents produced by the Assess Situation task group within Phase I.,A,"The correct answer is A. Option A is correct because business feasibility directly uses business objectives, success criteria, cost-benefit analysis results, and acceptable KPI performance values to determine whether the project makes business sense. These are all outputs from the Determine Business Objectives and AI System Performance and Operation task groups within Phase I. Option B is incorrect because cognitive requirements and AI pattern identification inform the technical approach, not the business case assessment. Option C is incorrect because data schema documentation and data quality reports are Phase II: Data Understanding outputs that inform data feasibility, not business feasibility. Option D is incorrect because resource requirements and schedule requirements from the Assess Situation task group are primary inputs to execution feasibility, not business feasibility. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Business Feasibility]"
Phase I: Business Understanding,AI Go/No-Go,Business Feasibility,"A project team has confirmed they can build a highly accurate deep learning model to predict employee turnover. The technology exists, the data is available, and the team has the necessary skills. However, senior leadership has not committed to acting on the predictions, and the HR department is resistant to algorithmic personnel decisions. Which feasibility dimension is most critically lacking?","Data feasibility, because HR data may contain sensitive personal information requiring compliance with privacy regulations.","Execution feasibility, because the organization lacks the operational readiness and change management capability to adopt AI-driven decisions.","Technical feasibility, because deep learning models may not be interpretable enough for sensitive HR application decisions.","Business feasibility, because stakeholder support and strategic alignment with organizational decision-making are insufficient.",D,"The correct answer is D. Option D is correct because business feasibility includes stakeholder support and organizational alignment with strategic goals. Despite confirmed technical capability, the lack of senior leadership commitment and active HR resistance to algorithmic decisions means the project cannot deliver business value. Option A is incorrect because it raises a data privacy concern about sensitive personal information that is not identified as a barrier in the scenario. Option B is incorrect because execution feasibility evaluates talent, infrastructure, and technical readiness to build and deploy the system, whereas the scenario describes organizational willingness to act on results, which is a business alignment issue. Option C is incorrect because technical feasibility concerns like model interpretability are not raised in the scenario; the team has confirmed the model can be built. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Business Feasibility]"
Phase I: Business Understanding,AI Go/No-Go,Data Feasibility,"A project team is assessing data feasibility for a real-time fraud detection system. They have access to transaction data but discover that the data is stored across 12 legacy systems with no API access, requiring manual extraction that would take 18 months. What is the appropriate conclusion regarding data feasibility?",Data is not feasible because accessibility constraints make it impossible to obtain the required data within reasonable timeline and budget constraints.,Data is feasible because the transaction information exists somewhere within the organization and could eventually be consolidated.,"Data feasibility is confirmed, and the team should proceed directly to Phase II: Data Understanding for detailed data analysis and profiling.",Data feasibility should be deferred until Phase IV: Model Development when the specific modeling requirements and data formats are clearer.,A,The correct answer is A. Option A is correct because data feasibility requires not just the existence of data but also its accessibility within project constraints including timeline and budget. The 18-month manual extraction requirement across 12 legacy systems without API access makes the data infeasible for practical use. Option B is incorrect because it conflates data existence with data feasibility; data that cannot be accessed within reasonable constraints is not feasible regardless of its existence. Option C is incorrect because it prematurely confirms feasibility and recommends proceeding to Phase II despite a clearly identified accessibility blocker that prevents reasonable data collection. Option D is incorrect because data feasibility is a Phase I gating decision that must be resolved before proceeding; deferring it to Phase IV would waste resources on infeasible data. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Data Feasibility]
Phase I: Business Understanding,AI Go/No-Go,Data Feasibility,"A healthcare AI project requires labeled medical images for training. The necessary images exist in hospital archives, but privacy regulations prohibit using them without patient consent, and obtaining consent for the historical dataset is prohibitively expensive. What is the primary data feasibility blocker in this scenario?",Technical infeasibility because the medical images are stored in incompatible proprietary formats that require expensive conversion tools.,Data volume insufficiency because the hospital archives do not contain enough labeled images to meet the minimum training requirements.,Budget constraints for procuring sufficient cloud storage and compute infrastructure needed to host and process the large image dataset.,Legal restrictions on data usage that prevent access to the required training data due to patient consent requirements under privacy regulations.,D,"The correct answer is D. Option D is correct because legal restrictions on data usage‚Äîspecifically, patient consent requirements under privacy regulations‚Äîcreate a data feasibility blocker. The data cannot be lawfully accessed for the intended purpose, and obtaining retroactive consent is described as prohibitively expensive. Option A is incorrect because it describes a technical format incompatibility issue that is not mentioned in the scenario; the images exist in usable form in hospital archives. Option B is incorrect because it describes a data volume problem that is not identified in the scenario; there is no indication that insufficient images exist. Option C is incorrect because it describes infrastructure budget constraints, which relate to execution feasibility rather than the core data accessibility barrier presented in the scenario. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Data Feasibility]"
Phase I: Business Understanding,AI Go/No-Go,Data Feasibility,"A project team completes their Phase I data feasibility assessment and determines the required data is not available within acceptable quality parameters. According to CPMAI principles, what is the most appropriate next step?",Proceed to Phase II: Data Understanding to conduct a more detailed analysis of the data gaps.,"Issue a No-Go decision, preventing investment in subsequent phases until data feasibility can be resolved.",Skip to Phase IV: Model Development to test if the model can work with available lower-quality data.,Revise the business objectives to match the available data without reassessing business feasibility.,B,The correct answer is B. Option B is correct because data feasibility is a Phase I gating decision within the AI Go/No-Go assessment. A No-Go decision at this point prevents wasted investment in subsequent phases when the foundational data requirement cannot be met. Option A is incorrect because Phase II: Data Understanding should only proceed after a Go decision; investing in detailed data analysis when feasibility has already failed wastes resources. Option C is incorrect because Phase IV: Model Development requires feasible data as a prerequisite; building models on data known to be infeasible is not a valid approach. Option D is incorrect because revising business objectives to match poor data without reassessing all feasibility dimensions undermines the integrity of the Go/No-Go framework. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Data Feasibility]
Phase I: Business Understanding,AI Go/No-Go,Execution Feasibility,"A manufacturing company wants to build an AI system for predictive maintenance. They have a strong business case, excellent sensor data, and leadership support. However, the company has no data scientists on staff, cannot hire externally due to a hiring freeze, and lacks the infrastructure to support model training and deployment. Which feasibility dimension is the primary barrier?","Execution feasibility, because the organization lacks the data science talent, model training infrastructure, and deployment readiness to execute the project.","Data feasibility, because the raw sensor data may require significant cleaning and transformation before it can be used for modeling.","Business feasibility, because the cost-benefit analysis did not properly account for the additional hiring and infrastructure investment costs.","Data feasibility, because the sensor data volume and variety may be insufficient to train a reliable predictive maintenance model.",A,"The correct answer is A. Option A is correct because execution feasibility evaluates whether the organization has the talent, infrastructure, and operational readiness to execute the project. The hiring freeze preventing data scientist recruitment and the lack of model training and deployment infrastructure are execution feasibility barriers. Option B is incorrect because it raises a data preparation concern about sensor data cleaning, but the scenario states the company has ""excellent sensor data,"" indicating data quality is not the issue. Option C is incorrect because business feasibility evaluates ROI and strategic alignment; the scenario states the business case is strong, so business feasibility is not the barrier. Option D is incorrect because it raises a data volume and variety concern that contradicts the scenario, which describes the sensor data as excellent rather than insufficient. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Execution Feasibility]"
Phase I: Business Understanding,AI Go/No-Go,Execution Feasibility,Which Phase I outputs provide the primary inputs for assessing execution feasibility?,The business objectives and success criteria documents from the Determine Business Objectives task group.,The cost-benefit analysis results and acceptable KPI performance values from the AI System Performance task group.,The resource requirements and schedule requirements from the Assess Situation task group within Phase I.,The cognitive requirements and AI pattern identification documents from the Cognitive Project Requirements task group.,C,"The correct answer is C. Option C is correct because execution feasibility directly uses resource requirements (talent, infrastructure, technology) and schedule requirements (timeline, milestones) from the Assess Situation task group to determine whether the organization can realistically execute the project. Option A is incorrect because business objectives and success criteria are primary inputs to business feasibility, not execution feasibility. Option B is incorrect because cost-benefit analysis results and KPI performance values feed into the business feasibility assessment rather than execution readiness. Option D is incorrect because cognitive requirements and AI pattern identification inform the technical approach and modeling strategy, not the organization's ability to execute. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Execution Feasibility]"
Phase I: Business Understanding,AI Go/No-Go,Execution Feasibility,"An insurance company has completed their Phase I assessments. The business case is strong, data is available and accessible, and they have experienced data scientists. However, the schedule requirements indicate a 6-month timeline, while regulatory changes will make the project obsolete in 4 months. What should the execution feasibility assessment conclude?","Execution is feasible because the team has the necessary data science talent, technology infrastructure, and domain expertise in insurance.","Execution feasibility is confirmed, and the team should begin Phase II: Data Understanding immediately to maintain project momentum.",Execution feasibility should be reassessed after the upcoming regulatory changes are finalized and their full impact is better understood.,Execution is not feasible because the required timeline exceeds the window in which the project can deliver meaningful business value.,D,"The correct answer is D. Option D is correct because execution feasibility includes timeline realism as a critical dimension. A project requiring 6 months to deliver when regulatory changes will make it obsolete in 4 months cannot deliver value within the available window, making execution infeasible. Option A is incorrect because it focuses only on talent and infrastructure while ignoring the critical timeline constraint; having the right team is necessary but not sufficient if the deadline is impossible. Option B is incorrect because it prematurely confirms feasibility and recommends proceeding to Phase II despite the known timeline impossibility. Option C is incorrect because it defers a decision that can and should be made in Phase I based on currently known information; the 6-month vs. 4-month gap is already clear. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Execution Feasibility]"
Phase I: Business Understanding,AI Go/No-Go,Business Feasibility,"A project team completes all three feasibility assessments with the following results: business feasibility is strong (positive ROI, stakeholder support), data feasibility is confirmed (data exists and is accessible), but execution feasibility fails due to lack of ML engineers and insufficient infrastructure. According to CPMAI principles, what should the AI Go/No-Go decision be?","Go, because two out of three feasibility dimensions are positive, and execution gaps can be addressed during later phases of the project.","Go, but only if the project timeline is extended to allow for the necessary infrastructure build-out and ML engineer hiring.","No-Go, because all three feasibility dimensions‚Äîbusiness, data, and execution‚Äîmust pass for a project to proceed.","No-Go, but only on execution feasibility grounds; the project can restart immediately once the execution capability gaps are resolved.",C,"The correct answer is C. Option C is correct because the CPMAI framework requires all three feasibility dimensions‚Äîbusiness, data, and execution‚Äîto pass for a Go decision. Failure on any single dimension warrants a No-Go, regardless of how strong the other dimensions are. Option A is incorrect because it applies a majority-rules logic (two out of three) that does not exist in the CPMAI framework; all three dimensions must independently pass. Option B is incorrect because it suggests proceeding conditionally despite a failed feasibility dimension; the framework does not allow a Go with unresolved feasibility failures. Option D is incorrect because while it correctly identifies a No-Go, it mischaracterizes the decision as limited to execution grounds only; the holistic Go/No-Go requires all three dimensions and the decision is based on overall assessment, not isolated to one dimension. [Maps to: Phase I: Business Understanding ‚Äì AI Go/No-Go ‚Äì Business Feasibility]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,"A project team has completed all Phase I tasks and is compiling the project plan. The document includes business objectives, success criteria, resource requirements, and a schedule. However, it omits the AI pattern identification and acceptable model performance values. What is the primary concern with this project plan?",The plan is too detailed and should focus only on high-level business goals rather than technical specifications.,The plan lacks AI-specific elements that are essential for guiding downstream phases like data collection and model selection.,The plan should defer all technical details until Phase IV when the team begins building models.,The plan meets all requirements because business objectives and success criteria are the only critical components.,B,"The correct answer is B. Option B is correct because the Phase I project plan must include AI-specific elements such as AI pattern identification and acceptable model performance values, which guide downstream phases including data collection in Phase II and model selection in Phase IV. These elements distinguish an AI project plan from a generic project plan. Option A is incorrect because AI pattern identification and model performance thresholds are necessary technical elements, not excessive detail. Option C is incorrect because deferring these elements to Phase IV leaves Phase II and Phase III without essential guidance on what data to collect and prepare. Option D is incorrect because business objectives and success criteria alone are insufficient; the project plan must synthesize outputs from all Phase I task groups. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,Which element of the Phase I project plan serves as the formal handoff mechanism to guide data collection activities in Phase II: Data Understanding?,"The cognitive requirements and AI pattern identification documents, which define what data is needed and what AI approach will be used.","The cost-benefit analysis showing projected ROI, which determines the business justification for continuing to Phase II activities.","The detailed project schedule with Gantt charts and milestone dates, which provides timeline guidance for data collection activities.","The stakeholder sign-off page with executive approval signatures, which confirms organizational alignment for the project to proceed.",A,"The correct answer is A. Option A is correct because cognitive requirements and AI pattern identification directly inform what data must be collected in Phase II: Data Understanding. These documents define the AI approach and data needs, making them the primary handoff mechanism from Phase I to Phase II. Option B is incorrect because the cost-benefit analysis informs business feasibility decisions but does not guide specific data collection activities. Option C is incorrect because project schedules are management tools that provide timeline structure but do not define what data is needed or why. Option D is incorrect because stakeholder sign-off confirms alignment and authorization but does not provide technical guidance for data collection work. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,"During Phase III: Data Preparation, the team discovers that the available data cannot support the AI pattern originally documented in the Phase I project plan. According to CPMAI principles, what should the team do?","Continue with Phase IV: Model Development using the original AI pattern, hoping the data will work.",Abandon the project entirely because the Phase I plan was incorrect and the data gap cannot be resolved through iteration.,Return to Phase I to revise the project plan with a new AI pattern and reassess feasibility.,Proceed to Phase V: Model Evaluation to test if the data issues can be ignored.,C,"The correct answer is C. Option C is correct because the CPMAI framework is iterative, not waterfall. When later phases reveal that original assumptions are invalid, teams should return to Phase I to revise the project plan, including updating the AI pattern and reassessing feasibility. Option A is incorrect because continuing to Phase IV: Model Development with an AI pattern that the data cannot support would produce an unreliable model. Option B is incorrect because project abandonment is too extreme; the iterative nature of CPMAI allows for plan refinement based on new information. Option D is incorrect because proceeding to Phase V: Model Evaluation without a viable model and with known data issues would not produce meaningful evaluation results. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,The project plan identifies potential data drift as a failure mode during Phase I. How does this element of the plan map to downstream phases in the CPMAI lifecycle?,"It maps to Phase III: Data Preparation, where data is cleaned and transformed to prevent drift from occurring in training datasets.","It maps to Phase V: Model Evaluation, where drift is assessed as part of the validation and model performance review process.","It maps to Phase VI: Model Operationalization, where monitoring mechanisms detect drift in production.","It maps to Phase II: Data Understanding, where initial data exploration and profiling reveals existing drift patterns in the data.",C,"The correct answer is C. Option C is correct because failure mode analysis conducted in Phase I, such as identifying data drift risk, directly informs the monitoring and maintenance plans created during Phase VI: Model Operationalization. Detection mechanisms for identified failure modes are implemented in production monitoring. Option A is incorrect because Phase III: Data Preparation addresses data quality through cleaning and transformation, not ongoing production drift detection. Option B is incorrect because Phase V: Model Evaluation assesses model performance against metrics before deployment, not ongoing drift monitoring in production. Option D is incorrect because Phase II: Data Understanding explores initial data characteristics, not production-time data drift that occurs after deployment. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,"A project plan includes acceptable KPI performance values established during Phase I. Where in the CPMAI lifecycle are these KPIs primarily monitored, creating a feedback loop to Phase I?","Phase II: Data Understanding, to ensure that the data quality and availability are sufficient to support KPI achievement targets.","Phase IV: Model Development, to tune model hyperparameters and architecture decisions toward meeting KPI performance targets.","Phase V: Model Evaluation, to validate through testing that the model performance will meet KPI targets before deployment.","Phase VI: Model Operationalization, where actual business impact is measured against Phase I KPI targets in a production environment.",D,"The correct answer is D. Option D is correct because KPI performance values established in Phase I are primarily monitored during Phase VI: Model Operationalization, where the deployed model's actual business impact is measured against the targets. This creates a feedback loop back to Phase I regarding whether business objectives are being met. Option A is incorrect because Phase II: Data Understanding assesses data characteristics and quality, not business KPI achievement in production. Option B is incorrect because Phase IV: Model Development focuses on building and training models, not measuring ongoing business outcomes. Option C is incorrect because Phase V: Model Evaluation validates model metrics before deployment, not long-term business KPI monitoring in production. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,The trustworthy AI requirements documented in the Phase I project plan establish constraints that will impact multiple downstream phases. Which phase activities are directly constrained by these requirements?,"Phases II, III, IV, and VI, as trustworthy AI requirements affect data collection, preparation, model selection, and operational monitoring.","Only Phase II: Data Understanding, where data sources must comply with privacy regulations and ethical data collection and usage standards.","Only Phase VI: Model Operationalization, where fairness, transparency, and accountability are monitored and enforced in the production environment.","Only Phase IV: Model Development, where explainable and interpretable models must be selected to satisfy transparency and compliance requirements.",A,"The correct answer is A. Option A is correct because trustworthy AI requirements established in Phase I constrain multiple downstream phases: Phase II (privacy-compliant data collection), Phase III (bias mitigation in data preparation), Phase IV (explainability requirements for model selection), and Phase VI (monitoring for fairness and compliance in production). Option B is incorrect because it captures only the Phase II impact on data privacy, ignoring the constraints on model development, data preparation, and operational monitoring. Option C is incorrect because it captures only the Phase VI monitoring impact, ignoring how trustworthy AI requirements also constrain data and modeling decisions. Option D is incorrect because it captures only the Phase IV explainability requirement, ignoring impacts on data collection, preparation, and operational monitoring. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,"Before the project can proceed from Phase I to Phase II, what must occur with the project plan according to CPMAI principles?",The plan must be submitted to the PMO for compliance review against organizational project management standards and governance policies.,"Stakeholder sign-off must be obtained, confirming alignment on what will be built, how success is measured, and what risks have been identified.",The data science team must validate that the AI pattern is technically feasible by running exploratory analysis on sample data.,The plan must be published on the project management information system so all team members have access to the documentation.,B,"The correct answer is B. Option B is correct because stakeholder sign-off on the project plan ensures all parties are aligned on business objectives, success criteria, and identified risks before committing resources to Phase II activities. This alignment is essential for the project to proceed on a shared foundation. Option A is incorrect because PMO compliance review may be an organizational practice but is not specified as a CPMAI Phase I requirement for proceeding to Phase II. Option C is incorrect because technical validation with sample data describes a Phase II: Data Understanding or Phase III activity, not a Phase I prerequisite for the plan. Option D is incorrect because publishing the plan is a communication and access step, not the formal gating mechanism that confirms stakeholder alignment. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase I: Business Understanding,Produce Project Plan,Produce Project Plan,A project manager is compiling the Phase I project plan and asks which documents need to be incorporated. The PMI-CPMAI professional explains that the plan must synthesize outputs from every Phase I task group. Which combination of inputs correctly represents the full scope of the project plan?,"The business objectives document, the cost-benefit analysis spreadsheet, and the success criteria only, representing outputs from the Determine Business Objectives task group without AI-specific elements.","The data feasibility report, the execution feasibility assessment, and the Go/No-Go decision outcome only, representing outputs from the AI Go/No-Go task group without business or cognitive elements.","The cognitive requirements document, the AI pattern identification matrix, and the resource and schedule requirements only, representing Cognitive Project Requirements and Assess Situation outputs without performance or trustworthy AI elements.","The business objectives, success criteria, cognitive requirements, AI pattern, resource requirements, schedule, model performance thresholds, KPI targets, trustworthy AI requirements, failure mode analysis, and Go/No-Go outcome.",D,"The correct answer is D. Option D is correct because the Phase I project plan must synthesize outputs from every Phase I task group: Determine Business Objectives, Cognitive Project Requirements, Assess Situation, AI System Performance and Operation, Trustworthy AI Requirements, AI Go/No-Go, and Produce Project Plan itself. Option A is incorrect because it includes only the Determine Business Objectives outputs, omitting cognitive, technical, trustworthy AI, and feasibility elements. Option B is incorrect because it includes only the AI Go/No-Go outputs, omitting business objectives, cognitive requirements, performance thresholds, and trustworthy AI requirements. Option C is incorrect because it includes only Cognitive Project Requirements and Assess Situation outputs, omitting business objectives, performance values, trustworthy AI, and feasibility elements. [Maps to: Phase I: Business Understanding ‚Äì Produce Project Plan ‚Äì Produce Project Plan]"
Phase II: Data Understanding,Collect Initial Data,Collect Initial Data,"A data science team is starting a project to predict customer churn. During Phase I, the project plan identified three potential data sources: CRM history, call center logs, and web clickstream data. In Phase II, the team begins work. Which activity best describes their first hands-on task with the data?",Querying the raw CRM database to confirm record counts and checking if the clickstream data vendor can provide the historical range specified in the project plan.,"Merging the CRM, call center, and clickstream data into a single consolidated table and preparing it for model training in a subsequent phase.",Using a histogram to analyze the distribution of call durations in the call center logs and flagging statistical outliers for further investigation.,Writing Python scripts to normalize the date formats across all three data sources so that timestamps are consistent before any analysis begins.,A,"The correct answer is A. This describes the core of ""Collect Initial Data""‚Äîgathering raw data from identified sources and validating basic availability and access assumptions from Phase I. Option B is incorrect because merging data into a consolidated table is a Data Preparation (Phase III) activity, not an initial collection task. Option C is incorrect because analyzing distributions and flagging outliers is an exploration activity within Phase II, but it occurs after the data has been successfully collected and described. Option D is incorrect because normalizing date formats is a data transformation task that belongs in Phase III: Data Preparation. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Collect Initial Data]"
Phase II: Data Understanding,Collect Initial Data,Collect Initial Data,"A project plan from Phase I states that a fraud detection model requires transaction data with a minimum of 12 months of history and at least a 5% fraud rate to be feasible. In Phase II, the team accesses the production database. What is the primary purpose of this initial data collection step in relation to the Phase I plan?",To begin training a preliminary model on the available transaction data to see if it can achieve the 5% fraud rate target specified in the project plan.,"To validate that data matching the specified volume, history, and class distribution is available and accessible as assumed in the Phase I feasibility assessment.",To clean the transaction data by removing any incomplete or duplicate records before the data science team begins reviewing it for analysis.,"To create a final, labeled dataset with fraud and non-fraud tags that is ready to be split into training and test sets for model development.",B,"The correct answer is B. Initial data collection is the checkpoint to validate the assumptions made in Phase I about data feasibility‚Äîconfirming that the data actually exists, is accessible, and meets the specifications. Option A is incorrect; model training occurs much later, in Phase IV: Model Development, and is not part of initial data collection. Option C is incorrect; cleaning and removing records is a Data Preparation (Phase III) task, not a Phase II collection activity. Option D is incorrect; creating the final labeled dataset is the output of multiple tasks in Phase III: Data Preparation, not Phase II. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Collect Initial Data]"
Phase II: Data Understanding,Collect Initial Data,Collect Initial Data,"During Phase II data collection for a predictive maintenance project, a team discovers that the sensor data they planned to use from the legacy manufacturing line is only stored for 30 days. The Phase I plan required three years of historical data to identify failure patterns. What is the correct immediate action for the team?",Proceed with model development using the 30 days of available data and adjust performance expectations during the model evaluation phase.,Attempt to impute the missing three years of data using statistical interpolation methods during the data preparation phase.,Document the finding and return to Phase I to re-evaluate the project's feasibility and data requirements given this constraint.,Ask the IT team to begin archiving the current sensor data daily so that in three years the project will have sufficient historical data.,C,"The correct answer is C. A core tenet of the CPMAI methodology is that Phase II data understanding activities can invalidate Phase I assumptions, triggering an iterative loop back to reassess business objectives and data feasibility. Option A is incorrect because proceeding with grossly insufficient data violates the agreed-upon project plan and would likely produce an unreliable model. Option B is incorrect; imputing three years of sensor data from 30 days of records is not feasible and would introduce severe bias‚Äîdata collection is about finding real data, not creating it artificially. Option D is incorrect because waiting three years is not a viable solution for the current project timeline and ignores the iterative nature of CPMAI. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Collect Initial Data]"
Phase II: Data Understanding,Collect Initial Data,Describe Data,"A team has successfully collected customer survey response data for a churn prediction project. Before performing any statistical analysis or creating visualizations, they produce a document listing all 42 survey questions as features, noting that responses range from 1 to 5, identifying that 15% of records have missing age data, and recording that the data spans from January to June 2023. Which Phase II activity does this scenario represent?","Data Exploration, because the team is investigating the characteristics of the dataset to understand patterns and relationships between variables.","Data Preparation, because the team is organizing and structuring the raw data into a usable format before it can be used for modeling.","Data Labeling, because the team is adding annotations and metadata tags to the dataset so the model can learn from categorized examples.","Data Description, because the team is creating an objective, factual profile of the data's features, value ranges, missing values, and time coverage.",D,"The correct answer is D. The scenario describes the factual, objective profiling of data‚Äîits features, value ranges, missing values, and time coverage‚Äîwhich is the definition of ""Describe Data"" in Phase II. Option A is incorrect because Data Exploration involves analyzing patterns, relationships, and distributions, which is a subsequent investigative step that goes beyond documenting what the data contains. Option B is incorrect because Data Preparation involves acting on the data by selecting, cleaning, and transforming it in Phase III, not simply documenting its current state. Option C is incorrect because Data Labeling is the process of adding target annotations to data for supervised learning, which is a Phase III: Data Preparation task. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Describe Data]"
Phase II: Data Understanding,Collect Initial Data,Describe Data,"A project manager notices the data science team is excited to immediately start building charts and running correlation analysis on a new dataset. Citing the CPMAI methodology, the project manager advises them to first perform a structured review to document the data's structure, the data types of each feature, and the percentage of null values per column. What is the primary risk of skipping this structured review and proceeding directly to analysis?",The team might create visualizations that are statistically invalid because underlying data type mismatches went undetected before the analysis began.,"The team may miss critical data quality issues like mixed data types in a column or unexpected null patterns, leading to inaccurate analytical results later.",The project will be delayed because the visualization tools and analysis platforms require a completed data description report as a mandatory input.,The team will be unable to select a model algorithm in Phase IV because algorithm selection requires the data description document as a formal prerequisite.,B,"The correct answer is B. The ""Describe Data"" step is a factual inventory of the dataset. Skipping it means the team might not realize, for example, that a column intended to be numerical is actually stored as text, which would cause errors in subsequent analysis and preparation. Option A is a possible downstream outcome, but the root cause is the undetected data quality issue described in B, making B the more direct and primary risk. Option C is incorrect; tools do not require a human-readable description report as a mandatory input to function. Option D is incorrect; model selection in Phase IV depends on the problem type and the prepared data, not the description document itself. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Describe Data]"
Phase II: Data Understanding,Collect Initial Data,Describe Data,"A data science team has just received access to a new customer transactions dataset. A junior analyst immediately opens the data in a notebook and begins plotting histograms and calculating correlations between variables. The project manager intervenes and says the team needs to first document the number of records, the data types of each column, the percentage of missing values, and the date range covered. Why is the project manager's directive methodologically correct according to CPMAI?","Because describing the data's basic characteristics is a required step before exploration, ensuring the team knows what they have before investigating patterns and relationships.",Because creating histograms and correlations requires a completed data description document to be uploaded into the analysis tool as a configuration input.,Because the team must return to Phase I and update the project plan with the data characteristics before any Phase II analysis activities can begin.,"Because data description and data exploration are the same activity in CPMAI, so performing them out of sequence has no methodological impact.",A,"The correct answer is A. In the CPMAI methodology, ""Describe Data"" is a foundational step within Phase II that must precede exploration. Description creates an objective inventory of what the data contains (volume, types, missing values, coverage), while exploration investigates patterns and relationships. The project manager is correct that the team should know what they have before analyzing it. Option B is incorrect; analysis tools do not require a formal description document as an input‚Äîthe description is for the team‚Äôs understanding, not a system dependency. Option C is incorrect; documenting data characteristics is a Phase II activity, not a trigger to return to Phase I. Option D is incorrect; description and exploration are distinct activities in CPMAI‚Äîdescription is factual profiling, while exploration involves statistical analysis and visualization. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Describe Data]"
Phase II: Data Understanding,Collect Initial Data,Explore Data and Cognitive Data Requirements,"A team is working on a project to classify support tickets by urgency. After collecting and describing the ticket data, they begin to investigate. They create a bar chart of ticket volumes by category, calculate the average response time per category, and note that the ""Critical"" category appears in only 1% of the tickets. Which two key activities of data exploration are demonstrated here?","Data labeling and data augmentation, because the team is annotating the ticket categories and expanding the dataset with synthetic observations.","Statistical analysis and data visualization, because the team is calculating summary metrics and representing patterns in graphical form.","Model training and hyperparameter tuning, because the team is fitting an algorithm to the ticket data and adjusting its settings for accuracy.","Data cleansing and feature selection, because the team is removing invalid records and choosing the most informative variables for modeling.",B,"The correct answer is B. The scenario describes calculating averages (statistical analysis) and creating bar charts (data visualization), which are primary activities in the ""Explore Data"" task of Phase II. Option A is incorrect because data labeling and augmentation are Phase III: Data Preparation tasks, not exploration activities. Option C is incorrect because model training and hyperparameter tuning are Phase IV: Model Development tasks that occur much later in the methodology. Option D is incorrect because data cleansing and feature selection are also Phase III: Data Preparation tasks, not Phase II exploration. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Explore Data and Cognitive Data Requirements]"
Phase II: Data Understanding,Collect Initial Data,Explore Data and Cognitive Data Requirements,"During Phase I, a project identified the AI pattern for a new application as ""anomaly detection"" to find unusual network traffic. In Phase II, during data exploration, the team plots the traffic volume over time and finds that the data is extremely consistent, with virtually no variance. What is the most significant implication of this exploration finding?","The team should immediately begin training a complex deep learning model, as subtle anomalies require sophisticated algorithms that can detect patterns invisible to basic exploration.","The data is perfectly clean and highly consistent, which means it will require very little preparation work in Phase III and can move quickly to model development.",The team should enrich the dataset by generating synthetic anomaly examples to ensure the detection model has sufficient positive cases to learn patterns from.,"The data may lack the signal necessary to support the ""anomaly detection"" cognitive requirement identified in Phase I, potentially invalidating the project's feasibility assumptions.",D,"The correct answer is D. Exploration is used to assess if the data can support the cognitive requirements from Phase I. If the data shows no variance, it is unlikely to contain any anomalies, meaning the project's core assumption is challenged. This finding would trigger an iterative loop back to Phase I to reassess feasibility. Option A is incorrect; training a model without any signal to detect anomalies is futile regardless of model complexity. Option B is incorrect; consistency does not mean clean‚Äîit means the data is potentially useless for this particular anomaly detection task. Option C is incorrect; while generating synthetic data is a valid advanced technique, the first step according to CPMAI is to reassess the feasibility of the cognitive requirement with stakeholders, not to artificially create the missing signal. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Explore Data and Cognitive Data Requirements]"
Phase II: Data Understanding,Collect Initial Data,Explore Data and Cognitive Data Requirements,"During exploration of a dataset for a binary classification model, a data scientist discovers that 95% of the records belong to Class A and only 5% to Class B. How does this finding most directly impact the downstream phases of the project?","It is a Phase II finding with no meaningful impact on Phase III or IV, as modern classification models are inherently robust to class imbalances and require no special handling.","It confirms the data is ready for model development without further preparation, since a real-world class imbalance is expected and does not require any intervention.","It triggers a specific data preparation strategy in Phase III, such as oversampling, undersampling, or cost-sensitive learning, to address the class imbalance before model training.","It requires an immediate return to Phase I to redefine the business objectives entirely, because a severe class imbalance means the prediction target is fundamentally unacceptable.",C,"The correct answer is C. Discovering a class imbalance is a classic exploration finding that directly informs Phase III: Data Preparation decisions. The team will need to plan for techniques like oversampling, undersampling, or cost-sensitive algorithms, ensuring the model can learn from the minority class. Option A is incorrect; class imbalance significantly impacts model performance and must be explicitly addressed‚Äîignoring it typically produces a model that predicts only the majority class. Option B is incorrect; while the imbalance might reflect real-world proportions, the data cannot simply be declared ""ready"" without a preparation strategy. Option D is incorrect; the business objective of predicting Class B remains valid‚Äîthe imbalance is a data characteristic to be managed through preparation techniques, not a reason to abandon the business goal. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Explore Data and Cognitive Data Requirements]"
Phase II: Data Understanding,Collect Initial Data,Explore Data and Cognitive Data Requirements,"A team is performing data exploration in Phase II. They are creating scatter plots to visualize relationships between variables and calculating correlation coefficients. A junior team member suggests that these correlation values could be used to remove highly correlated features now to simplify the dataset. According to the CPMAI methodology, why is this suggestion premature?",Correlation analysis is not a valid or recognized technique for understanding data relationships and should not be used during any Phase II exploration activity.,The team cannot remove any features until after a model has been trained in Phase IV and feature importance scores have been calculated from the modeling results.,"Removing features based on correlation is a data transformation activity that belongs in Phase III: Data Preparation, not in Phase II, which is for analysis and investigation only.","The project plan from Phase I must first be formally updated and re-approved by all stakeholders to authorize any feature removal, regardless of which phase it occurs in.",C,"The correct answer is C. The key distinction in CPMAI is that Phase II (Explore) is for analysis and investigation‚Äîunderstanding the data‚Äîwhile Phase III (Data Preparation) is for acting on that analysis by transforming the data. Removing features is a transformation action, not an exploration activity. Option A is incorrect; correlation analysis is a valid and commonly used exploration technique in Phase II. Option B is incorrect; feature selection based on correlation is a legitimate preparation step in Phase III and does not require a trained model from Phase IV. Option D is incorrect; while the Phase I project plan defines scope, it does not need formal re-approval for every technical decision‚Äîthe methodological phase determines when transformation activities are appropriate. [Maps to: Phase II: Data Understanding ‚Äì Collect Initial Data ‚Äì Explore Data and Cognitive Data Requirements]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,"A healthcare analytics team is reviewing a dataset of patient records for a readmission prediction model. They notice that 30% of records are missing the ""discharge disposition"" field, and in 5% of cases where the field is populated, the value is 'A' instead of the standard discharge code format. Which two data quality dimensions are being affected in this scenario?","Completeness and consistency, because records are missing values and the populated values follow different formatting standards across the dataset.","Accuracy and timeliness, because the discharge codes may be factually wrong and the records may not reflect the most current patient information.","Relevance and uniqueness, because the discharge disposition field may not be needed for the model and some records may be duplicated in the dataset.","Consistency and validity, because the formatting varies across records and the non-standard codes may fall outside the allowed value set for the field.",A,"The correct answer is A. Missing data directly affects the ""completeness"" dimension (30% of records lack the field), and inconsistent formatting (using 'A' instead of standard codes) affects the ""consistency"" dimension. Option B is incorrect because timeliness refers to data being current and available when needed, which is not the issue described; accuracy refers to factual correctness, but the scenario describes formatting inconsistency, not factual errors. Option C is incorrect because the scenario does not question the field's relevance to the model or mention duplicate records; uniqueness and relevance are not affected. Option D is incorrect because while consistency is one of the affected dimensions, validity (whether values fall within allowed ranges) is not the primary issue‚Äîthe problem is inconsistent formatting, not values outside a valid range. Additionally, Option D omits completeness, which is the more prominent issue with 30% missing data. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,"During Phase II data quality verification for a credit risk model, a data scientist discovers that customer income values in the database have multiple formatting issues, some records contain negative income values, and zip codes are inconsistently entered as 5-digit and 9-digit formats. The data scientist immediately begins writing Python scripts to standardize the formats and replace negative values with NULL. According to CPMAI methodology, what is the primary concern with this approach?","Python is not an approved language for data quality remediation according to PMI standards, and the team should use a PMI-certified data cleansing tool instead.",The team should be using commercial data quality tools with built-in validation rules rather than writing custom scripts that may introduce new errors.,"Fixing data problems is a Phase III: Data Preparation activity; Phase II should focus on identifying, documenting, and assessing issues, not remediating them.",Negative income values should be replaced with the average income rather than NULL to preserve the record count and avoid reducing the training data volume.,C,"The correct answer is C. A fundamental CPMAI principle is that Phase II (Data Understanding) identifies and documents data quality issues, while Phase III (Data Preparation) remediates them. The data scientist is jumping ahead to remediation before completing the assessment. Option A is incorrect because PMI does not prescribe specific programming languages for any phase of the methodology. Option B is incorrect because the choice between commercial tools and custom scripts is an implementation decision, not a methodological concern‚Äîthe issue is timing, not tooling. Option D is incorrect because the specific replacement strategy for negative values is a Phase III decision; the primary concern is not how the fix is done but that fixing is happening in the wrong phase entirely. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,"A project team completes their data quality assessment on a customer transaction dataset. The resulting report documents that 8% of transactions have missing merchant category codes, 3% show duplicate transaction IDs, and transaction timestamps are recorded in two different time zones across the dataset. What is the primary purpose of this data quality report within the CPMAI lifecycle?","To serve as the final sign-off document for the project sponsor, indicating that the data has been fully validated and is ready for production deployment.",To replace the need for a formal data dictionary or data lineage documentation by consolidating all data metadata into a single deliverable.,To be submitted to the project management office as evidence that Phase II activities have been completed on schedule and within the allocated budget.,"To provide input to Phase III: Data Preparation by specifying which data quality issues need to be addressed, their severity, and their potential impact on modeling.",D,"The correct answer is D. The data quality report is a critical output from Phase II that directly informs the planning and execution of Phase III: Data Preparation tasks. It specifies what needs to be fixed, how severe each issue is, and what impact it may have on downstream modeling. Option A is incorrect because the report identifies issues that must still be fixed; the data is not yet ready for modeling or production‚Äîit documents problems, not sign-off. Option B is incorrect because the data quality report and data dictionary serve different purposes; the quality report does not replace lineage or metadata documentation. Option C is incorrect because while the report may be referenced in project tracking, its primary purpose is technical‚Äîinforming data preparation‚Äînot administrative compliance. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,"A predictive maintenance project for aircraft engines requires 99.5% accuracy in predicting failures to meet safety regulations established in Phase I. During Phase II data quality verification, the team finds that sensor readings have a 2% random error rate due to calibration drift. How do the Phase I business requirements most directly impact the interpretation of this data quality finding?",The Phase I requirements are irrelevant to this assessment because data quality should always be maximized to the highest possible standard regardless of business needs.,"The 99.5% accuracy target means the 2% sensor error rate is unacceptable, as model performance cannot exceed the quality of its input data, making this a critical finding.",The 2% error rate is acceptable because sensor data is inherently noisy in industrial settings and modern models are designed to compensate for random measurement errors.,The team should immediately begin Phase IV: Model Development to build a prototype and empirically test whether the model can overcome the 2% error rate.,B,"The correct answer is B. Phase I establishes the required performance thresholds, and the principle of ""garbage in, garbage out"" means a model cannot be more reliable than its input data. If the data has a 2% error rate, achieving 99.5% accuracy is mathematically constrained, making this a critical data quality finding. Option A is incorrect because Phase I requirements are directly relevant‚Äîthey provide the criteria for evaluating whether a data quality issue is acceptable or critical. Option C is incorrect because the acceptability of noise depends on the performance requirement; 2% error may be tolerable for a 90% accuracy target but is critical for a 99.5% target. Option D is incorrect because jumping to Phase IV before resolving a critical data quality issue violates the CPMAI sequence; the team should document the finding and assess whether it requires a Phase I reassessment of feasibility. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,"A team building a resume screening model to reduce bias in hiring discovers during Phase II that their training data contains 90% resumes from male applicants and only 10% from female applicants, mirroring historical hiring patterns the company is trying to change. What type of AI-specific data quality issue does this represent, and what is its primary risk?","Representation bias, which could cause the model to perpetuate historical discrimination by under-selecting qualified female candidates due to their under-representation in training data.","Data leakage, which would cause the model to appear artificially accurate during testing but fail in production because gender information leaked into the target variable.","Label quality, because the resumes may not be correctly classified as qualified or unqualified and the labeling process itself may reflect the historical bias.","Temporal bias, because hiring patterns change over time and the training data may reflect outdated preferences that no longer align with current recruitment standards.",A,"The correct answer is A. This scenario describes representation bias, where certain groups are under-represented in the training data, leading to models that perform poorly or unfairly for those groups. This is a Trustworthy AI concern that connects directly to Phase I ethical AI requirements. Option B is incorrect; data leakage involves target information inadvertently appearing in features, which is not what this scenario describes‚Äîthe issue is demographic imbalance, not information leaking across data splits. Option C is incorrect; while label quality is a valid concern, the primary issue here is the demographic distribution of the data itself, not how individual resumes are labeled as qualified or unqualified. Option D is incorrect; temporal bias relates to data becoming outdated over time, but the core issue here is demographic imbalance in the current dataset, not the age of the data. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,"During Phase II data quality verification for a real-time fraud detection system, the team discovers that the transaction data they planned to use has a 24-hour delay before appearing in the data warehouse. The Phase I business requirement specifies that the model must flag potentially fraudulent transactions within 5 seconds. What is the appropriate action based on this finding?",Proceed with model development using the delayed data and add a documented note that fraud predictions will be generated on a 24-hour delay instead of real-time.,Build a data caching layer during Phase III: Data Preparation to store incoming transactions locally and bypass the 24-hour data warehouse delay.,Adjust the Phase II data quality report to classify this as a minor consistency issue that does not materially affect the feasibility of the fraud detection system.,"Document the finding and return to Phase I to reassess feasibility, as the timeliness dimension of data quality makes the current data source unusable for the real-time requirement.",D,"The correct answer is D. The data fails the ""timeliness"" dimension relative to the Phase I requirement for 5-second detection. When a data quality issue fundamentally undermines the project's cognitive feasibility, the correct CPMAI action is to loop back to Phase I to reassess. Option A is incorrect because it ignores the critical business requirement; a 24-hour delay fundamentally contradicts the real-time fraud detection objective. Option B is incorrect because while a caching solution may be technically valid, it is an architectural decision that does not belong in Phase III Data Preparation, and the fundamental feasibility question must be addressed in Phase I first. Option C is incorrect because a 24-hour delay against a 5-second requirement is not a minor issue‚Äîit is a critical feasibility concern that threatens the entire project premise. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,A data quality analyst is beginning the Verify Data Quality task for a customer churn prediction project. The project sponsor has emphasized that the model must be highly accurate to justify the cost of retention programs. Which sequence of steps best represents a systematic approach to this task?,"Write code to fix null values and standardize formats, then check data types against the schema, then generate summary statistics, and finally create a data quality dashboard.","Ask the business stakeholders what they think good data quality looks like, then train a preliminary model to see which features are most important and where quality matters.","Define quality criteria based on Phase I requirements, measure each dimension against those criteria, document findings in a quality report, and assess the impact on the cognitive task.","Load all available data into a visualization tool, create charts and distributions for every variable, and present the visual findings to the IT team for remediation.",C,"The correct answer is C. This describes the systematic CPMAI process for Verify Data Quality: define criteria from Phase I, measure against those criteria, document findings, and assess impact on the AI task. Option A is incorrect because it starts with remediation (fixing nulls and standardizing formats) before assessment, and it skips the crucial step of defining what ""good"" quality means relative to Phase I requirements. Option B is incorrect because while stakeholder input may inform quality criteria, training a preliminary model is a Phase IV activity and should not be done during Phase II data quality verification. Option D is incorrect because visualization alone is insufficient; it skips criteria definition, measurement against standards, and impact assessment‚Äîand handing findings to IT for remediation confuses Phase II documentation with Phase III action. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Data Quality,Verify Data Quality,"A financial services firm spends six months building a sophisticated ensemble model to predict loan defaults. The model achieves 95% accuracy on validation data and is deployed to production. Six months later, the compliance team reviews the model's decisions and discovers that the training data contained a systematic error: loans approved by a particular branch were incorrectly coded as non-default when they had actually defaulted. Despite the sophisticated algorithms, the model's predictions are unreliable. This scenario best illustrates which principle?",The need for more complex algorithms and deeper neural network architectures to detect and compensate for hidden data errors that simpler models would miss.,"The ""garbage in, garbage out"" principle, where poor data quality undermines model performance regardless of how sophisticated the algorithms or ensemble methods are.",The importance of Phase VI: Model Operationalization monitoring to detect data quality issues that were not identified during the initial development cycle.,"The value of ensemble methods in improving prediction accuracy, which in this case was insufficient because the ensemble was not configured with enough base learners.",B,"The correct answer is B. This classic ""garbage in, garbage out"" scenario demonstrates that even sophisticated models fail when trained on poor-quality data. The team either skipped or inadequately performed Phase II data quality verification, and no amount of algorithmic complexity can compensate for systematically incorrect training labels. Option A is incorrect because more complex algorithms cannot fix fundamentally incorrect data‚Äîthe errors are in the labels, not in subtle patterns that need deeper architectures to detect. Option C is incorrect because while Phase VI monitoring is important, the root failure occurred in Phase II; monitoring would have caught the symptoms in production but not the root cause, which was inadequate data quality verification before modeling began. Option D is incorrect because the number of base learners in the ensemble is irrelevant when the training data itself contains systematic errors‚Äîadding more models trained on the same bad data produces more bad predictions. [Maps to: Phase II: Data Understanding ‚Äì Data Quality ‚Äì Verify Data Quality]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Training and Test Data Requirements,"A team is building a binary classification model to predict customer churn. To assess performance, they train the model on 80% of their historical data and then evaluate its accuracy on the remaining 20% of the same dataset. They repeat this process several times, tweaking the model based on the accuracy scores from the 20% holdout. What is the fundamental flaw in this approach?","The 80/20 split is insufficient; a 70/15/15 split for training, validation, and test sets is always required by the CPMAI methodology to properly separate tuning from final evaluation of the model.","The team should have used k-fold cross-validation instead of a simple train-test split, because cross-validation is always methodologically superior to holdout methods regardless of the dataset size or context.",The flaw is using a binary classification model for churn prediction; a regression model would have been more appropriate because churn probability is a continuous value that requires numeric output.,"By using the holdout set repeatedly to make model adjustments, they are effectively allowing information from the test set to influence the training process, compromising the ability to estimate true performance on unseen data.",D,"The correct answer is D. This scenario describes a common form of data leakage where the test set is used repeatedly for model selection and tuning, causing it to indirectly influence the model. The test set should only be used once, at the very end, to get an unbiased estimate of performance. Option A is incorrect because no single split ratio is universally required‚Äîthe appropriate ratio depends on dataset size and project context. Option B is incorrect because while cross-validation is valuable, it is not ""always superior""‚Äîthe flaw here is the repeated use of the holdout set, not the choice of split method. Option C is incorrect because binary classification is appropriate for predicting whether a customer will churn or not; this is a modeling technique question unrelated to the data split flaw. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Training and Test Data Requirements]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Training and Test Data Requirements,"A fraud detection project has only 5,000 historical records of confirmed fraudulent transactions, which represent about 2% of the overall transaction volume. The team is defining their training and test data requirements. Which consideration is most critical for ensuring the test set provides a reliable evaluation of model performance?","Performing a stratified split that maintains the 2% fraud rate in both the training and test sets, ensuring each set reflects the real-world class distribution.","Using a simple random split of all data into 80% training and 20% testing, as this is the standard approach and will naturally distribute fraud cases evenly.",Ensuring the test set contains at least 50% fraudulent transactions to adequately test the model's ability to detect fraud in a balanced evaluation environment.,Increasing the size of the test set to 40% of the data to compensate for the small absolute number of fraud cases available for evaluation.,A,"The correct answer is A. Stratified splitting is critical for imbalanced datasets to ensure that the rare class (fraud) is represented in both training and test sets in proportions similar to the original dataset, preserving the real-world distribution. Option B is incorrect because a simple random split with only 2% fraud cases risks creating test sets with very few or even zero fraud examples, producing unreliable evaluation results. Option C is incorrect because artificially inflating fraud in the test set would not reflect real-world conditions, and model performance metrics would be misleadingly optimistic. Option D is incorrect because increasing the test set size to 40% reduces the training data available, and without stratification, a larger test set still might not adequately represent the minority class. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Training and Test Data Requirements]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Training and Test Data Requirements,"A data science team is about to begin splitting their dataset into training, validation, and test sets for a predictive model. The project manager asks where in the CPMAI lifecycle these split requirements should have been formally defined and documented. What is the correct answer?","These requirements should be defined in Phase II: Data Understanding, within the Machine Learning Model Data Requirements task group, before any data preparation begins.","These requirements should be defined in Phase III: Data Preparation, because the actual splitting of data is a preparation activity that happens during that phase.","These requirements should be defined in Phase IV: Model Development, because the modeling team needs to determine splits based on the algorithm they select.","These requirements should be defined in Phase V: Model Evaluation, because the evaluation team determines how much data is needed for a statistically valid test.",A,"The correct answer is A. Defining the requirements for how data will be split is a planning activity that occurs in Phase II, within the ""Machine Learning Model Data Requirements"" task group. The requirements are defined here so that Phase III can implement them. Option B is incorrect because Phase III is where those defined requirements are implemented by actually performing the split‚Äîthe specification must come first in Phase II. Option C is incorrect because model selection in Phase IV uses the data that was already prepared according to Phase II requirements; the splits should not be determined by algorithm choice. Option D is incorrect because Phase V evaluates results using the test set that was defined in Phase II and created in Phase III; defining splits during evaluation would be methodologically backwards. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Training and Test Data Requirements]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Training and Test Data Requirements,A research team has a very small dataset of only 500 labeled medical images for a rare disease detection model. They need to develop a model but are concerned that a standard 80/10/10 split would leave too few images for validation and testing. Which data strategy should they specify in their Phase II requirements to address this constraint?,"Request approval to collect 5,000 more images before proceeding with any modeling, as 500 is insufficient for any type of machine learning regardless of the evaluation strategy used.",Specify that they will use all 500 images exclusively for training and then test the model on a separate set of unlabeled images to avoid wasting any labeled data on evaluation.,"Specify that k-fold cross-validation will be used as the primary evaluation strategy instead of a single held-out test set, allowing all data to contribute to both training and evaluation.","Specify a 99/1 split, using 495 images for training and only 5 for testing, to maximize the amount of data available for the model to learn disease patterns from.",C,"The correct answer is C. K-fold cross-validation is a standard approach for small datasets, allowing all data to be used for training while still obtaining a robust performance estimate by rotating the validation set across folds. Option A is incorrect; while more data is always better, cross-validation is a valid strategy for small datasets‚Äîthe team should not halt the project when a viable methodology exists. Option B is incorrect because testing on unlabeled data provides no ground truth to evaluate performance against; evaluation requires labeled data to measure accuracy. Option D is incorrect because 5 test images would produce a statistically unreliable performance estimate‚Äîthe results would vary wildly depending on which specific images happened to be selected. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Training and Test Data Requirements]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Edge Model Data Needs,A team is planning an AI application that will run on a smartphone to provide real-time language translation without an internet connection. The model must be downloaded once and then function entirely on the device. Which data consideration is most critical to address during Phase II for this edge deployment scenario?,"The training data must be stored in a centralized cloud data warehouse for easy access during model updates, as edge models are refreshed from the cloud periodically.","All data processing for model inference must happen on the device, so no data preparation work is needed in Phase III since the data stays local.","The project should use a large pretrained model hosted in the cloud and stream translation data to it, as on-device models are never accurate enough for production use.","The model must have a small memory footprint, so the data requirements should include strategies like quantization or pruning to be defined in Phase II and implemented in Phase III.",D,"The correct answer is D. Edge deployment imposes size and compute constraints. Strategies like quantization or pruning, which reduce model size, must be anticipated in Phase II requirements so they can be implemented during Phase III data preparation and Phase IV model development. Option A is incorrect because the question specifies the model functions without internet; cloud storage for training is a separate concern from the on-device deployment constraints that Phase II must address. Option B is incorrect because data preparation is still needed in Phase III regardless of where inference occurs‚Äîtraining data must be prepared to support the edge-optimized model. Option C is incorrect because the scenario explicitly requires offline operation; streaming to a cloud model contradicts the stated requirement for no internet connection. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Edge Model Data Needs]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Edge Model Data Needs,"A healthcare wearable device company is developing an AI model to detect cardiac arrhythmias in real-time. Due to strict patient privacy regulations and intermittent connectivity, raw ECG data cannot be streamed to the cloud for analysis. What edge-specific data requirement must be defined in Phase II to address these constraints?",The requirement for high-bandwidth 5G connectivity to ensure that patient ECG data can be uploaded quickly to the cloud whenever a connection becomes available.,The requirement for a federated learning approach where the model trains locally on device data and only anonymous model updates are shared with the central server.,The requirement for all training data to be stored in a centralized repository managed by the compliance team for regulatory audit purposes and data governance.,The requirement to use only publicly available ECG datasets for training to avoid any privacy concerns with patient data and simplify regulatory compliance.,B,"The correct answer is B. Federated learning is an edge-specific strategy that addresses both privacy (data never leaves the device) and connectivity (only model updates, not raw data, are shared) constraints identified in the scenario. Option A is incorrect because the problem states connectivity is intermittent; relying on high bandwidth does not solve the fundamental privacy constraint that raw data cannot leave the device. Option C is incorrect because centralizing raw patient ECG data directly contradicts the stated privacy constraint that data cannot be streamed to the cloud. Option D is incorrect because limiting training to public datasets ignores the value of the device's proprietary patient data; the goal is to use patient data safely, not avoid it entirely. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Edge Model Data Needs]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Edge Model Data Needs,"During Phase II for a predictive maintenance project on remote oil pumps with limited satellite connectivity, the team identifies that the model must run on an edge device and only transmit alerts. How will this edge model data requirement most directly impact downstream phases?","It will have no impact on downstream phases, as edge deployment is exclusively a Phase VI: Model Operationalization concern that does not affect data preparation or model development.",It will require Phase I to be restarted because edge deployment was not considered during business objective definition and the project plan must be revised from scratch.,It will require Phase IV to select a lightweight model architecture and Phase III to prepare data that supports quantization to reduce the overall model size for edge deployment.,It will primarily affect Phase VI monitoring by requiring a real-time dashboard to track model performance metrics on each individual edge device in the field.,C,"The correct answer is C. Edge requirements identified in Phase II directly cascade downstream: Phase III data preparation must consider techniques that enable smaller models, and Phase IV model development must select algorithms suitable for edge constraints such as limited compute and memory. Option A is incorrect because edge requirements affect multiple phases‚Äîdata preparation, model selection, and operationalization‚Äînot just deployment. Option B is incorrect because identifying edge requirements during Phase II is the correct time to do so; there is no need to restart Phase I unless business objectives themselves need to change. Option D is incorrect because while monitoring is important, the most direct downstream impact is on model architecture selection (Phase IV) and data preparation (Phase III), not dashboards. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Edge Model Data Needs]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Pretrained and Third-Party Model Usage,"A team decides to use a publicly available pretrained image recognition model as the foundation for a medical diagnosis tool. They plan to fine-tune it on their proprietary dataset of X-ray images. During Phase II, what critical data-related consideration must they evaluate to avoid downstream risks?","Whether the pretrained model achieves 100% accuracy on the original benchmark dataset it was tested on, as anything less indicates the model is fundamentally flawed.","Whether the team has enough computational resources to train a model entirely from scratch, as fine-tuning pretrained models is rarely effective for specialized domains.","Whether the pretrained model's architecture is the most popular one on GitHub with the largest community, as this ensures long-term support and reliability.","Whether the pretrained model was trained on data that includes medical images similar to X-rays, and whether the licensing allows commercial use in a healthcare application.",D,"The correct answer is D. Key considerations for pretrained models include training data provenance (was it trained on data relevant to the target domain?) and licensing/compliance (can the model legally be used for this commercial healthcare purpose?). These must be evaluated in Phase II. Option A is incorrect because benchmark accuracy on the original dataset does not guarantee performance on medical X-rays‚Äîdomain relevance matters more than benchmark scores. Option B is incorrect because fine-tuning is a well-established technique that often outperforms training from scratch, especially when domain-specific data is limited. Option C is incorrect because GitHub popularity is not a valid criterion for evaluating model suitability; training data provenance and licensing are the critical factors. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Pretrained and Third-Party Model Usage]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Pretrained and Third-Party Model Usage,"A financial institution needs to build a credit scoring model that is highly explainable to meet regulatory requirements established in Phase I. The team is considering using a complex, black-box pretrained model from a third-party vendor to save development time. Based on the Phase I requirements, what is the appropriate evaluation of this option during Phase II?","The pretrained model is acceptable because it will save significant development time, and explainability can always be added after deployment using post-hoc interpretation techniques.","The pretrained model should be rejected because its black-box nature conflicts with the high explainability requirements from Phase I, making a simpler, custom-trained model more appropriate.","The team should proceed with the pretrained model for production scoring and plan to use a separate, more explainable model only for the regulatory reporting requirements.","The Phase I requirements should be revised to remove the explainability requirement, as modern AI models are inherently too complex to provide meaningful explanations of their decisions.",B,"The correct answer is B. Phase I defines trustworthy AI requirements including explainability. Phase II evaluates whether pretrained models can meet those requirements. If a model is a black box and explainability is a regulatory requirement, it should be rejected in favor of an approach that can meet the constraint. Option A is incorrect because post-hoc explainability methods may not satisfy strict regulatory requirements‚Äîthey provide approximations, not guaranteed transparency into model decisions. Option C is incorrect because using two different models for scoring and reporting creates inconsistency and potential regulatory compliance issues; regulators expect the production model itself to be explainable. Option D is incorrect because Phase II should not override Phase I regulatory requirements‚Äîthose requirements reflect legal obligations that cannot simply be removed for technical convenience. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Pretrained and Third-Party Model Usage]"
Phase II: Data Understanding,Machine Learning Model Data Requirements,Pretrained and Third-Party Model Usage,"A project manager asks the data science team to evaluate whether a large pretrained foundation model could be used for their natural language processing project, including checking what data it was trained on and whether the licensing permits their intended use. The team is unsure which CPMAI phase this evaluation belongs in. Where should this evaluation be performed?","Phase II: Data Understanding, within the Machine Learning Model Data Requirements task group, because evaluating pretrained model suitability is a data understanding activity.","Phase I: Business Understanding, because evaluating third-party tools and models is part of defining business objectives and assessing the project situation.","Phase III: Data Preparation, because evaluating pretrained models is part of preparing the data pipeline and selecting the tools that will process the data.","Phase IV: Model Development, because evaluating pretrained models is a modeling technique decision that belongs with algorithm selection and model building.",A,"The correct answer is A. The evaluation of whether to use a pretrained or third-party model, based on data considerations like training data provenance and licensing, occurs in Phase II within the ""Machine Learning Model Data Requirements"" task group. This is a data understanding activity. Option B is incorrect because Phase I defines the business and cognitive requirements that inform this evaluation, but the actual assessment of pretrained model suitability against those requirements happens in Phase II. Option C is incorrect because Phase III implements data preparation decisions already made‚Äîit does not evaluate whether a pretrained model is suitable. Option D is incorrect because while fine-tuning pretrained models occurs in Phase IV, the initial suitability evaluation based on data provenance and licensing is a Phase II activity. [Maps to: Phase II: Data Understanding ‚Äì Machine Learning Model Data Requirements ‚Äì Pretrained and Third-Party Model Usage]"
Phase III: Data Preparation,Data Selection,Select Data,"A team is building a model to predict employee turnover. During Phase II, they collected HR records including age, salary, department, performance ratings, and home address. The Phase II data quality report flagged home address as only 40% complete. Phase I business objectives focus on identifying department-level turnover drivers, and legal counsel flagged home address as a potential privacy concern. During the Select Data task, what should the team do?","Select all features except home address, as its low data quality, potential irrelevance to turnover drivers, and privacy concerns outweigh any possible predictive value it might provide.","Include all available data including home address, because more features always improve model performance regardless of the quality or compliance issues identified in Phase II.","Defer the selection decision entirely to Phase IV: Model Development, allowing the modeling team to empirically test which features are important during the training process.","Select home address but plan to impute all missing values during the Clean Data task, as statistical imputation techniques can fully resolve any data completeness issues.",A,"The correct answer is A. Data selection in Phase III uses criteria from Phase II findings (quality) and Phase I objectives (compliance/relevance). Home address has low quality (40% complete), questionable relevance to department-level turnover, and privacy concerns, so excluding it is appropriate. Option B is incorrect because more data is not always better‚Äîlow-quality or non-compliant features can harm model performance and violate regulatory requirements. Option C is incorrect because data selection is a Phase III activity that should occur before model development in Phase IV, not be deferred to it. Option D is incorrect because imputing 60% of values introduces significant assumptions and does not address the privacy and relevance concerns. [Maps to: Phase III: Data Preparation ‚Äì Data Selection ‚Äì Select Data]"
Phase III: Data Preparation,Data Selection,Select Data,"A project team has just completed Phase II, which included collecting customer transaction data from three different source systems. A junior data scientist suggests they can now begin the Select Data task immediately. The project manager asks the junior scientist to explain how data selection differs from data collection. What is the key distinction?","Model evaluation in Phase V determines whether the selected data was appropriate for the business objectives, but it does not define the selection criteria themselves.",Data augmentation in Phase III creates synthetic data points to expand the dataset rather than selecting subsets of existing data that was already collected during Phase II.,Algorithm selection in Phase IV determines which modeling technique will be applied to the data that was selected and prepared during Phase III activities.,"Data collection in Phase II is about gathering raw data from source systems, while data selection in Phase III is about choosing which subsets of that already-collected data to use for modeling.",D,"The correct answer is D. A key distinction is that data collection occurs in Phase II (gathering raw data), while data selection occurs in Phase III (choosing which parts of the collected data to actually use for modeling). Option A is incorrect because evaluation comes later in Phase V but does not define the distinction between collection and selection. Option B is incorrect because augmentation is a separate Phase III activity that creates new data, not the same as selecting from existing data. Option C is incorrect because algorithm selection is a Phase IV activity unrelated to the collection-versus-selection distinction being tested. [Maps to: Phase III: Data Preparation ‚Äì Data Selection ‚Äì Select Data]"
Phase III: Data Preparation,Data Selection,Select Data,"A financial services firm is developing a credit approval model. During Phase I, legal compliance requirements prohibited using race, gender, or zip code as factors due to fair lending laws. During Phase II data exploration, the team discovered that zip code is strongly correlated with approval rates and would significantly improve model accuracy. What should the team do during the Select Data task?","Include zip code in the selected features because it significantly improves model accuracy, and maximizing accuracy is the primary goal of the credit approval project.","Exclude zip code despite its predictive power, as using it would violate the Phase I regulatory compliance requirements that take precedence over pure model accuracy.","Include zip code but transform it into a different format such as urban versus rural classification, as this repackaging would circumvent the fair lending restriction.",Delay the selection decision and escalate to the legal team for a full reinterpretation of the fair lending laws before making any data selection choices.,B,"The correct answer is B. Phase I regulatory requirements take precedence over model accuracy. Using a prohibited attribute, even if predictive, violates compliance established in Phase I. Option A is incorrect because compliance cannot be sacrificed for accuracy‚Äîregulatory constraints are non-negotiable. Option C is incorrect because transforming a prohibited attribute into a proxy does not eliminate the legal risk; the underlying prohibited factor still influences the model. Option D is incorrect because the legal interpretation was already established in Phase I; delaying Phase III for reinterpretation is not appropriate when requirements are clear. [Maps to: Phase III: Data Preparation ‚Äì Data Selection ‚Äì Select Data]"
Phase III: Data Preparation,Data Cleansing and Enhancement,Clean Data,"A healthcare analytics team is cleaning a patient dataset for a readmission prediction model. They find that 15% of records have missing values for the ""discharge time"" field, and a small number of records show negative values for ""length of stay"" which is physically impossible. What is the most appropriate cleaning strategy?","Delete all records with any data quality issue to ensure the model trains only on perfectly clean data, even though this removes 15% of the dataset.","Impute missing discharge times using the median value for the department, and replace negative length of stay values with positive values by taking the absolute number.","For missing discharge times, consider imputation or flagging as appropriate; for impossible negative values, investigate the root cause and then correct or delete those specific records.","Document both issues in the data quality report and defer all remediation to Phase IV: Model Development, where the modeling team can handle them during training.",C,"The correct answer is C. Different data quality issues require different remediation strategies. Missing values can be handled via imputation or flagging, while impossible values (negative length of stay) require root cause investigation and correction or removal. Option A is incorrect because deleting 15% of records may remove valuable data unnecessarily and reduce the training set significantly. Option B is incorrect because taking the absolute value of a negative length of stay masks the underlying error without understanding what caused it‚Äîthe data may be systematically wrong. Option D is incorrect because data cleaning is a Phase III activity; deferring remediation to Phase IV violates the CPMAI phase sequence. [Maps to: Phase III: Data Preparation ‚Äì Data Cleansing and Enhancement ‚Äì Clean Data]"
Phase III: Data Preparation,Data Cleansing and Enhancement,Clean Data,"During Phase II data quality verification for a customer churn model, a data analyst notices that 8% of records have missing income values and that some income values are unrealistically high due to data entry errors. The analyst immediately begins writing Python scripts to impute missing incomes with the median and cap extreme values at the 99th percentile. According to CPMAI methodology, what is the primary concern?",Using Python for data cleaning is not permitted under PMI standards; only commercial ETL tools with built-in validation are acceptable for Phase III remediation activities.,"The analyst is performing remediation activities during Phase II, when the Clean Data task should occur in Phase III after all quality issues have been fully documented and assessed.",Imputing missing values with the median is always an incorrect approach; mean imputation should be used instead because it preserves the overall distribution of the feature.,Capping extreme values at the 99th percentile should never be done because outliers always contain important information about customer behavior that should be preserved.,B,"The correct answer is B. Phase II is for identifying and documenting quality issues; Phase III is for remediating them through Clean Data activities. The analyst is jumping ahead to remediation before completing the assessment. Option A is incorrect because the CPMAI methodology does not prescribe specific programming languages or tools for data cleaning. Option C is incorrect because median imputation can be a valid approach; the issue here is timing (wrong phase), not the imputation method chosen. Option D is incorrect because capping extreme values can be appropriate depending on the context; the methodology does not prohibit outlier treatment as a general rule. [Maps to: Phase III: Data Preparation ‚Äì Data Cleansing and Enhancement ‚Äì Clean Data]"
Phase III: Data Preparation,Data Cleansing and Enhancement,Clean Data,"A data science team is cleaning a dataset for a housing price prediction model. They need to decide how to handle 5% of records with missing square footage values. They consider three options: delete the records, impute with the neighborhood median, or flag the missing values as a separate indicator. Why is it critical to document whichever cleaning decision they make?",Because PMI requires formal documentation for all project management activities regardless of their specific impact on the final model deliverable.,Because the Phase II data quality report cannot be finalized until all downstream cleaning decisions are fully documented and approved by the project sponsor.,Because the legal team needs to review and approve all missing value treatment decisions before the model can be deployed to production in Phase VI.,"Because every cleaning decision is effectively a modeling decision that affects what the model learns, and documentation ensures traceability and reproducibility of those choices.",D,"The correct answer is D. How missing values are handled directly affects the training data and what the model learns, so documenting the decision supports traceability and reproducibility. Option A is incorrect because the rationale is overly broad‚Äîthe importance of documenting cleaning decisions is specifically tied to their modeling impact, not generic PMI documentation requirements. Option B is incorrect because the Phase II data quality report is completed before Phase III cleaning begins; cleaning decisions do not feed back into the Phase II report. Option C is incorrect because legal review of missing value treatment is not a standard requirement‚Äîdocumentation serves the data science team's reproducibility needs, not legal approval. [Maps to: Phase III: Data Preparation ‚Äì Data Cleansing and Enhancement ‚Äì Clean Data]"
Phase III: Data Preparation,Data Cleansing and Enhancement,Clean Data,"A team is cleaning a customer survey dataset where 20% of records are missing the ""annual income"" field. They are debating three strategies: deleting all records with missing income, imputing income based on other demographic features, or creating a binary indicator flag for missingness. Which statement most accurately describes the trade-offs among these three approaches?",Deletion preserves data integrity but reduces sample size; imputation maintains sample size but introduces assumptions about the missing values; flagging preserves missingness information but may add complexity to the model.,Deletion is the fastest approach but it always introduces selection bias; imputation is mathematically complex and rarely produces accurate estimates; flagging is simple but never provides any value to model performance.,Deletion should only be used when missing values are under 5% of records; imputation is only valid when the data follows a normal distribution; flagging is only appropriate for categorical features.,Deletion requires no documentation in the data preparation report; imputation requires advanced statistical expertise that most teams lack; flagging requires formal approval from the project sponsor.,A,"The correct answer is A. Each strategy involves genuine trade-offs: deletion reduces data volume, imputation adds assumptions, and flagging preserves missingness information while potentially adding model complexity. Option B is incorrect because it uses absolutes (""always,"" ""rarely,"" ""never"") that are not supported‚Äîdeletion does not always introduce bias, imputation can be quite accurate, and flagging can improve model performance. Option C is incorrect because these arbitrary thresholds and restrictions are not established in the methodology‚Äîthe appropriate approach depends on the specific context and data characteristics. Option D is incorrect because all cleaning decisions should be documented regardless of the approach chosen, imputation does not require exceptional expertise, and flagging does not require sponsor approval. [Maps to: Phase III: Data Preparation ‚Äì Data Cleansing and Enhancement ‚Äì Clean Data]"
Phase III: Data Preparation,Data Cleansing and Enhancement,Enhance and Augment Data,"A computer vision team has 2,000 images of products for a quality inspection model. To improve model robustness, they apply random rotations, flips, and brightness adjustments to create additional training examples. Separately, they derive a new feature called ""edge density"" from each original image. What is the correct classification of these two activities?","Both activities are examples of data augmentation, as they both increase the overall size and diversity of the dataset available for model training by creating additional data points from the originals.","Both activities are examples of data enhancement, as they both improve the quality and analytical usefulness of the existing data through different types of transformation applied to the original records.","Creating rotated and flipped images is data augmentation (increasing dataset diversity through synthetic examples), while deriving edge density is data enhancement (improving existing data through feature engineering).","Applying image rotations and flips is a data cleaning activity that corrects orientation problems in the original images, while deriving new features like edge density is a data selection activity.",C,"The correct answer is C. Augmentation creates additional synthetic examples to increase dataset diversity (rotations, flips, brightness), while enhancement improves existing data through transformations or feature engineering (deriving edge density). Option A is incorrect because feature derivation does not create new training samples‚Äîit enriches existing records with additional information, which is enhancement not augmentation. Option B is incorrect because augmentation and enhancement are distinct activities; grouping them both as enhancement ignores the distinction between creating new samples and enriching existing ones. Option D is incorrect because rotations are deliberate augmentation to improve robustness, not cleaning, and feature derivation is enhancement, not selection. [Maps to: Phase III: Data Preparation ‚Äì Data Cleansing and Enhancement ‚Äì Enhance and Augment Data]"
Phase III: Data Preparation,Data Cleansing and Enhancement,Enhance and Augment Data,"During Phase II exploration for a credit card fraud detection model, the team discovered that only 0.1% of transactions are fraudulent, creating a severe class imbalance. The Phase I business objectives require high recall for fraudulent transactions. What is the most appropriate Phase III strategy to address this imbalance?",Proceed with the imbalanced data and rely on the model algorithm in Phase IV to handle the class imbalance automatically during the training process.,Postpone the project and collect additional real transaction data for several years until the fraud rate naturally increases to a more balanced level.,Exclude all fraudulent transactions from the training data entirely and build a model that only learns to identify normal transaction patterns.,Apply data augmentation techniques such as SMOTE to generate synthetic fraudulent transactions and improve the class distribution balance in the training data.,D,The correct answer is D. Severe class imbalance identified in Phase II is a key driver for augmentation in Phase III. Techniques like SMOTE generate synthetic minority-class examples to improve the model's ability to learn fraud patterns. Option A is incorrect because extreme imbalance (0.1%) often requires active intervention beyond relying on the algorithm alone‚Äîmost algorithms struggle with such severe skew. Option B is incorrect because it is impractical and delays the project indefinitely; augmentation is the standard approach for addressing known imbalance. Option C is incorrect because removing the target class entirely would make it impossible to train a fraud detection model‚Äîthe model needs examples of fraud to learn to detect it. [Maps to: Phase III: Data Preparation ‚Äì Data Cleansing and Enhancement ‚Äì Enhance and Augment Data]
Phase III: Data Preparation,Data Cleansing and Enhancement,Enhance and Augment Data,"A team developing a facial recognition system uses aggressive data augmentation in Phase III, generating 10 synthetic variations of each original face image through rotations, lighting changes, and artificial blur. The model performs well on validation data but fails when deployed in real-world conditions with actual lighting and angle variations. What is the most likely explanation?","The original dataset before augmentation was too small to be useful, and no amount of synthetic augmentation can ever compensate for an insufficient number of real training images.","The augmentation introduced artifacts that do not represent real-world conditions, causing the model to learn patterns specific to the synthetic data rather than generalizable visual features.","The team should have selected a different model architecture in Phase IV that is specifically optimized for facial recognition tasks, rather than relying on augmented training data.",The Phase II data exploration failed to identify that facial recognition technology is fundamentally unsuitable for real-world security applications with variable conditions.,B,"The correct answer is B. Over-augmentation can introduce unrealistic artifacts that shift the data distribution, causing strong validation results on similar synthetic patterns but poor real-world generalization. The model learned augmentation artifacts rather than genuine visual features. Option A is incorrect because augmentation can help with limited data, and the failure here is specifically about unrealistic artifacts, not dataset size. Option C is incorrect because the problem is in the data preparation (augmentation quality), not the model architecture choice‚Äîchanging the architecture would not fix training data that contains unrealistic patterns. Option D is incorrect because facial recognition is a valid application; the issue is the specific augmentation approach used in Phase III, not the fundamental feasibility of the project. [Maps to: Phase III: Data Preparation ‚Äì Data Cleansing and Enhancement ‚Äì Enhance and Augment Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"A project team is developing a supervised learning model to classify insurance claims as fraudulent or legitimate. They have collected 100,000 claims records. According to CPMAI, what is the primary purpose of the Label Data task for this project?","To assign ground truth labels of ""fraudulent"" or ""legitimate"" to each claim, providing the target variable for the model to learn from during training.","To remove duplicate claims, standardize date formats, and resolve inconsistencies across all records before any analysis or modeling begins.",To explore the data visually using charts and statistics to identify patterns that distinguish fraudulent from legitimate claims.,"To select which features, such as claim amount, provider history, and submission date, should be included in the final modeling dataset.",A,"Option A is correct. Label Data assigns ground truth targets (""fraudulent"" or ""legitimate"") required for supervised learning so the model can learn mappings from features to labels during training. Option B describes Clean Data work such as deduplication and format standardization, which falls under Data Cleansing and Enhancement, not labeling. Option C describes exploratory data analysis, which is a Phase II: Data Understanding activity, not Phase III labeling. Option D describes feature selection, which is part of Select Data, not Label Data. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"During Phase II data exploration for a sentiment analysis project, a data analyst begins reading through customer reviews and assigning ""positive,"" ""negative,"" or ""neutral"" labels to build a training set. A senior team member explains this approach is methodologically incorrect. What is the primary reason this activity should stop?",The analyst should first complete the Describe Data task to document the volume and structure of the reviews before any labeling can begin.,Labeling is a Phase III Data Preparation activity; performing it during Phase II conflates the understanding and preparation phases of the methodology.,"Sentiment analysis requires specialized linguistic training, and the analyst is not qualified to assign accurate sentiment labels.",The team should first enhance the data by correcting spelling errors and normalizing formatting in the reviews to ensure label quality before any labeling begins.,B,"Option A is incorrect because while Describe Data is a Phase II task, completing it is not the prerequisite that fixes the phase boundary issue‚Äîlabeling simply does not belong in Phase II regardless of task completion order. Option B is correct. CPMAI separates Phase II: Data Understanding from Phase III: Data Preparation; labeling belongs in Phase III, not Phase II, so performing it during exploration conflates the two phases. Option C may reflect a practical concern about annotator qualifications, but the primary methodological issue is performing a Phase III activity during Phase II. Option D is incorrect because data enhancement is a separate Phase III activity and does not resolve the fundamental problem of labeling in the wrong phase. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"A medical imaging startup needs to label 500,000 X-ray images for fracture detection. They have a very limited budget but require extremely high accuracy due to patient safety concerns. The project timeline allows for 12 months of work. Which labeling approach best balances these constraints while maintaining the required quality?",Use a fully automated pretrained fracture detection model to generate all labels instantly without any human involvement or review.,"Rely exclusively on crowdsourced workers from an online platform, as this is the fastest and most cost-effective approach available.","Implement a process where a small team of expert radiologists labels all 500,000 images, even though this will take several years and exceed the budget.","Use an active learning approach where a model suggests labels for expert review, focusing limited expert time on the most uncertain or challenging cases.",D,"Option A is incorrect because fully automated labeling without any human review risks unacceptable errors in a patient safety context where accuracy is critical. Option B is incorrect because relying exclusively on crowdsourced workers without expert medical oversight can undermine the high accuracy required for clinical applications. Option C is incorrect because having a small team label all 500,000 images would exceed both the budget and timeline constraints, making it infeasible despite its quality advantages. Option D is correct. Active learning focuses limited expert effort on the most uncertain and challenging cases, supporting high label quality while controlling cost and timeline‚Äîbalancing all three project constraints. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"Three annotators are independently labeling customer support tickets as ""billing issue,"" ""technical problem,"" or ""account question."" After labeling 1,000 tickets each, the project lead calculates agreement statistics and finds that Cohen's Kappa between any two annotators ranges from 0.35 to 0.42. What is the most appropriate interpretation of these values and the next step?",The annotators are working too slowly; the project lead should incentivize faster labeling to complete the remaining tickets and improve overall throughput.,"The Kappa values are excellent, indicating strong agreement among the annotators; the team should proceed directly to model training without delay.","The labeling guidelines are likely ambiguous or insufficient, leading to poor inter-annotator agreement; the team should refine the guidelines and retrain the annotators.",The disagreement indicates that the AI pattern should be changed from classification to regression to better handle the ambiguity in the ticket categories.,C,"Option A is incorrect because incentivizing speed does not address the quality problem and may actually worsen agreement by encouraging hasty decisions. Option B is incorrect because Kappa values of 0.35‚Äì0.42 represent poor agreement, not strong agreement‚Äîproceeding to model training would use unreliable labels. Option C is correct. Low Cohen's Kappa values indicate only fair agreement beyond chance, strongly suggesting the labeling guidelines are ambiguous or insufficient; refining guidelines and retraining annotators addresses label consistency at the root cause. Option D is incorrect because inter-annotator disagreement is a labeling quality issue, not evidence that the AI pattern itself needs to change. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"Four weeks into an eight-week labeling project for content moderation, the project manager notices that the number of posts labeled per hour has dropped by 40%, and error rates on quality checks have tripled. Labelers report feeling distressed by the violent and disturbing content they are reviewing. What is the most likely explanation for this quality decline?",Annotator fatigue and emotional exhaustion from reviewing disturbing content are degrading both productivity and accuracy.,The labeling platform has a technical bug that is slowing down the user interface and frustrating the annotators.,The Phase II data exploration failed to identify that the dataset contains too many false positives for violent content.,"The Phase I cognitive requirements should have specified a different AI pattern, such as anomaly detection instead of classification.",A,"Option A is correct. Annotator fatigue and emotional exhaustion from reviewing disturbing content can reduce throughput and increase errors, creating the observed pattern of lower productivity combined with higher mistake rates‚Äîconsistent with the labelers' own reports of distress. Option B is possible in theory, but a technical bug alone does not explain the emotional distress reported by labelers or the tripled error rates on quality checks. Option C is incorrect because a Phase II exploration issue would not explain the human performance decline that emerged four weeks into a labeling effort. Option D is incorrect because the problem is labeling execution and annotator support, not a mismatch in the AI pattern selected during Phase I. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"A Phase I cognitive requirements document specifies that an AI system must identify the specific type of plant disease from photographs of leaves. The Phase I AI pattern is identified as image classification with 10 disease categories plus healthy. In Phase III, what specific type of labels must be created to align with this Phase I requirement?","Bounding box coordinates drawn around each diseased leaf spot in the image, marking the location and size of affected regions.","Segmentation masks that highlight every pixel belonging to diseased tissue, distinguishing affected from healthy leaf areas.","Numerical severity scores indicating the percentage of leaf area affected by disease, quantifying damage on a continuous scale.",A single categorical label per image indicating which of the 11 classes (10 diseases or healthy) the leaf represents.,D,"Option A is incorrect because bounding box coordinates are used for object detection tasks, not image classification. Option B is incorrect because segmentation masks are used for pixel-level segmentation tasks, not whole-image classification. Option C is incorrect because numerical severity scores represent a regression-style label, not the categorical classification label required by the Phase I specification. Option D is correct. Image classification requires one categorical class label per image aligned to the defined set of classes specified in Phase I‚Äîhere, 11 classes consisting of 10 diseases plus healthy. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"A team completes Phase III labeling for a resume screening model, creating 50,000 labeled resumes indicating ""interview"" or ""reject."" During Phase IV, the model achieves 92% accuracy. During Phase V, the team discovers that 8% of the test set labels are incorrect due to ambiguous criteria about what constitutes relevant experience. What is the most significant implication of this discovery?",The model's accuracy is likely even higher than 92% because correcting label errors would only improve performance metrics.,"The Phase IV model training was successful based on the provided labels, so the model can be deployed while acknowledging the label limitations in documentation.","The Phase V evaluation metrics are unreliable because they are based on flawed ground truth, meaning the model's true performance on correct labels is unknown.",The team should return to Phase II: Data Understanding to explore why ambiguous resumes exist in the dataset and whether additional data sources are needed.,C,"Option A is incorrect because label errors can inflate or deflate measured accuracy in unpredictable ways‚Äîcorrecting errors does not guarantee improvement. Option B is incorrect because deployment decisions require trustworthy evaluation, and acknowledging label limitations in documentation does not make unreliable metrics safe to act on. Option C is correct. If 8% of ground truth labels in the test set are incorrect, Phase V evaluation metrics derived from those labels are unreliable and the model's true performance on correctly labeled data is unknown. Option D may be a relevant follow-up activity, but the most significant immediate implication is that the current evaluation results cannot be trusted. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"
Phase III: Data Preparation,Data Labeling,Label Data,"During Phase I, a project estimated that labeling 100,000 legal documents for contract clause classification would cost $150,000 and take 10 weeks using paralegals. In Phase III, after labeling 40,000 documents, the team has already spent $140,000 and used 9 weeks. The remaining 60,000 documents will require additional funding that exceeds the project's contingency reserves. What is the appropriate CPMAI-aligned action?",Reduce labeling quality standards to speed up the remaining documents and complete the work within the original budget and timeline.,Document the cost overrun and trigger a return to Phase I to reassess project feasibility and resource requirements with business stakeholders.,"Skip labeling the remaining documents and train the model on only the 40,000 already-labeled documents to avoid additional costs.",Continue labeling using less expensive but untrained temporary workers to complete the work within the original budget.,B,"Option A is incorrect because lowering quality standards undermines label reliability and compromises downstream model validity. Option B is correct. A major resource overrun in Phase III should trigger an iterative return to Phase I to reassess project feasibility, budget, and scope with business stakeholders, as CPMAI supports iterative movement between phases when conditions change. Option C is incorrect because reducing the labeled data volume to only the 40,000 already-labeled documents is a scope change that should be decided with stakeholders during a formal reassessment, not unilaterally. Option D is incorrect because using untrained temporary workers risks poor label quality and does not resolve the underlying feasibility issue. [Maps to: Phase III: Data Preparation ‚Äì Data Labeling ‚Äì Label Data]"